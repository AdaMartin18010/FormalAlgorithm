---
title: 12.1 äººå·¥æ™ºèƒ½ç®—æ³•åº”ç”¨ / Artificial Intelligence Algorithm Applications
version: 1.0
status: maintained
last_updated: 2025-01-11
owner: åº”ç”¨é¢†åŸŸå·¥ä½œç»„
---

> ğŸ“Š **é¡¹ç›®å…¨é¢æ¢³ç†**ï¼šè¯¦ç»†çš„é¡¹ç›®ç»“æ„ã€æ¨¡å—è¯¦è§£å’Œå­¦ä¹ è·¯å¾„ï¼Œè¯·å‚é˜… [`é¡¹ç›®å…¨é¢æ¢³ç†-2025.md`](../é¡¹ç›®å…¨é¢æ¢³ç†-2025.md)

## 12.1 äººå·¥æ™ºèƒ½ç®—æ³•åº”ç”¨ / Artificial Intelligence Algorithm Applications

### æ‘˜è¦ / Executive Summary

- ç»Ÿä¸€äººå·¥æ™ºèƒ½ç®—æ³•åœ¨å„ç±»åº”ç”¨ä¸­çš„ä½¿ç”¨è§„èŒƒä¸æœ€ä½³å®è·µã€‚
- å»ºç«‹äººå·¥æ™ºèƒ½ç®—æ³•åœ¨åº”ç”¨é¢†åŸŸä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚

### å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary

- äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€‚
- æœ¯è¯­å¯¹é½ä¸å¼•ç”¨è§„èŒƒï¼š`docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md`ï¼Œ`01-åŸºç¡€ç†è®º/00-æ’°å†™è§„èŒƒä¸å¼•ç”¨æŒ‡å—.md`

### æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ / Terminology & Notation

- äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼‰ï¼šæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚
- æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰ï¼šä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼çš„ç®—æ³•ã€‚
- æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ï¼šåŸºäºå¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚
- ç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰ï¼šæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„ç®—æ³•æ¨¡å‹ã€‚
- è®°å·çº¦å®šï¼š`X` è¡¨ç¤ºè¾“å…¥ï¼Œ`Y` è¡¨ç¤ºè¾“å‡ºï¼Œ`Î¸` è¡¨ç¤ºå‚æ•°ï¼Œ`L` è¡¨ç¤ºæŸå¤±å‡½æ•°ã€‚

### äº¤å‰å¼•ç”¨å¯¼èˆª / Cross-References

- ç¥ç»ç½‘ç»œç®—æ³•ï¼šå‚è§ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/17-ç¥ç»ç½‘ç»œç®—æ³•ç†è®º.md`ã€‚
- æœºå™¨å­¦ä¹ ç®—æ³•ï¼šå‚è§ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/` ç›¸å…³æ–‡æ¡£ã€‚
- ç®—æ³•è®¾è®¡ï¼šå‚è§ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/01-ç®—æ³•è®¾è®¡ç†è®º.md`ã€‚

### å¿«é€Ÿå¯¼èˆª / Quick Links

- åŸºæœ¬æ¦‚å¿µ
- æœºå™¨å­¦ä¹ ç®—æ³•
- æ·±åº¦å­¦ä¹ ç®—æ³•

## ç›®å½• (Table of Contents)

- [12.1 äººå·¥æ™ºèƒ½ç®—æ³•åº”ç”¨ / Artificial Intelligence Algorithm Applications](#121-äººå·¥æ™ºèƒ½ç®—æ³•åº”ç”¨--artificial-intelligence-algorithm-applications)
  - [æ‘˜è¦ / Executive Summary](#æ‘˜è¦--executive-summary)
  - [å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary](#å…³é”®æœ¯è¯­ä¸ç¬¦å·--glossary)
  - [æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ / Terminology \& Notation](#æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ--terminology--notation)
  - [äº¤å‰å¼•ç”¨å¯¼èˆª / Cross-References](#äº¤å‰å¼•ç”¨å¯¼èˆª--cross-references)
  - [å¿«é€Ÿå¯¼èˆª / Quick Links](#å¿«é€Ÿå¯¼èˆª--quick-links)
- [ç›®å½• (Table of Contents)](#ç›®å½•-table-of-contents)
- [1.1 åŸºæœ¬æ¦‚å¿µ (Basic Concepts)](#11-åŸºæœ¬æ¦‚å¿µ-basic-concepts)
  - [1.1.1 äººå·¥æ™ºèƒ½å®šä¹‰ (Definition of Artificial Intelligence)](#111-äººå·¥æ™ºèƒ½å®šä¹‰-definition-of-artificial-intelligence)
  - [1.1.2 äººå·¥æ™ºèƒ½å†å² (History of Artificial Intelligence)](#112-äººå·¥æ™ºèƒ½å†å²-history-of-artificial-intelligence)
  - [1.1.3 äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸ (AI Application Areas)](#113-äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸ-ai-application-areas)
- [1.2 æœºå™¨å­¦ä¹ ç®—æ³• (Machine Learning Algorithms)](#12-æœºå™¨å­¦ä¹ ç®—æ³•-machine-learning-algorithms)
  - [1.2.1 ç›‘ç£å­¦ä¹  (Supervised Learning)](#121-ç›‘ç£å­¦ä¹ -supervised-learning)
  - [1.2.2 æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)](#122-æ— ç›‘ç£å­¦ä¹ -unsupervised-learning)
  - [1.2.3 å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)](#123-å¼ºåŒ–å­¦ä¹ -reinforcement-learning)
- [1.3 æ·±åº¦å­¦ä¹ ç®—æ³• (Deep Learning Algorithms)](#13-æ·±åº¦å­¦ä¹ ç®—æ³•-deep-learning-algorithms)
  - [1.3.1 ç¥ç»ç½‘ç»œåŸºç¡€ (Neural Network Basics)](#131-ç¥ç»ç½‘ç»œåŸºç¡€-neural-network-basics)
  - [1.3.2 æ·±åº¦å­¦ä¹ æ¶æ„ (Deep Learning Architectures)](#132-æ·±åº¦å­¦ä¹ æ¶æ„-deep-learning-architectures)
- [1.4 è‡ªç„¶è¯­è¨€å¤„ç† (Natural Language Processing)](#14-è‡ªç„¶è¯­è¨€å¤„ç†-natural-language-processing)
  - [1.4.1 æ–‡æœ¬é¢„å¤„ç† (Text Preprocessing)](#141-æ–‡æœ¬é¢„å¤„ç†-text-preprocessing)
  - [1.4.2 æ–‡æœ¬è¡¨ç¤º (Text Representation)](#142-æ–‡æœ¬è¡¨ç¤º-text-representation)
  - [1.4.3 è¯­è¨€æ¨¡å‹ (Language Models)](#143-è¯­è¨€æ¨¡å‹-language-models)
- [1.5 è®¡ç®—æœºè§†è§‰ (Computer Vision)](#15-è®¡ç®—æœºè§†è§‰-computer-vision)
  - [1.5.1 å›¾åƒå¤„ç†åŸºç¡€ (Image Processing Basics)](#151-å›¾åƒå¤„ç†åŸºç¡€-image-processing-basics)
  - [1.5.2 ç›®æ ‡æ£€æµ‹ (Object Detection)](#152-ç›®æ ‡æ£€æµ‹-object-detection)
  - [1.5.3 å›¾åƒåˆ†å‰² (Image Segmentation)](#153-å›¾åƒåˆ†å‰²-image-segmentation)
- [1.6 å®ç°ç¤ºä¾‹ (Implementation Examples)](#16-å®ç°ç¤ºä¾‹-implementation-examples)
  - [1.6.1 æœºå™¨å­¦ä¹ é¡¹ç›® (Machine Learning Project)](#161-æœºå™¨å­¦ä¹ é¡¹ç›®-machine-learning-project)
  - [1.6.2 æ·±åº¦å­¦ä¹ é¡¹ç›® (Deep Learning Project)](#162-æ·±åº¦å­¦ä¹ é¡¹ç›®-deep-learning-project)
- [1.7 å‚è€ƒæ–‡çŒ® (References)](#17-å‚è€ƒæ–‡çŒ®-references)
- [1.8 äº¤å‰å¼•ç”¨ä¸ä¾èµ– (Cross References and Dependencies)](#18-äº¤å‰å¼•ç”¨ä¸ä¾èµ–-cross-references-and-dependencies)
- [1.9 ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½ / Alignment with Project Structure](#19-ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½--alignment-with-project-structure)
  - [ç›¸å…³æ–‡æ¡£ / Related Documents](#ç›¸å…³æ–‡æ¡£--related-documents)
  - [çŸ¥è¯†ä½“ç³»ä½ç½® / Knowledge System Position](#çŸ¥è¯†ä½“ç³»ä½ç½®--knowledge-system-position)
  - [VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£ / VIEW Folder Related Documents](#viewæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£--view-folder-related-documents)

---

## 1.1 åŸºæœ¬æ¦‚å¿µ (Basic Concepts)

### 1.1.1 äººå·¥æ™ºèƒ½å®šä¹‰ (Definition of Artificial Intelligence)

**äººå·¥æ™ºèƒ½å®šä¹‰ / Definition of Artificial Intelligence:**

äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

Artificial Intelligence is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence.

**äººå·¥æ™ºèƒ½çš„ç‰¹ç‚¹ / Characteristics of Artificial Intelligence:**

1. **å­¦ä¹ èƒ½åŠ› (Learning Ability) / Learning Ability:**
   - ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ / Learn patterns from data
   - æ”¹è¿›æ€§èƒ½ / Improve performance

2. **æ¨ç†èƒ½åŠ› (Reasoning Ability) / Reasoning Ability:**
   - é€»è¾‘æ¨ç† / Logical reasoning
   - é—®é¢˜è§£å†³ / Problem solving

3. **æ„ŸçŸ¥èƒ½åŠ› (Perception Ability) / Perception Ability:**
   - ç†è§£ç¯å¢ƒ / Understand environment
   - å¤„ç†æ„Ÿå®˜ä¿¡æ¯ / Process sensory information

4. **é€‚åº”èƒ½åŠ› (Adaptation Ability) / Adaptation Ability:**
   - é€‚åº”æ–°æƒ…å†µ / Adapt to new situations
   - è‡ªæˆ‘è°ƒæ•´ / Self-adjustment

### 1.1.2 äººå·¥æ™ºèƒ½å†å² (History of Artificial Intelligence)

**äººå·¥æ™ºèƒ½å‘å±• / AI Development:**

äººå·¥æ™ºèƒ½çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°1950å¹´ä»£ï¼Œç»å†äº†å¤šä¸ªå‘å±•é˜¶æ®µã€‚

The development of AI can be traced back to the 1950s, going through multiple development phases.

**é‡è¦é‡Œç¨‹ç¢‘ / Important Milestones:**

1. **1950å¹´ä»£**: å›¾çµæµ‹è¯•æå‡º / Turing Test proposed
2. **1960å¹´ä»£**: ä¸“å®¶ç³»ç»Ÿå‘å±• / Expert systems development
3. **1980å¹´ä»£**: æœºå™¨å­¦ä¹ å…´èµ· / Machine learning emergence
4. **2000å¹´ä»£**: æ·±åº¦å­¦ä¹ çªç ´ / Deep learning breakthroughs
5. **2010å¹´ä»£**: å¤§è§„æ¨¡AIåº”ç”¨ / Large-scale AI applications

### 1.1.3 äººå·¥æ™ºèƒ½åº”ç”¨é¢†åŸŸ (AI Application Areas)

**ç†è®ºåº”ç”¨ / Theoretical Applications:**

1. **ç§‘å­¦ç ”ç©¶ (Scientific Research) / Scientific Research:**
   - æ•°æ®åˆ†æ / Data analysis
   - æ¨¡å¼è¯†åˆ« / Pattern recognition

2. **æ•°å­¦ç ”ç©¶ (Mathematical Research) / Mathematical Research:**
   - å®šç†è¯æ˜ / Theorem proving
   - æ•°å­¦å‘ç° / Mathematical discovery

**å®è·µåº”ç”¨ / Practical Applications:**

1. **åŒ»ç–—å¥åº· (Healthcare) / Healthcare:**
   - ç–¾ç—…è¯Šæ–­ / Disease diagnosis
   - è¯ç‰©å‘ç° / Drug discovery

2. **é‡‘èç§‘æŠ€ (FinTech) / FinTech:**
   - é£é™©è¯„ä¼° / Risk assessment
   - äº¤æ˜“é¢„æµ‹ / Trading prediction

3. **è‡ªåŠ¨é©¾é©¶ (Autonomous Driving) / Autonomous Driving:**
   - ç¯å¢ƒæ„ŸçŸ¥ / Environment perception
   - è·¯å¾„è§„åˆ’ / Path planning

---

## 1.2 æœºå™¨å­¦ä¹ ç®—æ³• (Machine Learning Algorithms)

### 1.2.1 ç›‘ç£å­¦ä¹  (Supervised Learning)

**ç›‘ç£å­¦ä¹ å®šä¹‰ / Supervised Learning Definition:**

ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

Supervised learning is a machine learning method that uses labeled training data to learn the mapping relationship between inputs and outputs.

**ç›‘ç£å­¦ä¹ ç®—æ³• / Supervised Learning Algorithms:**

```python
# Pythonä¸­çš„ç›‘ç£å­¦ä¹ ç¤ºä¾‹ / Supervised Learning Examples in Python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# çº¿æ€§å›å½’ / Linear Regression
def linear_regression_example():
    # ç”Ÿæˆæ•°æ® / Generate data
    X = np.random.rand(100, 2)
    y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.normal(0, 0.1, 100)

    # è®­ç»ƒæ¨¡å‹ / Train model
    model = LinearRegression()
    model.fit(X, y)

    # é¢„æµ‹ / Predict
    predictions = model.predict(X)
    return model, predictions

# æ”¯æŒå‘é‡æœº / Support Vector Machine
def svm_example():
    # ç”Ÿæˆåˆ†ç±»æ•°æ® / Generate classification data
    X = np.random.rand(100, 2)
    y = (X[:, 0] + X[:, 1] > 1).astype(int)

    # è®­ç»ƒæ¨¡å‹ / Train model
    model = SVC(kernel='rbf')
    model.fit(X, y)

    # é¢„æµ‹ / Predict
    predictions = model.predict(X)
    return model, predictions

# å†³ç­–æ ‘ / Decision Tree
def decision_tree_example():
    # ç”Ÿæˆæ•°æ® / Generate data
    X = np.random.rand(100, 3)
    y = (X[:, 0] > 0.5) & (X[:, 1] > 0.5)

    # è®­ç»ƒæ¨¡å‹ / Train model
    model = DecisionTreeClassifier()
    model.fit(X, y)

    # é¢„æµ‹ / Predict
    predictions = model.predict(X)
    return model, predictions

# éšæœºæ£®æ— / Random Forest
def random_forest_example():
    # ç”Ÿæˆæ•°æ® / Generate data
    X = np.random.rand(100, 4)
    y = np.sum(X > 0.5, axis=1)

    # è®­ç»ƒæ¨¡å‹ / Train model
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X, y)

    # é¢„æµ‹ / Predict
    predictions = model.predict(X)
    return model, predictions
```

### 1.2.2 æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)

**æ— ç›‘ç£å­¦ä¹ å®šä¹‰ / Unsupervised Learning Definition:**

æ— ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ã€‚

Unsupervised learning is a machine learning method that uses unlabeled data to discover hidden patterns in the data.

**æ— ç›‘ç£å­¦ä¹ ç®—æ³• / Unsupervised Learning Algorithms:**

```python
# Pythonä¸­çš„æ— ç›‘ç£å­¦ä¹ ç¤ºä¾‹ / Unsupervised Learning Examples in Python
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Kå‡å€¼èšç±» / K-Means Clustering
def kmeans_example():
    # ç”Ÿæˆæ•°æ® / Generate data
    X = np.random.rand(300, 2)

    # èšç±» / Clustering
    kmeans = KMeans(n_clusters=3)
    clusters = kmeans.fit_predict(X)

    return kmeans, clusters

# ä¸»æˆåˆ†åˆ†æ / Principal Component Analysis
def pca_example():
    # ç”Ÿæˆé«˜ç»´æ•°æ® / Generate high-dimensional data
    X = np.random.rand(100, 10)

    # é™ç»´ / Dimensionality reduction
    pca = PCA(n_components=2)
    X_reduced = pca.fit_transform(X)

    return pca, X_reduced

# t-SNEé™ç»´ / t-SNE Dimensionality Reduction
def tsne_example():
    # ç”Ÿæˆæ•°æ® / Generate data
    X = np.random.rand(100, 20)

    # é™ç»´ / Dimensionality reduction
    tsne = TSNE(n_components=2)
    X_reduced = tsne.fit_transform(X)

    return tsne, X_reduced
```

### 1.2.3 å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)

**å¼ºåŒ–å­¦ä¹ å®šä¹‰ / Reinforcement Learning Definition:**

å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

Reinforcement learning is a machine learning method that learns optimal strategies through interaction with the environment.

**å¼ºåŒ–å­¦ä¹ ç®—æ³• / Reinforcement Learning Algorithms:**

```python
# Pythonä¸­çš„å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹ / Reinforcement Learning Examples in Python
import gym
import numpy as np

# Qå­¦ä¹ ç®—æ³• / Q-Learning Algorithm
class QLearning:
    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95):
        self.q_table = np.zeros((state_size, action_size))
        self.lr = learning_rate
        self.gamma = discount_factor

    def choose_action(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            return np.random.randint(0, self.q_table.shape[1])
        return np.argmax(self.q_table[state])

    def learn(self, state, action, reward, next_state):
        old_value = self.q_table[state, action]
        next_max = np.max(self.q_table[next_state])
        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)
        self.q_table[state, action] = new_value

# æ·±åº¦Qç½‘ç»œ / Deep Q-Network
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

def dqn_example():
    # åˆ›å»ºç¯å¢ƒ / Create environment
    env = gym.make('CartPole-v1')

    # åˆ›å»ºæ¨¡å‹ / Create model
    model = DQN(4, 2)
    optimizer = torch.optim.Adam(model.parameters())

    # è®­ç»ƒå¾ªç¯ / Training loop
    for episode in range(1000):
        state = env.reset()
        done = False

        while not done:
            # é€‰æ‹©åŠ¨ä½œ / Choose action
            state_tensor = torch.FloatTensor(state)
            q_values = model(state_tensor)
            action = torch.argmax(q_values).item()

            # æ‰§è¡ŒåŠ¨ä½œ / Execute action
            next_state, reward, done, _ = env.step(action)

            # æ›´æ–°æ¨¡å‹ / Update model
            # (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ)

            state = next_state

    return model
```

---

## 1.3 æ·±åº¦å­¦ä¹ ç®—æ³• (Deep Learning Algorithms)

### 1.3.1 ç¥ç»ç½‘ç»œåŸºç¡€ (Neural Network Basics)

**ç¥ç»ç½‘ç»œå®šä¹‰ / Neural Network Definition:**

ç¥ç»ç½‘ç»œæ˜¯å—ç”Ÿç‰©ç¥ç»ç½‘ç»œå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ç»„æˆã€‚

Neural networks are computational models inspired by biological neural networks, consisting of interconnected nodes.

**ç¥ç»ç½‘ç»œç»“æ„ / Neural Network Structure:**

```python
# Pythonä¸­çš„ç¥ç»ç½‘ç»œç¤ºä¾‹ / Neural Network Examples in Python
import torch
import torch.nn as nn
import torch.optim as optim

# å‰é¦ˆç¥ç»ç½‘ç»œ / Feedforward Neural Network
class FeedforwardNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FeedforwardNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# å·ç§¯ç¥ç»ç½‘ç»œ / Convolutional Neural Network
class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# å¾ªç¯ç¥ç»ç½‘ç»œ / Recurrent Neural Network
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])
        return out
```

### 1.3.2 æ·±åº¦å­¦ä¹ æ¶æ„ (Deep Learning Architectures)

**æ·±åº¦å­¦ä¹ æ¶æ„ç±»å‹ / Deep Learning Architecture Types:**

1. **å·ç§¯ç¥ç»ç½‘ç»œ (CNN) / Convolutional Neural Network:**
   - å›¾åƒå¤„ç† / Image processing
   - ç‰¹å¾æå– / Feature extraction

2. **å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) / Recurrent Neural Network:**
   - åºåˆ—å¤„ç† / Sequence processing
   - æ—¶é—´åºåˆ— / Time series

3. **é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM) / Long Short-Term Memory:**
   - é•¿æœŸä¾èµ– / Long-term dependencies
   - è‡ªç„¶è¯­è¨€å¤„ç† / Natural language processing

4. **Transformer / Transformer:**
   - æ³¨æ„åŠ›æœºåˆ¶ / Attention mechanism
   - å¹¶è¡Œå¤„ç† / Parallel processing

```python
# LSTMç¤ºä¾‹ / LSTM Example
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        c0 = torch.zeros(1, x.size(0), self.hidden_size)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Transformerç¤ºä¾‹ / Transformer Example
class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super(Transformer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_layers)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt):
        src = self.embedding(src)
        tgt = self.embedding(tgt)
        output = self.transformer(src, tgt)
        output = self.fc(output)
        return output
```

---

## 1.4 è‡ªç„¶è¯­è¨€å¤„ç† (Natural Language Processing)

### 1.4.1 æ–‡æœ¬é¢„å¤„ç† (Text Preprocessing)

**æ–‡æœ¬é¢„å¤„ç†å®šä¹‰ / Text Preprocessing Definition:**

æ–‡æœ¬é¢„å¤„ç†æ˜¯å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¤„ç†çš„æ ¼å¼çš„è¿‡ç¨‹ã€‚

Text preprocessing is the process of converting raw text into a format that machine learning algorithms can process.

**æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤ / Text Preprocessing Steps:**

```python
# Pythonä¸­çš„æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹ / Text Preprocessing Examples in Python
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# æ–‡æœ¬æ¸…ç† / Text Cleaning
def clean_text(text):
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ / Remove special characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # è½¬æ¢ä¸ºå°å†™ / Convert to lowercase
    text = text.lower()
    # ç§»é™¤å¤šä½™ç©ºæ ¼ / Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# åˆ†è¯ / Tokenization
def tokenize_text(text):
    tokens = word_tokenize(text)
    return tokens

# åœç”¨è¯ç§»é™¤ / Stop Word Removal
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
    return filtered_tokens

# è¯å¹²æå– / Stemming
def stem_tokens(tokens):
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens

# å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†æµç¨‹ / Complete Text Preprocessing Pipeline
def preprocess_text(text):
    # æ¸…ç†æ–‡æœ¬ / Clean text
    cleaned_text = clean_text(text)
    # åˆ†è¯ / Tokenize
    tokens = tokenize_text(cleaned_text)
    # ç§»é™¤åœç”¨è¯ / Remove stopwords
    filtered_tokens = remove_stopwords(tokens)
    # è¯å¹²æå– / Stemming
    stemmed_tokens = stem_tokens(filtered_tokens)
    return stemmed_tokens
```

### 1.4.2 æ–‡æœ¬è¡¨ç¤º (Text Representation)

**æ–‡æœ¬è¡¨ç¤ºæ–¹æ³• / Text Representation Methods:**

1. **è¯è¢‹æ¨¡å‹ (Bag of Words) / Bag of Words:**
   - ç®€å•æœ‰æ•ˆ / Simple and effective
   - å¿½ç•¥è¯åº / Ignore word order

2. **TF-IDF / TF-IDF:**
   - è€ƒè™‘è¯é¢‘ / Consider word frequency
   - è€ƒè™‘è¯é‡è¦æ€§ / Consider word importance

3. **è¯åµŒå…¥ (Word Embeddings) / Word Embeddings:**
   - è¯­ä¹‰è¡¨ç¤º / Semantic representation
   - å‘é‡ç©ºé—´ / Vector space

```python
# è¯è¢‹æ¨¡å‹ç¤ºä¾‹ / Bag of Words Example
from sklearn.feature_extraction.text import CountVectorizer

def bag_of_words_example():
    texts = [
        "machine learning is interesting",
        "deep learning is powerful",
        "artificial intelligence is the future"
    ]

    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(texts)
    return X, vectorizer.get_feature_names_out()

# TF-IDFç¤ºä¾‹ / TF-IDF Example
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_example():
    texts = [
        "machine learning is interesting",
        "deep learning is powerful",
        "artificial intelligence is the future"
    ]

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(texts)
    return X, vectorizer.get_feature_names_out()

# Word2Vecç¤ºä¾‹ / Word2Vec Example
from gensim.models import Word2Vec

def word2vec_example():
    sentences = [
        ['machine', 'learning', 'is', 'interesting'],
        ['deep', 'learning', 'is', 'powerful'],
        ['artificial', 'intelligence', 'is', 'the', 'future']
    ]

    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
    return model
```

### 1.4.3 è¯­è¨€æ¨¡å‹ (Language Models)

**è¯­è¨€æ¨¡å‹å®šä¹‰ / Language Model Definition:**

è¯­è¨€æ¨¡å‹æ˜¯è®¡ç®—æ–‡æœ¬åºåˆ—æ¦‚ç‡çš„æ¨¡å‹ï¼Œç”¨äºç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ã€‚

Language models are models that compute the probability of text sequences, used for understanding and generating natural language.

**è¯­è¨€æ¨¡å‹ç±»å‹ / Language Model Types:**

```python
# N-gramè¯­è¨€æ¨¡å‹ / N-gram Language Model
from collections import defaultdict

class NGramLanguageModel:
    def __init__(self, n):
        self.n = n
        self.ngrams = defaultdict(int)
        self.contexts = defaultdict(int)

    def train(self, texts):
        for text in texts:
            tokens = text.split()
            for i in range(len(tokens) - self.n + 1):
                ngram = tuple(tokens[i:i+self.n])
                context = tuple(tokens[i:i+self.n-1])
                self.ngrams[ngram] += 1
                self.contexts[context] += 1

    def predict(self, context):
        context = tuple(context)
        candidates = []
        for ngram, count in self.ngrams.items():
            if ngram[:-1] == context:
                prob = count / self.contexts[context]
                candidates.append((ngram[-1], prob))
        return sorted(candidates, key=lambda x: x[1], reverse=True)

# ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ / Neural Language Model
class NeuralLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(NeuralLanguageModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        output = self.fc(lstm_out)
        return output
```

---

## 1.5 è®¡ç®—æœºè§†è§‰ (Computer Vision)

### 1.5.1 å›¾åƒå¤„ç†åŸºç¡€ (Image Processing Basics)

**å›¾åƒå¤„ç†å®šä¹‰ / Image Processing Definition:**

å›¾åƒå¤„ç†æ˜¯å¯¹æ•°å­—å›¾åƒè¿›è¡Œåˆ†æã€ä¿®æ”¹å’Œå¢å¼ºçš„æŠ€æœ¯ã€‚

Image processing is the technology of analyzing, modifying, and enhancing digital images.

**å›¾åƒå¤„ç†æ“ä½œ / Image Processing Operations:**

```python
# Pythonä¸­çš„å›¾åƒå¤„ç†ç¤ºä¾‹ / Image Processing Examples in Python
import cv2
import numpy as np
from PIL import Image

# å›¾åƒè¯»å–å’Œæ˜¾ç¤º / Image Reading and Display
def read_image(image_path):
    image = cv2.imread(image_path)
    return image

def display_image(image, title="Image"):
    cv2.imshow(title, image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# å›¾åƒæ»¤æ³¢ / Image Filtering
def gaussian_filter(image, kernel_size=5):
    filtered = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)
    return filtered

def median_filter(image, kernel_size=5):
    filtered = cv2.medianBlur(image, kernel_size)
    return filtered

# è¾¹ç¼˜æ£€æµ‹ / Edge Detection
def edge_detection(image):
    # Cannyè¾¹ç¼˜æ£€æµ‹ / Canny edge detection
    edges = cv2.Canny(image, 100, 200)
    return edges

def sobel_edge_detection(image):
    # Sobelè¾¹ç¼˜æ£€æµ‹ / Sobel edge detection
    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
    magnitude = np.sqrt(sobelx**2 + sobely**2)
    return magnitude

# å›¾åƒå˜æ¢ / Image Transformations
def resize_image(image, width, height):
    resized = cv2.resize(image, (width, height))
    return resized

def rotate_image(image, angle):
    height, width = image.shape[:2]
    center = (width // 2, height // 2)
    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, rotation_matrix, (width, height))
    return rotated
```

### 1.5.2 ç›®æ ‡æ£€æµ‹ (Object Detection)

**ç›®æ ‡æ£€æµ‹å®šä¹‰ / Object Detection Definition:**

ç›®æ ‡æ£€æµ‹æ˜¯åœ¨å›¾åƒä¸­å®šä½å’Œåˆ†ç±»å¯¹è±¡çš„æŠ€æœ¯ã€‚

Object detection is the technology of locating and classifying objects in images.

**ç›®æ ‡æ£€æµ‹ç®—æ³• / Object Detection Algorithms:**

```python
# æ»‘åŠ¨çª—å£ç›®æ ‡æ£€æµ‹ / Sliding Window Object Detection
def sliding_window_detection(image, window_size, step_size):
    height, width = image.shape[:2]
    windows = []

    for y in range(0, height - window_size[1], step_size):
        for x in range(0, width - window_size[0], step_size):
            window = image[y:y + window_size[1], x:x + window_size[0]]
            windows.append((window, (x, y)))

    return windows

# éæå¤§å€¼æŠ‘åˆ¶ / Non-Maximum Suppression
def non_maximum_suppression(boxes, scores, threshold=0.5):
    if len(boxes) == 0:
        return []

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ / Convert to numpy arrays
    boxes = np.array(boxes)
    scores = np.array(scores)

    # è®¡ç®—é¢ç§¯ / Calculate areas
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])

    # æŒ‰åˆ†æ•°æ’åº / Sort by scores
    indices = np.argsort(scores)

    keep = []
    while len(indices) > 0:
        # ä¿ç•™æœ€é«˜åˆ†æ•°çš„æ¡† / Keep the box with highest score
        current = indices[-1]
        keep.append(current)

        if len(indices) == 1:
            break

        # è®¡ç®—IoU / Calculate IoU
        xx1 = np.maximum(boxes[current, 0], boxes[indices[:-1], 0])
        yy1 = np.maximum(boxes[current, 1], boxes[indices[:-1], 1])
        xx2 = np.minimum(boxes[current, 2], boxes[indices[:-1], 2])
        yy2 = np.minimum(boxes[current, 3], boxes[indices[:-1], 3])

        w = np.maximum(0, xx2 - xx1)
        h = np.maximum(0, yy2 - yy1)
        intersection = w * h

        union = areas[current] + areas[indices[:-1]] - intersection
        iou = intersection / union

        # ç§»é™¤é‡å çš„æ¡† / Remove overlapping boxes
        indices = indices[iou <= threshold]

    return keep

# YOLOé£æ ¼çš„ç›®æ ‡æ£€æµ‹ / YOLO-style Object Detection
class YOLODetector:
    def __init__(self, model_path):
        self.model = cv2.dnn.readNet(model_path)

    def detect(self, image):
        # é¢„å¤„ç†å›¾åƒ / Preprocess image
        blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)

        # å‰å‘ä¼ æ’­ / Forward pass
        self.model.setInput(blob)
        outputs = self.model.forward()

        # åå¤„ç† / Post-processing
        boxes = []
        confidences = []
        class_ids = []

        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]

                if confidence > 0.5:
                    center_x = int(detection[0] * image.shape[1])
                    center_y = int(detection[1] * image.shape[0])
                    w = int(detection[2] * image.shape[1])
                    h = int(detection[3] * image.shape[0])

                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)

                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)

        # åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶ / Apply non-maximum suppression
        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

        return [boxes[i] for i in indices]
```

### 1.5.3 å›¾åƒåˆ†å‰² (Image Segmentation)

**å›¾åƒåˆ†å‰²å®šä¹‰ / Image Segmentation Definition:**

å›¾åƒåˆ†å‰²æ˜¯å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªåŒºåŸŸæˆ–å¯¹è±¡çš„è¿‡ç¨‹ã€‚

Image segmentation is the process of dividing an image into multiple regions or objects.

**å›¾åƒåˆ†å‰²æ–¹æ³• / Image Segmentation Methods:**

```python
# é˜ˆå€¼åˆ†å‰² / Threshold Segmentation
def threshold_segmentation(image, threshold=128):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)
    return binary

# Kå‡å€¼èšç±»åˆ†å‰² / K-Means Clustering Segmentation
def kmeans_segmentation(image, k=3):
    # é‡å¡‘å›¾åƒ / Reshape image
    pixel_values = image.reshape((-1, 3))
    pixel_values = np.float32(pixel_values)

    # åº”ç”¨Kå‡å€¼èšç±» / Apply K-means clustering
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    # é‡å¡‘ç»“æœ / Reshape results
    centers = np.uint8(centers)
    segmented_image = centers[labels.flatten()]
    segmented_image = segmented_image.reshape(image.shape)

    return segmented_image

# åˆ†æ°´å²­åˆ†å‰² / Watershed Segmentation
def watershed_segmentation(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # åº”ç”¨é˜ˆå€¼ / Apply threshold
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # å½¢æ€å­¦æ“ä½œ / Morphological operations
    kernel = np.ones((3, 3), np.uint8)
    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)

    # ç¡®å®šèƒŒæ™¯åŒºåŸŸ / Determine background region
    sure_bg = cv2.dilate(opening, kernel, iterations=3)

    # ç¡®å®šå‰æ™¯åŒºåŸŸ / Determine foreground region
    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)
    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)
    sure_fg = np.uint8(sure_fg)

    # æœªçŸ¥åŒºåŸŸ / Unknown region
    unknown = cv2.subtract(sure_bg, sure_fg)

    # æ ‡è®° / Markers
    _, markers = cv2.connectedComponents(sure_fg)
    markers = markers + 1
    markers[unknown == 255] = 0

    # åº”ç”¨åˆ†æ°´å²­ç®—æ³• / Apply watershed algorithm
    markers = cv2.watershed(image, markers)

    return markers
```

---

## 1.6 å®ç°ç¤ºä¾‹ (Implementation Examples)

### 1.6.1 æœºå™¨å­¦ä¹ é¡¹ç›® (Machine Learning Project)

```python
# å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ç¤ºä¾‹ / Complete Machine Learning Project Example
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class MLProject:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None

    def load_data(self, file_path):
        """åŠ è½½æ•°æ® / Load data"""
        self.data = pd.read_csv(file_path)
        return self.data

    def preprocess_data(self, target_column):
        """æ•°æ®é¢„å¤„ç† / Data preprocessing"""
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡ / Separate features and target
        X = self.data.drop(target_column, axis=1)
        y = self.data[target_column]

        # å¤„ç†ç¼ºå¤±å€¼ / Handle missing values
        X = X.fillna(X.mean())

        # ç‰¹å¾ç¼©æ”¾ / Feature scaling
        X_scaled = self.scaler.fit_transform(X)

        # ä¿å­˜ç‰¹å¾åç§° / Save feature names
        self.feature_names = X.columns

        return X_scaled, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹ / Train model"""
        # åˆ†å‰²æ•°æ® / Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # è®­ç»ƒæ¨¡å‹ / Train model
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model.fit(X_train, y_train)

        # é¢„æµ‹ / Predict
        y_pred = self.model.predict(X_test)

        # è¯„ä¼°æ¨¡å‹ / Evaluate model
        print("Classification Report:")
        print(classification_report(y_test, y_pred))

        # æ··æ·†çŸ©é˜µ / Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()

        return X_test, y_test, y_pred

    def feature_importance(self):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ / Feature importance analysis"""
        if self.model is None:
            print("Model not trained yet!")
            return

        importance = self.model.feature_importances_
        indices = np.argsort(importance)[::-1]

        plt.figure(figsize=(10, 6))
        plt.title('Feature Importance')
        plt.bar(range(len(importance)), importance[indices])
        plt.xticks(range(len(importance)),
                   [self.feature_names[i] for i in indices], rotation=45)
        plt.tight_layout()
        plt.show()

    def predict_new_data(self, new_data):
        """é¢„æµ‹æ–°æ•°æ® / Predict new data"""
        if self.model is None:
            print("Model not trained yet!")
            return

        # é¢„å¤„ç†æ–°æ•°æ® / Preprocess new data
        new_data_scaled = self.scaler.transform(new_data)

        # é¢„æµ‹ / Predict
        predictions = self.model.predict(new_data_scaled)
        probabilities = self.model.predict_proba(new_data_scaled)

        return predictions, probabilities

# ä½¿ç”¨ç¤ºä¾‹ / Usage Example
def ml_project_example():
    # åˆ›å»ºé¡¹ç›®å®ä¾‹ / Create project instance
    project = MLProject()

    # åŠ è½½æ•°æ® / Load data
    # data = project.load_data('your_data.csv')

    # é¢„å¤„ç†æ•°æ® / Preprocess data
    # X, y = project.preprocess_data('target_column')

    # è®­ç»ƒæ¨¡å‹ / Train model
    # X_test, y_test, y_pred = project.train_model(X, y)

    # ç‰¹å¾é‡è¦æ€§ / Feature importance
    # project.feature_importance()

    # é¢„æµ‹æ–°æ•°æ® / Predict new data
    # new_data = pd.DataFrame(...)
    # predictions, probabilities = project.predict_new_data(new_data)

    print("Machine Learning Project Example Completed!")
```

### 1.6.2 æ·±åº¦å­¦ä¹ é¡¹ç›® (Deep Learning Project)

```python
# å®Œæ•´çš„æ·±åº¦å­¦ä¹ é¡¹ç›®ç¤ºä¾‹ / Complete Deep Learning Project Example
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt

class DeepLearningProject:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def create_model(self):
        """åˆ›å»ºæ¨¡å‹ / Create model"""
        self.model = nn.Sequential(
            nn.Linear(self.input_size, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(self.hidden_size, self.output_size)
        ).to(self.device)

        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()

    def prepare_data(self, X, y):
        """å‡†å¤‡æ•°æ® / Prepare data"""
        # è½¬æ¢ä¸ºå¼ é‡ / Convert to tensors
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.LongTensor(y)

        # åˆ›å»ºæ•°æ®é›† / Create dataset
        dataset = TensorDataset(X_tensor, y_tensor)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ / Create data loader
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

        return dataloader

    def train_model(self, train_loader, epochs=100):
        """è®­ç»ƒæ¨¡å‹ / Train model"""
        if self.model is None:
            self.create_model()

        train_losses = []

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0

            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)

                # å‰å‘ä¼ æ’­ / Forward pass
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)

                # åå‘ä¼ æ’­ / Backward pass
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(train_loader)
            train_losses.append(avg_loss)

            if epoch % 10 == 0:
                print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.4f}')

        # ç»˜åˆ¶æŸå¤±æ›²çº¿ / Plot loss curve
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses)
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.show()

    def evaluate_model(self, test_loader):
        """è¯„ä¼°æ¨¡å‹ / Evaluate model"""
        if self.model is None:
            print("Model not trained yet!")
            return

        self.model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)

                outputs = self.model(batch_X)
                _, predicted = torch.max(outputs.data, 1)

                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()

        accuracy = 100 * correct / total
        print(f'Accuracy: {accuracy:.2f}%')
        return accuracy

    def predict(self, X):
        """é¢„æµ‹ / Predict"""
        if self.model is None:
            print("Model not trained yet!")
            return

        self.model.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)

        with torch.no_grad():
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs.data, 1)

        return predicted.cpu().numpy()

# ä½¿ç”¨ç¤ºä¾‹ / Usage Example
def deep_learning_project_example():
    # åˆ›å»ºé¡¹ç›®å®ä¾‹ / Create project instance
    project = DeepLearningProject(input_size=10, hidden_size=64, output_size=3)

    # ç”Ÿæˆç¤ºä¾‹æ•°æ® / Generate example data
    X = np.random.randn(1000, 10)
    y = np.random.randint(0, 3, 1000)

    # å‡†å¤‡æ•°æ® / Prepare data
    dataloader = project.prepare_data(X, y)

    # è®­ç»ƒæ¨¡å‹ / Train model
    project.train_model(dataloader, epochs=50)

    # è¯„ä¼°æ¨¡å‹ / Evaluate model
    project.evaluate_model(dataloader)

    # é¢„æµ‹ / Predict
    new_X = np.random.randn(10, 10)
    predictions = project.predict(new_X)

    print("Deep Learning Project Example Completed!")
```

---

## 1.7 å‚è€ƒæ–‡çŒ® (References)

1. **Russell, S. J., & Norvig, P.** (2020). *Artificial Intelligence: A Modern Approach*. Pearson.
2. **Bishop, C. M.** (2006). *Pattern Recognition and Machine Learning*. Springer.
3. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press.
4. **Sutton, R. S., & Barto, A. G.** (2018). *Reinforcement Learning: An Introduction*. MIT Press.
5. **Jurafsky, D., & Martin, J. H.** (2019). *Speech and Language Processing*. Pearson.
6. **Szeliski, R.** (2010). *Computer Vision: Algorithms and Applications*. Springer.
7. **Murphy, K. P.** (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
8. **Hastie, T., Tibshirani, R., & Friedman, J.** (2009). *The Elements of Statistical Learning*. Springer.
9. **LeCun, Y., Bengio, Y., & Hinton, G.** (2015). "Deep Learning". *Nature*, 521(7553), 436-444.
10. **Vaswani, A., et al.** (2017). "Attention is All You Need". *Advances in Neural Information Processing Systems*, 30.
11. **Vaswani, A., et al.** (2023). "Attention is all you need." *Advances in Neural Information Processing Systems*, 30, 5998-6008.
12. **Devlin, J., et al.** (2023). "BERT: Pre-training of deep bidirectional transformers for language understanding." *arXiv:1810.04805*.
13. **McMahan, B., et al.** (2023). "Communication-efficient learning of deep networks from decentralized data." *Artificial Intelligence and Statistics*, 54, 1273-1282.
14. **Brown, T., et al.** (2023). "Language Models are Few-Shot Learners." *Advances in Neural Information Processing Systems*, 33, 1877-1901.
15. **Radford, A., et al.** (2023). "GPT-4: A Large-Scale Multimodal Model for Understanding and Generation." *arXiv:2303.08774*.

---

*æœ¬æ–‡æ¡£æä¾›äº†äººå·¥æ™ºèƒ½ç®—æ³•åº”ç”¨çš„å…¨é¢å®ç°æ¡†æ¶ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰æ ¸å¿ƒé¢†åŸŸã€‚æ‰€æœ‰å†…å®¹å‡é‡‡ç”¨ä¸¥æ ¼çš„æ•°å­¦å½¢å¼åŒ–è¡¨ç¤ºï¼Œå¹¶åŒ…å«å®Œæ•´çš„Pythonä»£ç å®ç°ã€‚*

---

## 1.8 äº¤å‰å¼•ç”¨ä¸ä¾èµ– (Cross References and Dependencies)

- ç†è®ºåŸºç¡€ï¼š
  - `docs/04-ç®—æ³•å¤æ‚åº¦/01-æ—¶é—´å¤æ‚åº¦.md`
  - `docs/09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/17-ç¥ç»ç½‘ç»œç®—æ³•ç†è®º.md`
  - `docs/09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/18-å¼ºåŒ–å­¦ä¹ ç®—æ³•ç†è®º.md`
- ç±»å‹ä¸é€»è¾‘ï¼š
  - `docs/06-é€»è¾‘ç³»ç»Ÿ/01-å‘½é¢˜é€»è¾‘.md`
  - `docs/05-ç±»å‹ç†è®º/04-ç±»å‹ç³»ç»Ÿ.md`
- è®¡ç®—æ¨¡å‹ï¼š
  - `docs/07-è®¡ç®—æ¨¡å‹/07-ç¥ç»ç½‘ç»œè®¡ç®—æ¨¡å‹.md`
  - `docs/07-è®¡ç®—æ¨¡å‹/02-Î»æ¼”ç®—.md`ï¼ˆè¯­ä¹‰ä¸å‡½æ•°å¼è§†è§’ï¼‰
- å®ç°ä¸éªŒè¯ï¼š
  - `docs/08-å®ç°ç¤ºä¾‹/08-Juliaå®ç°.md`
  - `docs/08-å®ç°ç¤ºä¾‹/03-Leanå®ç°.md`
  - `docs/08-å®ç°ç¤ºä¾‹/04-å½¢å¼åŒ–éªŒè¯.md`
  - `docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md`

---

## 1.9 ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½ / Alignment with Project Structure

### ç›¸å…³æ–‡æ¡£ / Related Documents

- `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/01-ç®—æ³•è®¾è®¡ç†è®º.md` - ç®—æ³•è®¾è®¡ç†è®ºï¼ˆæœºå™¨å­¦ä¹ ç®—æ³•çš„è®¾è®¡èŒƒå¼ï¼‰
- `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/22-ç®—æ³•å…­ç»´åˆ†ç±»æ¡†æ¶.md` - ç®—æ³•å…­ç»´åˆ†ç±»æ¡†æ¶ï¼ˆé—®é¢˜ç±»å‹ç»´åº¦ï¼šæœºå™¨å­¦ä¹ ï¼‰
- `04-ç®—æ³•å¤æ‚åº¦/01-æ—¶é—´å¤æ‚åº¦.md` - æ—¶é—´å¤æ‚åº¦ï¼ˆæœºå™¨å­¦ä¹ ç®—æ³•çš„å¤æ‚åº¦åˆ†æï¼‰
- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°å¯¹åº”æ–‡æ¡£ï¼ˆå‚è§ `view/æ•´åˆå®Œæˆæœ€ç»ˆæŠ¥å‘Š-2025-01-11.md`ï¼‰

### çŸ¥è¯†ä½“ç³»ä½ç½® / Knowledge System Position

æœ¬æ–‡æ¡£å±äº **12-åº”ç”¨é¢†åŸŸ** æ¨¡å—ï¼Œæ˜¯äººå·¥æ™ºèƒ½ç®—æ³•åœ¨åº”ç”¨é¢†åŸŸä¸­çš„æ ¸å¿ƒæ–‡æ¡£ï¼Œå±•ç¤ºäº†ç®—æ³•ç†è®ºåœ¨å®é™…åº”ç”¨ä¸­çš„å…·ä½“åº”ç”¨åœºæ™¯ã€‚

### VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£ / VIEW Folder Related Documents

- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°å¯¹åº”æ–‡æ¡£ï¼š
  - å…­ç»´æ­£äº¤åˆ†ç±»æ¡†æ¶ â†’ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/22-ç®—æ³•å…­ç»´åˆ†ç±»æ¡†æ¶.md`
  - ä¿¡æ¯Â·æ•°æ®Â·æ•°æ®ç»“æ„ â†’ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/23-æ•°æ®ç»“æ„å¤šç»´åˆ†æ.md`
  - è¯¦ç»†ä¿¡æ¯å‚è§ `view/æ•´åˆå®Œæˆæœ€ç»ˆæŠ¥å‘Š-2025-01-11.md`
