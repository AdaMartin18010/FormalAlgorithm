---
title: 10.30 è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ / Algorithm Systems in Edge Computing
version: 1.0
status: maintained
last_updated: 2025-01-11
owner: é«˜çº§ä¸»é¢˜å·¥ä½œç»„
---

> ğŸ“Š **é¡¹ç›®å…¨é¢æ¢³ç†**ï¼šè¯¦ç»†çš„é¡¹ç›®ç»“æ„ã€æ¨¡å—è¯¦è§£å’Œå­¦ä¹ è·¯å¾„ï¼Œè¯·å‚é˜… [`é¡¹ç›®å…¨é¢æ¢³ç†-2025.md`](../é¡¹ç›®å…¨é¢æ¢³ç†-2025.md)

## 10.30 è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ / Algorithm Systems in Edge Computing

> è¯´æ˜ï¼šæœ¬æ–‡æ¡£ä¸­çš„ä»£ç /ä¼ªä»£ç ä¸ºè¯´æ˜æ€§ç‰‡æ®µï¼Œä»…ç”¨äºç†è®ºé˜é‡Šï¼›æœ¬ä»“åº“ä¸æä¾›å¯è¿è¡Œå·¥ç¨‹æˆ– CIã€‚

### æ‘˜è¦ / Executive Summary

- ç»Ÿä¸€è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿï¼Œç ”ç©¶å¦‚ä½•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ™ºèƒ½ç®—æ³•ã€‚
- å»ºç«‹è¾¹ç¼˜è®¡ç®—ç®—æ³•ç³»ç»Ÿåœ¨é«˜çº§ä¸»é¢˜ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚

### å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary

- è¾¹ç¼˜è®¡ç®—ã€è¾¹ç¼˜è®¾å¤‡ã€èµ„æºçº¦æŸã€ä»»åŠ¡è°ƒåº¦ã€èµ„æºåˆ†é…ã€è¾¹ç¼˜-äº‘ååŒã€éšç§ä¿æŠ¤ã€‚
- æœ¯è¯­å¯¹é½ä¸å¼•ç”¨è§„èŒƒï¼š`docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md`ï¼Œ`01-åŸºç¡€ç†è®º/00-æ’°å†™è§„èŒƒä¸å¼•ç”¨æŒ‡å—.md`

### æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ / Terminology & Notation

- è¾¹ç¼˜è®¡ç®—ï¼ˆEdge Computingï¼‰ï¼šåœ¨è®¾å¤‡è¾¹ç¼˜è¿›è¡Œè®¡ç®—çš„æ¨¡å¼ã€‚
- è¾¹ç¼˜è®¾å¤‡ï¼ˆEdge Deviceï¼‰ï¼šéƒ¨ç½²åœ¨è¾¹ç¼˜çš„è®¡ç®—è®¾å¤‡ã€‚
- èµ„æºçº¦æŸï¼ˆResource Constraintï¼‰ï¼šè¾¹ç¼˜è®¾å¤‡çš„èµ„æºé™åˆ¶ã€‚
- ä»»åŠ¡è°ƒåº¦ï¼ˆTask Schedulingï¼‰ï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè°ƒåº¦ä»»åŠ¡çš„æ–¹æ³•ã€‚
- è®°å·çº¦å®šï¼š`E` è¡¨ç¤ºè¾¹ç¼˜è®¾å¤‡ï¼Œ`T` è¡¨ç¤ºä»»åŠ¡ï¼Œ`R` è¡¨ç¤ºèµ„æºï¼Œ`S` è¡¨ç¤ºè°ƒåº¦ã€‚

### äº¤å‰å¼•ç”¨å¯¼èˆª / Cross-References

- ç‰©è”ç½‘ç®—æ³•ï¼šå‚è§ `12-åº”ç”¨é¢†åŸŸ/07-ç‰©è”ç½‘ç®—æ³•åº”ç”¨.md`ã€‚
- åˆ†å¸ƒå¼ç®—æ³•ï¼šå‚è§ `09-ç®—æ³•ç†è®º/03-ä¼˜åŒ–ç†è®º/03-åˆ†å¸ƒå¼ç®—æ³•ç†è®º.md`ã€‚
- åœ¨çº¿ç®—æ³•ï¼šå‚è§ `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/13-åœ¨çº¿ç®—æ³•ç†è®º.md`ã€‚
- é¡¹ç›®å¯¼èˆªä¸å¯¹æ ‡ï¼šè§ [é¡¹ç›®å…¨é¢æ¢³ç†-2025](../é¡¹ç›®å…¨é¢æ¢³ç†-2025.md)ã€[é¡¹ç›®æ‰©å±•ä¸æŒç»­æ¨è¿›ä»»åŠ¡ç¼–æ’](../é¡¹ç›®æ‰©å±•ä¸æŒç»­æ¨è¿›ä»»åŠ¡ç¼–æ’.md)ã€[å›½é™…è¯¾ç¨‹å¯¹æ ‡è¡¨](../å›½é™…è¯¾ç¨‹å¯¹æ ‡è¡¨.md)ã€‚

### å¿«é€Ÿå¯¼èˆª / Quick Links

- åŸºæœ¬æ¦‚å¿µ
- ä»»åŠ¡è°ƒåº¦
- èµ„æºåˆ†é…

## ç›®å½• (Table of Contents)

- [10.30 è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ / Algorithm Systems in Edge Computing](#1030-è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ--algorithm-systems-in-edge-computing)

## æ¦‚è¿° / Overview

è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿç ”ç©¶å¦‚ä½•åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ™ºèƒ½ç®—æ³•ï¼Œå®ç°ä½å»¶è¿Ÿã€é«˜å¯é ã€éšç§ä¿æŠ¤çš„åˆ†å¸ƒå¼æ™ºèƒ½è®¡ç®—ã€‚

## å­¦ä¹ ç›®æ ‡ / Learning Objectives

1. **åŸºç¡€çº§** ç†è§£è¾¹ç¼˜è®¡ç®—æ¶æ„ä¸èµ„æºçº¦æŸä¸‹çš„ç®—æ³•è®¾è®¡
2. **è¿›é˜¶çº§** æŒæ¡ä»»åŠ¡è°ƒåº¦ä¸èµ„æºåˆ†é…ç®—æ³•
3. **è¿›é˜¶çº§** èƒ½å¤Ÿè®¾è®¡è¾¹ç¼˜-äº‘ååŒçš„ç®—æ³•æ¡†æ¶
4. **é«˜çº§çº§** äº†è§£è¾¹ç¼˜è®¡ç®—ä¸­çš„éšç§ä¿æŠ¤ä¸å®‰å…¨æœºåˆ¶
5. **é«˜çº§çº§** æŒæ¡è¾¹ç¼˜æ™ºèƒ½åœ¨ç‰©è”ç½‘ä¸å®æ—¶ç³»ç»Ÿä¸­çš„åº”ç”¨

## æœ¯è¯­ä¸å®šä¹‰

| æœ¯è¯­ | è‹±æ–‡ | å®šä¹‰ |
|------|------|------|
| è¾¹ç¼˜è®¡ç®— | Edge Computing | åœ¨æ•°æ®æºé™„è¿‘è¿›è¡Œæ•°æ®å¤„ç†å’Œè®¡ç®—çš„è®¡ç®—æ¨¡å¼ |
| ç«¯è®¾å¤‡ | Edge Device | ä½äºç½‘ç»œè¾¹ç¼˜çš„è®¡ç®—è®¾å¤‡ï¼Œå¦‚ä¼ æ„Ÿå™¨ã€æ‘„åƒå¤´ç­‰ |
| è¾¹ç¼˜èŠ‚ç‚¹ | Edge Node | ä½äºè¾¹ç¼˜ç½‘ç»œä¸­çš„è®¡ç®—èŠ‚ç‚¹ï¼Œæä¾›æœ¬åœ°å¤„ç†èƒ½åŠ› |
| åœ¨ç«¯å­¦ä¹  | On-device Learning | åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ›´æ–°çš„è¿‡ç¨‹ |
| æ¨¡å‹å‹ç¼© | Model Compression | å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦çš„æŠ€æœ¯ |
| ä»»åŠ¡å¸è½½ | Task Offloading | å°†è®¡ç®—ä»»åŠ¡ä»è¾¹ç¼˜è®¾å¤‡è½¬ç§»åˆ°äº‘ç«¯çš„è¿‡ç¨‹ |
| èµ„æºæ„ŸçŸ¥ | Resource Awareness | ç®—æ³•å¯¹è®¡ç®—èµ„æºçŠ¶æ€çš„æ„ŸçŸ¥å’Œé€‚åº”èƒ½åŠ› |
| è‡ªé€‚åº”æ¨ç† | Adaptive Inference | æ ¹æ®èµ„æºçº¦æŸåŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥çš„æ–¹æ³• |
| è¾¹ç¼˜ååŒ | Edge Collaboration | å¤šä¸ªè¾¹ç¼˜èŠ‚ç‚¹ä¹‹é—´çš„åä½œå’Œèµ„æºå…±äº« |
| æ—¶å»¶æ•æ„Ÿ | Latency Sensitive | å¯¹å“åº”æ—¶é—´æœ‰ä¸¥æ ¼è¦æ±‚çš„åº”ç”¨åœºæ™¯ |

### å†…å®¹è¡¥å……ä¸æ€ç»´è¡¨å¾ / Content Supplement and Thinking Representation

> æœ¬èŠ‚æŒ‰ [å†…å®¹è¡¥å……ä¸æ€ç»´è¡¨å¾å…¨é¢è®¡åˆ’æ–¹æ¡ˆ](../å†…å®¹è¡¥å……ä¸æ€ç»´è¡¨å¾å…¨é¢è®¡åˆ’æ–¹æ¡ˆ.md) **åªè¡¥å……ã€ä¸åˆ é™¤**ã€‚æ ‡å‡†è§ [å†…å®¹è¡¥å……æ ‡å‡†](../å†…å®¹è¡¥å……æ ‡å‡†-æ¦‚å¿µå®šä¹‰å±æ€§å…³ç³»è§£é‡Šè®ºè¯å½¢å¼è¯æ˜.md)ã€[æ€ç»´è¡¨å¾æ¨¡æ¿é›†](../æ€ç»´è¡¨å¾æ¨¡æ¿é›†.md)ã€‚

#### è§£é‡Šä¸ç›´è§‚ / Explanation and Intuition

è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿå°†æœ¯è¯­ä¸å®šä¹‰ä¸ç³»ç»Ÿæ¶æ„ã€åœ¨ç«¯å­¦ä¹ ã€æ¨¡å‹å‹ç¼©ä¸åˆ†å‘ã€èµ„æºæ„ŸçŸ¥ä¸è¾¹ç¼˜ååŒç»“åˆã€‚ä¸ 10-30 é«˜çº§æ·±åŒ–ã€10-37 è¾¹ç¼˜æ™ºèƒ½è¡”æ¥ï¼›Â§æœ¯è¯­ä¸å®šä¹‰ã€Â§ç³»ç»Ÿæ¶æ„ã€å„èŠ‚å½¢æˆå®Œæ•´è¡¨å¾ã€‚

#### æ¦‚å¿µå±æ€§è¡¨ / Concept Attribute Table

| å±æ€§å | ç±»å‹/èŒƒå›´ | å«ä¹‰ | å¤‡æ³¨ |
|--------|-----------|------|------|
| æœ¯è¯­ä¸å®šä¹‰ | åŸºæœ¬æ¦‚å¿µ | Â§æœ¯è¯­ä¸å®šä¹‰ | ä¸ 10-37 å¯¹ç…§ |
| ç³»ç»Ÿæ¶æ„ã€åœ¨ç«¯å­¦ä¹ ã€æ¨¡å‹å‹ç¼©ä¸åˆ†å‘ã€èµ„æºæ„ŸçŸ¥ä¸é²æ£’æ€§ã€è¾¹ç¼˜ååŒä¸è°ƒåº¦ | æ¶æ„/ç®—æ³• | å»¶è¿Ÿã€å¸¦å®½ã€éšç§ | Â§å„èŠ‚ |
| åœ¨ç«¯å­¦ä¹ /æ¨¡å‹å‹ç¼©/èµ„æºæ„ŸçŸ¥ | å¯¹æ¯” | Â§å„èŠ‚ | å¤šç»´çŸ©é˜µ |

#### æ¦‚å¿µå…³ç³» / Concept Relations

| æºæ¦‚å¿µ | ç›®æ ‡æ¦‚å¿µ | å…³ç³»ç±»å‹ | è¯´æ˜ |
|--------|----------|----------|------|
| è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ | 10-30 é«˜çº§æ·±åŒ–ã€10-37 | depends_on | è¾¹ç¼˜æ·±åŒ–ä¸æ™ºèƒ½è¡”æ¥ |
| è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ | 12 åº”ç”¨é¢†åŸŸ | applies_to | è¾¹ç¼˜å®è·µ |

#### æ¦‚å¿µä¾èµ–å›¾ / Concept Dependency Graph

```mermaid
graph LR
  Term[æœ¯è¯­ä¸å®šä¹‰ Â§æœ¯è¯­ä¸å®šä¹‰]
  Arch[ç³»ç»Ÿæ¶æ„ Â§ç³»ç»Ÿæ¶æ„]
  Other[åœ¨ç«¯å­¦ä¹ /æ¨¡å‹å‹ç¼©/èµ„æºæ„ŸçŸ¥ Â§å„èŠ‚]
  Collab[è¾¹ç¼˜ååŒä¸è°ƒåº¦ Â§è¾¹ç¼˜ååŒä¸è°ƒåº¦]
  Term --> Arch
  Arch --> Other
  Other --> Collab
  10_37[10-37]
  Term --> 10_37
```

#### è®ºè¯ä¸è¯æ˜è¡”æ¥ / Argumentation and Proof Link

ä»»åŠ¡è°ƒåº¦æ­£ç¡®æ€§è§ Â§ç³»ç»Ÿæ¶æ„ï¼›åœ¨ç«¯å­¦ä¹ æ”¶æ•›æ€§è§ Â§åœ¨ç«¯å­¦ä¹ ï¼›ä¸ 10-37 è®ºè¯è¡”æ¥ã€‚

#### æ€ç»´å¯¼å›¾ï¼šæœ¬ç« æ¦‚å¿µç»“æ„ / Mind Map

```mermaid
graph TD
  Edge[è¾¹ç¼˜è®¡ç®—ä¸­çš„ç®—æ³•ç³»ç»Ÿ]
  Edge --> Term[æœ¯è¯­ä¸å®šä¹‰]
  Edge --> Arch[ç³»ç»Ÿæ¶æ„]
  Edge --> OnDev[åœ¨ç«¯å­¦ä¹ ]
  Edge --> Comp[æ¨¡å‹å‹ç¼©]
  Edge --> Res[èµ„æºæ„ŸçŸ¥]
  Edge --> Collab[è¾¹ç¼˜ååŒ]
```

#### å¤šç»´çŸ©é˜µï¼šè¾¹ç¼˜ç®—æ³•å¯¹æ¯” / Multi-Dimensional Comparison

| æ¦‚å¿µ/æŠ€æœ¯ | å»¶è¿Ÿ | å¸¦å®½ | éšç§ | å¤‡æ³¨ |
|-----------|------|------|------|------|
| åœ¨ç«¯å­¦ä¹ /æ¨¡å‹å‹ç¼©/èµ„æºæ„ŸçŸ¥ | Â§å„èŠ‚ | Â§å„èŠ‚ | Â§å„èŠ‚ | â€” |

#### å†³ç­–æ ‘ï¼šåœºæ™¯åˆ°æŠ€æœ¯é€‰æ‹© / Decision Tree

```mermaid
flowchart TD
  Start([åœºæ™¯])
  Start --> Scene{åœºæ™¯?}
  Scene -->|ä½å»¶è¿Ÿ/éšç§/èµ„æºå—é™| Tech[åœ¨ç«¯å­¦ä¹ æˆ–æ¨¡å‹å‹ç¼©æˆ–ä»»åŠ¡å¸è½½ Â§å„èŠ‚]
  Tech --> App[Â§æ¡ˆä¾‹]
```

#### å…¬ç†å®šç†æ¨ç†è¯æ˜å†³ç­–æ ‘ / Axiom-Theorem-Proof Tree

```mermaid
graph LR
  Ax[è¾¹ç¼˜è®¡ç®—å…¬è®¾ Â§æœ¯è¯­ä¸å®šä¹‰]
  Arch[ç³»ç»Ÿæ¶æ„æ­£ç¡®æ€§ Â§ç³»ç»Ÿæ¶æ„]
  Algo[å„ç®—æ³•æ­£ç¡®æ€§ Â§å„èŠ‚]
  Ax --> Arch
  Arch --> Algo
```

#### åº”ç”¨å†³ç­–å»ºæ¨¡æ ‘ / Application Decision Modeling Tree

```mermaid
flowchart TD
  Need([åº”ç”¨éœ€æ±‚])
  Need --> App{éœ€æ±‚ç±»å‹?}
  App -->|ç«¯/è¾¹/äº‘éƒ¨ç½²| Meth[åœ¨ç«¯å­¦ä¹ ã€æ¨¡å‹å‹ç¼©æˆ–è¾¹ç¼˜ååŒ Â§æ¡ˆä¾‹]
  Meth --> Impl[Â§æ¡ˆä¾‹]
```

## ç³»ç»Ÿæ¶æ„

- å±‚æ¬¡: ç«¯è®¾å¤‡(Edge Device) â€” è¾¹ç¼˜èŠ‚ç‚¹(Edge Node) â€” åŒºåŸŸæ±‡èš(Aggregator) â€” äº‘(Cloud)
- æ•°æ®æµ: é‡‡é›†â†’é¢„å¤„ç†â†’æœ¬åœ°æ¨ç†/è®­ç»ƒâ†’åä½œåŒæ­¥â†’èšåˆ

```rust
// ä»»åŠ¡æŠ½è±¡ä¸è°ƒåº¦
pub struct EdgeTask { id: String, deadline_ms: u64, size_bytes: u64, priority: u8, model_id: String }

pub trait EdgeScheduler { fn schedule(&self, tasks: &[EdgeTask], resources: &EdgeResources) -> SchedulePlan }

pub struct EDFScheduler;
impl EdgeScheduler for EDFScheduler { fn schedule(&self, tasks: &[EdgeTask], _r: &EdgeResources) -> SchedulePlan { /* earliest-deadline-first */ SchedulePlan::new(tasks) } }
```

## åœ¨ç«¯å­¦ä¹  (On-device / On-edge Learning)

- å¢é‡/åœ¨çº¿å­¦ä¹ ã€è’¸é¦å­¦ä¹ ã€å°‘æ ·æœ¬å¾®è°ƒ
- è”é‚¦å­¦ä¹ åœ¨è¾¹ç¼˜çš„èµ„æºçº¦æŸï¼šç®—åŠ›ã€èƒ½è€—ã€ç½‘ç»œ

```rust
pub struct EdgeLearner { optimizer: EdgeOptimizer, budget: EnergyBudget }
impl EdgeLearner { pub fn train_step(&mut self, batch: &MiniBatch) -> TrainStat { /* é¢„ç®—çº¦æŸä¸‹æ¢¯åº¦æ­¥ */ TrainStat::default() } }
```

## æ¨¡å‹å‹ç¼©ä¸åˆ†å‘

- é‡åŒ–(PTQ/QAT)ã€å‰ªæã€è’¸é¦ã€ç»“æ„é‡å‚æ•°åŒ–
- åˆ†å‘: åˆ‡ç‰‡åˆ†å‘ã€å·®åˆ†æ›´æ–°ã€åˆ†é˜¶æ®µç°åº¦

```rust
pub struct ModelDistributor { delta_encoder: DeltaEncoder, rollout: RolloutStrategy }
impl ModelDistributor { pub fn rollout(&self, model: &EdgeModel, fleet: &[Device]) -> RolloutPlan { /* åˆ†å±‚ç°åº¦ */ RolloutPlan::new() } }
```

## èµ„æºæ„ŸçŸ¥ä¸é²æ£’æ€§

- èµ„æºç›‘æ§: CPU/GPUã€å†…å­˜ã€åŠŸè€—ã€ç½‘ç»œ
- è‡ªé€‚åº”æ¨ç†: åŠ¨æ€æ·±åº¦ã€æ—©é€€å‡ºã€å¤šåˆ†è¾¨ç‡
- é²æ£’æ€§: å™ªå£°/æŠ–åŠ¨ã€ä¸¢åŒ…ã€æ–­è¿ã€è®¾å¤‡æ¼‚ç§»

```rust
pub struct AdaptiveInference { policies: Vec<Policy> }
impl AdaptiveInference { pub fn infer(&self, x: &Tensor, budget: &LatencyBudget) -> Output { /* æ—©åœ/å¤šåˆ†è¾¨ç‡ */ Output::default() } }
```

## è¾¹ç¼˜ååŒä¸è°ƒåº¦

- è¾¹-è¾¹ååŒ: é‚»åŸŸäº’åŠ©ã€åˆ†å¸ƒå¼ç¼“å­˜ã€ä»»åŠ¡è¿ç§»
- è¾¹-äº‘ååŒ: äº‘æ§ç¼–æ’ã€å‡½æ•°å¸è½½ã€å¼¹æ€§ä¼¸ç¼©

```rust
pub struct OffloadOrchestrator { estimator: OffloadEstimator }
impl OffloadOrchestrator { pub fn decide(&self, task: &EdgeTask, ctx: &Context) -> OffloadPlan { /* ä¼°è®¡ç«¯åˆ°ç«¯æ—¶å»¶ä¸èƒ½è€— */ OffloadPlan::local() } }
```

## åº¦é‡ä¸SLA

- å…³é”®æŒ‡æ ‡: ç«¯åˆ°ç«¯æ—¶å»¶ã€ä»»åŠ¡å®Œæˆç‡ã€èƒ½è€—/æ¯ç“¦æ€§èƒ½ã€å¯ç”¨æ€§ã€é²æ£’æ€§æŒ‡æ•°
- SLAåˆçº¦: è¿çº¦æ£€æµ‹ã€å¼¹æ€§é‡è¯•ã€è¡¥å¿ç­–ç•¥

## æ•°å­¦ä¸ä¼˜åŒ–

- ä»»åŠ¡è°ƒåº¦: \( \min \sum_i w_i T_i \) s.t. èµ„æºä¸æ—¶é™çº¦æŸ
- å¸è½½å†³ç­–: \( \min E[latency+energy] \) over partition/placement
- å‹ç¼©-ç²¾åº¦æƒè¡¡: \( \max Acc(q,p) - \lambda C(q,p) \)

## å®ç°è“å›¾

- ç«¯ä¾§: TFLite/ONNX Runtime/TVM + ç®€åŒ–è°ƒåº¦å™¨
- èŠ‚ç‚¹: éƒ¨ç½²ç¼“å­˜ã€æ»šåŠ¨å‡çº§ã€é¥æµ‹
- äº‘ä¾§: ç¼–æ’å™¨(K8s)ã€è”é‚¦åè°ƒå™¨ã€ç›‘æ§çœ‹æ¿

## æ¡ˆä¾‹

- è§†é¢‘åˆ†æ: åˆ†è¾¨ç‡è‡ªé€‚åº”+æ—©é€€å‡º
- å·¥ä¸šè§†è§‰: æ–­è¿é²æ£’æ¨ç†ä¸æœ¬åœ°é‡è¯•
- è½¦è·¯ååŒ: ä½æ—¶å»¶è·¯å¾„è§„åˆ’ä¸å¸è½½

## æ€»ç»“

è¾¹ç¼˜ç®—æ³•ç³»ç»Ÿéœ€åœ¨ä¸¥æ ¼çº¦æŸä¸‹å®ç°â€œå¤Ÿç”¨çš„æ™ºèƒ½â€ï¼Œé€šè¿‡è°ƒåº¦ã€å‹ç¼©ã€ååŒä¸è‡ªé€‚åº”ï¼Œæé«˜æ—¶å»¶æ•æ„Ÿä¸éšç§æ•æ„Ÿåœºæ™¯çš„å¯ç”¨æ€§ä¸å¯é æ€§ã€‚

## æ¶æ„å›¾ï¼ˆMermaidï¼‰

```mermaid
flowchart TB
  subgraph EdgeDevice
    S[Sensor/Camera]
    P[Preprocess]
    I[On-device Inference]
  end
  subgraph EdgeNode
    C[Cache]
    K[KV Store]
    D[Dist Scheduler]
  end
  subgraph Cloud
    O[Orchestrator]
    M[Model Repo]
    F[Federated Coordinator]
  end

  S --> P --> I
  I -->|offload?| D
  D --> O
  O --> M
  O --> F
  M --> I
```

## äº¤å‰é“¾æ¥

- å‚è§ `27-ç®—æ³•è”é‚¦å­¦ä¹ ä¸éšç§ä¿æŠ¤ç†è®º.md`
- å‚è§ `30-ç®—æ³•é²æ£’æ€§ä¸å¯¹æŠ—æ€§é˜²å¾¡ç†è®º.md`
- å‚è§ `25-ç®—æ³•å¯è§£é‡Šæ€§ä¸é€æ˜åº¦ç†è®º.md`

## ç›¸å…³æ–‡æ¡£ï¼ˆäº¤å‰é“¾æ¥ï¼‰

- `10-é«˜çº§ä¸»é¢˜/27-ç®—æ³•è”é‚¦å­¦ä¹ ä¸éšç§ä¿æŠ¤ç†è®º.md`
- `10-é«˜çº§ä¸»é¢˜/26-ç®—æ³•é²æ£’æ€§ä¸å¯¹æŠ—æ€§é˜²å¾¡ç†è®º.md`
- `09-ç®—æ³•ç†è®º/03-ä¼˜åŒ–ç†è®º/02-å¹¶è¡Œç®—æ³•ç†è®º.md`

## å‚è€ƒæ–‡çŒ®ï¼ˆç¤ºä¾‹ï¼‰

1. Satyanarayanan, M. The Emergence of Edge Computing. Computer, 2017.
2. Mao, Y. et al. A Survey on Mobile Edge Computing: The Communication Perspective. IEEE Communications Surveys & Tutorials, 2017.
3. Li, E. et al. Learning and Inferencing on the Edge: A Survey. Proceedings of the IEEE, 2018.

## å¯è¿è¡ŒRustç¤ºä¾‹éª¨æ¶

```rust
use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::time::sleep;

// è¾¹ç¼˜ä»»åŠ¡
#[derive(Clone, Debug)]
pub struct EdgeTask {
    pub id: String,
    pub deadline_ms: u64,
    pub size_bytes: u64,
    pub priority: u8,
    pub model_id: String,
    pub task_type: TaskType,
}

#[derive(Clone, Debug)]
pub enum TaskType {
    Inference,
    Training,
    Compression,
    Offload,
}

// è¾¹ç¼˜èµ„æº
#[derive(Clone, Debug)]
pub struct EdgeResources {
    pub cpu_cores: u32,
    pub memory_mb: u64,
    pub storage_gb: u64,
    pub bandwidth_mbps: u64,
    pub battery_level: f64,
}

// è¾¹ç¼˜è°ƒåº¦å™¨
pub trait EdgeScheduler {
    fn schedule(&self, tasks: &[EdgeTask], resources: &EdgeResources) -> SchedulePlan;
}

pub struct EDFScheduler;

impl EdgeScheduler for EDFScheduler {
    fn schedule(&self, tasks: &[EdgeTask], _resources: &EdgeResources) -> SchedulePlan {
        let mut sorted_tasks = tasks.to_vec();
        sorted_tasks.sort_by_key(|task| task.deadline_ms);

        SchedulePlan {
            tasks: sorted_tasks,
            estimated_completion_time: Instant::now(),
        }
    }
}

pub struct PriorityScheduler;

impl EdgeScheduler for PriorityScheduler {
    fn schedule(&self, tasks: &[EdgeTask], _resources: &EdgeResources) -> SchedulePlan {
        let mut sorted_tasks = tasks.to_vec();
        sorted_tasks.sort_by_key(|task| std::cmp::Reverse(task.priority));

        SchedulePlan {
            tasks: sorted_tasks,
            estimated_completion_time: Instant::now(),
        }
    }
}

// è¾¹ç¼˜å­¦ä¹ å™¨
pub struct EdgeLearner {
    pub optimizer: EdgeOptimizer,
    pub budget: EnergyBudget,
    pub local_data: LocalDataset,
}

impl EdgeLearner {
    pub fn new(optimizer: EdgeOptimizer, budget: EnergyBudget) -> Self {
        Self {
            optimizer,
            budget,
            local_data: LocalDataset::new(),
        }
    }

    pub async fn train_step(&mut self, batch: &MiniBatch) -> Result<TrainStat, TrainingError> {
        if !self.budget.can_afford_training() {
            return Err(TrainingError::InsufficientBudget);
        }

        let start_time = Instant::now();
        let gradients = self.compute_gradients(batch);

        // åº”ç”¨å·®åˆ†éšç§
        let noisy_gradients = self.add_differential_privacy(gradients);

        // æ›´æ–°æ¨¡å‹
        self.optimizer.update(&noisy_gradients);

        // æ¶ˆè€—é¢„ç®—
        self.budget.consume_training_energy();

        let duration = start_time.elapsed();

        Ok(TrainStat {
            loss: self.compute_loss(batch),
            accuracy: self.compute_accuracy(batch),
            duration,
            energy_consumed: self.budget.get_last_consumption(),
        })
    }

    fn compute_gradients(&self, batch: &MiniBatch) -> Vec<f64> {
        // ç®€åŒ–çš„æ¢¯åº¦è®¡ç®—
        batch.features.iter()
            .flat_map(|feature| feature.iter().map(|&x| x * 0.01))
            .collect()
    }

    fn add_differential_privacy(&self, gradients: Vec<f64>) -> Vec<f64> {
        use rand::Rng;
        let mut rng = rand::thread_rng();

        gradients.into_iter()
            .map(|g| g + rng.gen_range(-0.1..0.1))
            .collect()
    }

    fn compute_loss(&self, batch: &MiniBatch) -> f64 {
        // ç®€åŒ–çš„æŸå¤±è®¡ç®—
        batch.features.iter()
            .zip(batch.labels.iter())
            .map(|(feature, &label)| {
                let prediction = feature.iter().sum::<f64>();
                (prediction - label).powi(2)
            })
            .sum::<f64>() / batch.features.len() as f64
    }

    fn compute_accuracy(&self, batch: &MiniBatch) -> f64 {
        // ç®€åŒ–çš„å‡†ç¡®ç‡è®¡ç®—
        let correct = batch.features.iter()
            .zip(batch.labels.iter())
            .filter(|(feature, &label)| {
                let prediction = feature.iter().sum::<f64>();
                (prediction > 0.5) == (label > 0.5)
            })
            .count();

        correct as f64 / batch.features.len() as f64
    }
}

// æ¨¡å‹åˆ†å‘å™¨
pub struct ModelDistributor {
    pub delta_encoder: DeltaEncoder,
    pub rollout: RolloutStrategy,
    pub compression: ModelCompression,
}

impl ModelDistributor {
    pub fn new() -> Self {
        Self {
            delta_encoder: DeltaEncoder::new(),
            rollout: RolloutStrategy::Gradual,
            compression: ModelCompression::Quantization,
        }
    }

    pub fn distribute_model(&self, model: &EdgeModel, devices: &[EdgeDevice]) -> DistributionPlan {
        let mut plan = DistributionPlan::new();

        for device in devices {
            let compressed_model = self.compress_model(model, device);
            let delta = self.delta_encoder.encode_delta(model, &compressed_model);

            plan.add_distribution(Distribution {
                device_id: device.id.clone(),
                model_delta: delta,
                compression_ratio: self.compression.get_compression_ratio(),
                estimated_transfer_time: self.estimate_transfer_time(&delta, device),
            });
        }

        plan
    }

    fn compress_model(&self, model: &EdgeModel, device: &EdgeDevice) -> CompressedModel {
        match self.compression {
            ModelCompression::Quantization => self.quantize_model(model, 8),
            ModelCompression::Pruning => self.prune_model(model, 0.5),
            ModelCompression::Distillation => self.distill_model(model),
        }
    }

    fn quantize_model(&self, model: &EdgeModel, bits: u8) -> CompressedModel {
        let scale = (1 << (bits - 1)) as f64;
        let quantized_params = model.parameters.iter()
            .map(|&p| (p * scale).round() / scale)
            .collect();

        CompressedModel {
            parameters: quantized_params,
            compression_type: ModelCompression::Quantization,
        }
    }

    fn prune_model(&self, model: &EdgeModel, sparsity: f64) -> CompressedModel {
        let mut params = model.parameters.clone();
        let threshold = self.compute_pruning_threshold(&params, sparsity);

        for param in &mut params {
            if param.abs() < threshold {
                *param = 0.0;
            }
        }

        CompressedModel {
            parameters: params,
            compression_type: ModelCompression::Pruning,
        }
    }

    fn distill_model(&self, _model: &EdgeModel) -> CompressedModel {
        // ç®€åŒ–çš„çŸ¥è¯†è’¸é¦
        CompressedModel {
            parameters: vec![0.0; 10], // ç®€åŒ–
            compression_type: ModelCompression::Distillation,
        }
    }

    fn compute_pruning_threshold(&self, params: &[f64], sparsity: f64) -> f64 {
        let mut sorted = params.iter().map(|&p| p.abs()).collect::<Vec<_>>();
        sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let index = (sorted.len() as f64 * sparsity) as usize;
        sorted.get(index).copied().unwrap_or(0.0)
    }

    fn estimate_transfer_time(&self, delta: &ModelDelta, device: &EdgeDevice) -> Duration {
        let size_bytes = delta.size_bytes as u64;
        let bandwidth_bps = device.bandwidth_mbps * 1_000_000;
        let transfer_time_ms = (size_bytes * 8 * 1000) / bandwidth_bps;
        Duration::from_millis(transfer_time_ms)
    }
}

// è‡ªé€‚åº”æ¨ç†
pub struct AdaptiveInference {
    pub policies: Vec<InferencePolicy>,
    pub resource_monitor: ResourceMonitor,
}

impl AdaptiveInference {
    pub fn new() -> Self {
        Self {
            policies: vec![
                InferencePolicy::EarlyExit,
                InferencePolicy::DynamicDepth,
                InferencePolicy::MultiResolution,
            ],
            resource_monitor: ResourceMonitor::new(),
        }
    }

    pub async fn infer(&self, input: &Tensor, budget: &LatencyBudget) -> Output {
        let resources = self.resource_monitor.get_current_resources();
        let policy = self.select_policy(&resources, budget);

        match policy {
            InferencePolicy::EarlyExit => self.early_exit_inference(input, budget),
            InferencePolicy::DynamicDepth => self.dynamic_depth_inference(input, budget),
            InferencePolicy::MultiResolution => self.multi_resolution_inference(input, budget),
        }
    }

    fn select_policy(&self, resources: &EdgeResources, budget: &LatencyBudget) -> InferencePolicy {
        if resources.battery_level < 0.2 {
            InferencePolicy::EarlyExit
        } else if budget.max_latency_ms < 100 {
            InferencePolicy::DynamicDepth
        } else {
            InferencePolicy::MultiResolution
        }
    }

    fn early_exit_inference(&self, input: &Tensor, budget: &LatencyBudget) -> Output {
        let start_time = Instant::now();
        let mut confidence = 0.0;
        let mut prediction = 0.0;

        // ç®€åŒ–çš„æ—©é€€æ¨ç†
        for layer in 0..5 {
            prediction = self.forward_layer(input, layer);
            confidence = self.compute_confidence(prediction);

            if confidence > 0.9 || start_time.elapsed() > budget.max_latency {
                break;
            }
        }

        Output {
            prediction,
            confidence,
            inference_time: start_time.elapsed(),
            layers_used: 5,
        }
    }

    fn dynamic_depth_inference(&self, input: &Tensor, budget: &LatencyBudget) -> Output {
        // ç®€åŒ–çš„åŠ¨æ€æ·±åº¦æ¨ç†
        let start_time = Instant::now();
        let mut prediction = 0.0;

        for layer in 0..10 {
            prediction = self.forward_layer(input, layer);

            if start_time.elapsed() > budget.max_latency {
                break;
            }
        }

        Output {
            prediction,
            confidence: 0.8,
            inference_time: start_time.elapsed(),
            layers_used: 10,
        }
    }

    fn multi_resolution_inference(&self, input: &Tensor, _budget: &LatencyBudget) -> Output {
        // ç®€åŒ–çš„å¤šåˆ†è¾¨ç‡æ¨ç†
        let resized_input = self.resize_tensor(input, 0.5);
        let prediction = self.forward_layer(&resized_input, 0);

        Output {
            prediction,
            confidence: 0.7,
            inference_time: Duration::from_millis(50),
            layers_used: 1,
        }
    }

    fn forward_layer(&self, input: &Tensor, layer: usize) -> f64 {
        // ç®€åŒ–çš„å‰å‘ä¼ æ’­
        input.data.iter().sum::<f64>() * (layer as f64 + 1.0)
    }

    fn compute_confidence(&self, prediction: f64) -> f64 {
        // ç®€åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—
        prediction.abs().min(1.0)
    }

    fn resize_tensor(&self, tensor: &Tensor, scale: f64) -> Tensor {
        let new_size = (tensor.data.len() as f64 * scale) as usize;
        Tensor {
            data: tensor.data.iter().take(new_size).copied().collect(),
        }
    }
}

// å¸è½½ç¼–æ’å™¨
pub struct OffloadOrchestrator {
    pub estimator: OffloadEstimator,
    pub decision_maker: OffloadDecisionMaker,
}

impl OffloadOrchestrator {
    pub fn new() -> Self {
        Self {
            estimator: OffloadEstimator::new(),
            decision_maker: OffloadDecisionMaker::new(),
        }
    }

    pub async fn decide(&self, task: &EdgeTask, context: &EdgeContext) -> OffloadPlan {
        let local_estimate = self.estimator.estimate_local_execution(task, context);
        let cloud_estimate = self.estimator.estimate_cloud_execution(task, context);

        let decision = self.decision_maker.make_decision(
            task,
            &local_estimate,
            &cloud_estimate,
            context,
        );

        match decision {
            OffloadDecision::Local => OffloadPlan::local(local_estimate),
            OffloadDecision::Cloud => OffloadPlan::cloud(cloud_estimate),
            OffloadDecision::Hybrid => OffloadPlan::hybrid(local_estimate, cloud_estimate),
        }
    }
}

// è¾…åŠ©ç»“æ„
#[derive(Clone, Debug)]
pub struct SchedulePlan {
    pub tasks: Vec<EdgeTask>,
    pub estimated_completion_time: Instant,
}

#[derive(Clone, Debug)]
pub struct EdgeOptimizer {
    pub learning_rate: f64,
    pub momentum: f64,
}

impl EdgeOptimizer {
    pub fn new() -> Self {
        Self {
            learning_rate: 0.01,
            momentum: 0.9,
        }
    }

    pub fn update(&mut self, gradients: &[f64]) {
        // ç®€åŒ–çš„ä¼˜åŒ–å™¨æ›´æ–°
    }
}

#[derive(Clone, Debug)]
pub struct EnergyBudget {
    pub total_energy: f64,
    pub consumed_energy: f64,
    pub training_cost: f64,
    pub inference_cost: f64,
}

impl EnergyBudget {
    pub fn new(total_energy: f64) -> Self {
        Self {
            total_energy,
            consumed_energy: 0.0,
            training_cost: 0.1,
            inference_cost: 0.01,
        }
    }

    pub fn can_afford_training(&self) -> bool {
        self.consumed_energy + self.training_cost <= self.total_energy
    }

    pub fn consume_training_energy(&mut self) {
        self.consumed_energy += self.training_cost;
    }

    pub fn get_last_consumption(&self) -> f64 {
        self.training_cost
    }
}

#[derive(Clone, Debug)]
pub struct LocalDataset {
    pub features: Vec<Vec<f64>>,
    pub labels: Vec<f64>,
}

impl LocalDataset {
    pub fn new() -> Self {
        Self {
            features: Vec::new(),
            labels: Vec::new(),
        }
    }
}

#[derive(Clone, Debug)]
pub struct MiniBatch {
    pub features: Vec<Vec<f64>>,
    pub labels: Vec<f64>,
}

#[derive(Clone, Debug)]
pub struct TrainStat {
    pub loss: f64,
    pub accuracy: f64,
    pub duration: Duration,
    pub energy_consumed: f64,
}

#[derive(Clone, Debug)]
pub enum TrainingError {
    InsufficientBudget,
    InvalidData,
    ModelError,
}

#[derive(Clone, Debug)]
pub struct EdgeModel {
    pub parameters: Vec<f64>,
}

#[derive(Clone, Debug)]
pub struct CompressedModel {
    pub parameters: Vec<f64>,
    pub compression_type: ModelCompression,
}

#[derive(Clone, Debug)]
pub enum ModelCompression {
    Quantization,
    Pruning,
    Distillation,
}

impl ModelCompression {
    pub fn get_compression_ratio(&self) -> f64 {
        match self {
            ModelCompression::Quantization => 0.25,
            ModelCompression::Pruning => 0.5,
            ModelCompression::Distillation => 0.1,
        }
    }
}

#[derive(Clone, Debug)]
pub struct ModelDelta {
    pub size_bytes: usize,
    pub data: Vec<u8>,
}

#[derive(Clone, Debug)]
pub struct Distribution {
    pub device_id: String,
    pub model_delta: ModelDelta,
    pub compression_ratio: f64,
    pub estimated_transfer_time: Duration,
}

#[derive(Clone, Debug)]
pub struct DistributionPlan {
    pub distributions: Vec<Distribution>,
}

impl DistributionPlan {
    pub fn new() -> Self {
        Self {
            distributions: Vec::new(),
        }
    }

    pub fn add_distribution(&mut self, distribution: Distribution) {
        self.distributions.push(distribution);
    }
}

#[derive(Clone, Debug)]
pub enum InferencePolicy {
    EarlyExit,
    DynamicDepth,
    MultiResolution,
}

#[derive(Clone, Debug)]
pub struct ResourceMonitor {
    pub resources: EdgeResources,
}

impl ResourceMonitor {
    pub fn new() -> Self {
        Self {
            resources: EdgeResources {
                cpu_cores: 4,
                memory_mb: 8192,
                storage_gb: 64,
                bandwidth_mbps: 100,
                battery_level: 0.8,
            },
        }
    }

    pub fn get_current_resources(&self) -> EdgeResources {
        self.resources.clone()
    }
}

#[derive(Clone, Debug)]
pub struct Tensor {
    pub data: Vec<f64>,
}

#[derive(Clone, Debug)]
pub struct LatencyBudget {
    pub max_latency_ms: u64,
}

#[derive(Clone, Debug)]
pub struct Output {
    pub prediction: f64,
    pub confidence: f64,
    pub inference_time: Duration,
    pub layers_used: usize,
}

#[derive(Clone, Debug)]
pub struct EdgeDevice {
    pub id: String,
    pub bandwidth_mbps: u64,
}

#[derive(Clone, Debug)]
pub struct EdgeContext {
    pub network_condition: NetworkCondition,
    pub device_resources: EdgeResources,
}

#[derive(Clone, Debug)]
pub enum NetworkCondition {
    Good,
    Fair,
    Poor,
}

#[derive(Clone, Debug)]
pub struct OffloadEstimator;

impl OffloadEstimator {
    pub fn new() -> Self {
        Self
    }

    pub fn estimate_local_execution(&self, task: &EdgeTask, context: &EdgeContext) -> ExecutionEstimate {
        ExecutionEstimate {
            time_ms: task.size_bytes as u64 / 1000,
            energy: task.size_bytes as f64 * 0.001,
            cost: 0.0,
        }
    }

    pub fn estimate_cloud_execution(&self, task: &EdgeTask, context: &EdgeContext) -> ExecutionEstimate {
        let network_factor = match context.network_condition {
            NetworkCondition::Good => 1.0,
            NetworkCondition::Fair => 2.0,
            NetworkCondition::Poor => 5.0,
        };

        ExecutionEstimate {
            time_ms: (task.size_bytes as u64 / 1000) * network_factor as u64,
            energy: task.size_bytes as f64 * 0.0001,
            cost: task.size_bytes as f64 * 0.00001,
        }
    }
}

#[derive(Clone, Debug)]
pub struct OffloadDecisionMaker;

impl OffloadDecisionMaker {
    pub fn new() -> Self {
        Self
    }

    pub fn make_decision(
        &self,
        task: &EdgeTask,
        local: &ExecutionEstimate,
        cloud: &ExecutionEstimate,
        context: &EdgeContext,
    ) -> OffloadDecision {
        if local.time_ms <= task.deadline_ms && context.device_resources.battery_level > 0.3 {
            OffloadDecision::Local
        } else if cloud.time_ms <= task.deadline_ms {
            OffloadDecision::Cloud
        } else {
            OffloadDecision::Hybrid
        }
    }
}

#[derive(Clone, Debug)]
pub struct ExecutionEstimate {
    pub time_ms: u64,
    pub energy: f64,
    pub cost: f64,
}

#[derive(Clone, Debug)]
pub enum OffloadDecision {
    Local,
    Cloud,
    Hybrid,
}

#[derive(Clone, Debug)]
pub enum OffloadPlan {
    Local(ExecutionEstimate),
    Cloud(ExecutionEstimate),
    Hybrid(ExecutionEstimate, ExecutionEstimate),
}

impl OffloadPlan {
    pub fn local(estimate: ExecutionEstimate) -> Self {
        OffloadPlan::Local(estimate)
    }

    pub fn cloud(estimate: ExecutionEstimate) -> Self {
        OffloadPlan::Cloud(estimate)
    }

    pub fn hybrid(local: ExecutionEstimate, cloud: ExecutionEstimate) -> Self {
        OffloadPlan::Hybrid(local, cloud)
    }
}

#[derive(Clone, Debug)]
pub struct DeltaEncoder;

impl DeltaEncoder {
    pub fn new() -> Self {
        Self
    }

    pub fn encode_delta(&self, original: &EdgeModel, compressed: &CompressedModel) -> ModelDelta {
        // ç®€åŒ–çš„å¢é‡ç¼–ç 
        let delta_data = original.parameters.iter()
            .zip(compressed.parameters.iter())
            .map(|(orig, comp)| ((orig - comp) * 1000.0) as u8)
            .collect();

        ModelDelta {
            size_bytes: delta_data.len(),
            data: delta_data,
        }
    }
}

#[derive(Clone, Debug)]
pub enum RolloutStrategy {
    Gradual,
    Immediate,
    Staged,
}

// ç¤ºä¾‹ä½¿ç”¨
#[tokio::main]
async fn main() {
    // åˆ›å»ºè¾¹ç¼˜å­¦ä¹ å™¨
    let optimizer = EdgeOptimizer::new();
    let budget = EnergyBudget::new(100.0);
    let mut learner = EdgeLearner::new(optimizer, budget);

    // åˆ›å»ºè¾¹ç¼˜ä»»åŠ¡
    let tasks = vec![
        EdgeTask {
            id: "task1".to_string(),
            deadline_ms: 1000,
            size_bytes: 1024,
            priority: 8,
            model_id: "model1".to_string(),
            task_type: TaskType::Inference,
        },
        EdgeTask {
            id: "task2".to_string(),
            deadline_ms: 500,
            size_bytes: 2048,
            priority: 9,
            model_id: "model2".to_string(),
            task_type: TaskType::Training,
        },
    ];

    // è¾¹ç¼˜è°ƒåº¦
    let scheduler = EDFScheduler;
    let resources = EdgeResources {
        cpu_cores: 4,
        memory_mb: 8192,
        storage_gb: 64,
        bandwidth_mbps: 100,
        battery_level: 0.8,
    };

    let schedule = scheduler.schedule(&tasks, &resources);
    println!("Scheduled tasks: {:?}", schedule.tasks);

    // è¾¹ç¼˜å­¦ä¹ 
    let batch = MiniBatch {
        features: vec![vec![1.0, 2.0, 3.0]; 10],
        labels: vec![1.0; 10],
    };

    match learner.train_step(&batch).await {
        Ok(stat) => println!("Training completed: {:?}", stat),
        Err(e) => println!("Training failed: {:?}", e),
    }

    // è‡ªé€‚åº”æ¨ç†
    let adaptive_inference = AdaptiveInference::new();
    let input = Tensor {
        data: vec![1.0, 2.0, 3.0, 4.0, 5.0],
    };
    let budget = LatencyBudget { max_latency_ms: 100 };

    let output = adaptive_inference.infer(&input, &budget).await;
    println!("Inference result: {:?}", output);

    // å¸è½½å†³ç­–
    let orchestrator = OffloadOrchestrator::new();
    let context = EdgeContext {
        network_condition: NetworkCondition::Good,
        device_resources: resources,
    };

    let offload_plan = orchestrator.decide(&tasks[0], &context).await;
    println!("Offload plan: {:?}", offload_plan);
}

## å‰ç½®é˜…è¯»ï¼ˆå»ºè®®ï¼‰
- åˆ†å¸ƒå¼ç³»ç»Ÿä¸ç½‘ç»œåŸºç¡€ï¼ˆå¸¦å®½/æ—¶å»¶/ä¸€è‡´æ€§ï¼‰
- å®æ—¶ç³»ç»Ÿä¸è°ƒåº¦ï¼ˆæˆªæ­¢æœŸ/ä¼˜å…ˆçº§/èµ„æºç®¡ç†ï¼‰
- éšç§ä¸å®‰å…¨ï¼ˆç«¯ä¾§æ•°æ®/åŠ å¯†/è®¿é—®æ§åˆ¶ï¼‰
- è”é‚¦å­¦ä¹ ä¸ååŒæ¨ç†åŸºç¡€

## å‚è€ƒæ–‡çŒ®ï¼ˆç¤ºä¾‹ï¼‰
1. Satyanarayanan, M. The Emergence of Edge Computing. Computer, 2017.
2. Mao, Y. et al. A Survey on Mobile Edge Computing: The Communication Perspective. IEEE Communications Surveys & Tutorials, 2017.
3. Li, E. et al. Learning and Inferencing on the Edge: A Survey. Proceedings of the IEEE, 2018.
