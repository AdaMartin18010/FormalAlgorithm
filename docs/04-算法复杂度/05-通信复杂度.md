---
title: 4.5 é€šä¿¡å¤æ‚åº¦ / Communication Complexity
version: 1.0
status: maintained
last_updated: 2025-01-11
owner: å¤æ‚åº¦ç†è®ºå·¥ä½œç»„
---

> ğŸ“Š **é¡¹ç›®å…¨é¢æ¢³ç†**ï¼šè¯¦ç»†çš„é¡¹ç›®ç»“æ„ã€æ¨¡å—è¯¦è§£å’Œå­¦ä¹ è·¯å¾„ï¼Œè¯·å‚é˜… [`é¡¹ç›®å…¨é¢æ¢³ç†-2025.md`](../é¡¹ç›®å…¨é¢æ¢³ç†-2025.md)

## 4.5 é€šä¿¡å¤æ‚åº¦ / Communication Complexity

### æ‘˜è¦ / Executive Summary

- ç»Ÿä¸€é€šä¿¡å¤æ‚åº¦çš„å½¢å¼åŒ–å®šä¹‰ã€æ¨¡å‹ä¸åˆ†ææ–¹æ³•ï¼Œå»ºç«‹é€šä¿¡å¤æ‚åº¦ä¸å…¶ä»–å¤æ‚åº¦ç»´åº¦çš„å…³ç³»ã€‚
- æä¾›é€šä¿¡å¤æ‚åº¦åœ¨åˆ†å¸ƒå¼è®¡ç®—ã€å¹¶è¡Œè®¡ç®—å’Œæµç®—æ³•ä¸­çš„åº”ç”¨æ¡†æ¶ã€‚

### å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary

- é€šä¿¡å¤æ‚åº¦ã€é€šä¿¡åè®®ã€é€šä¿¡çŸ©é˜µã€ç¡®å®šæ€§/éšæœºæ€§/é‡å­é€šä¿¡å¤æ‚åº¦ã€åˆ†å¸ƒå¼è®¡ç®—ã€æµç®—æ³•ã€‚
- æœ¯è¯­å¯¹é½ä¸å¼•ç”¨è§„èŒƒï¼š`docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md`ï¼Œ`01-åŸºç¡€ç†è®º/00-æ’°å†™è§„èŒƒä¸å¼•ç”¨æŒ‡å—.md`

### å¿«é€Ÿå¯¼èˆª / Quick Links

- [ç›®å½•](#ç›®å½•--table-of-contents)
- [åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
- [é€šä¿¡æ¨¡å‹](#2-é€šä¿¡æ¨¡å‹)
- [ä¸‹ç•ŒæŠ€æœ¯](#3-ä¸‹ç•ŒæŠ€æœ¯)
- [åº”ç”¨é¢†åŸŸ](#4-åº”ç”¨é¢†åŸŸ)

> å¯¼èˆªï¼š`docs/å½¢å¼åŒ–ç®—æ³•æ–‡æ¡£æ”¹è¿›å®ŒæˆæŠ¥å‘Š.md` Â· `docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md` Â· `docs/è·¨æ–‡æ¡£ç´¢å¼•.md`

## ç›®å½• / Table of Contents

- [4.5 é€šä¿¡å¤æ‚åº¦ / Communication Complexity](#45-é€šä¿¡å¤æ‚åº¦--communication-complexity)
  - [æ‘˜è¦ / Executive Summary](#æ‘˜è¦--executive-summary)
  - [å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary](#å…³é”®æœ¯è¯­ä¸ç¬¦å·--glossary)
  - [å¿«é€Ÿå¯¼èˆª / Quick Links](#å¿«é€Ÿå¯¼èˆª--quick-links)
- [ç›®å½• / Table of Contents](#ç›®å½•--table-of-contents)
- [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
  - [1.1 é€šä¿¡å¤æ‚åº¦å®šä¹‰](#11-é€šä¿¡å¤æ‚åº¦å®šä¹‰)
  - [1.1.1 å¤æ‚åº¦å®šä¹‰çš„ç»Ÿä¸€æ¡†æ¶ / Unified Framework for Complexity Definitions](#111-å¤æ‚åº¦å®šä¹‰çš„ç»Ÿä¸€æ¡†æ¶--unified-framework-for-complexity-definitions)
  - [1.2 é€šä¿¡åè®®](#12-é€šä¿¡åè®®)
  - [1.3 é€šä¿¡çŸ©é˜µ](#13-é€šä¿¡çŸ©é˜µ)
- [2. é€šä¿¡æ¨¡å‹](#2-é€šä¿¡æ¨¡å‹)
  - [2.1 ç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦](#21-ç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦)
  - [2.2 éšæœºæ€§é€šä¿¡å¤æ‚åº¦](#22-éšæœºæ€§é€šä¿¡å¤æ‚åº¦)
  - [2.3 é‡å­é€šä¿¡å¤æ‚åº¦](#23-é‡å­é€šä¿¡å¤æ‚åº¦)
- [3. ä¸‹ç•ŒæŠ€æœ¯](#3-ä¸‹ç•ŒæŠ€æœ¯)
  - [3.1 é€šä¿¡çŸ©é˜µæ–¹æ³•](#31-é€šä¿¡çŸ©é˜µæ–¹æ³•)
  - [3.2 ä¿¡æ¯è®ºæ–¹æ³•](#32-ä¿¡æ¯è®ºæ–¹æ³•)
  - [3.3 ç»„åˆæ–¹æ³•](#33-ç»„åˆæ–¹æ³•)
- [4. åº”ç”¨é¢†åŸŸ](#4-åº”ç”¨é¢†åŸŸ)
  - [4.1 åˆ†å¸ƒå¼è®¡ç®—](#41-åˆ†å¸ƒå¼è®¡ç®—)
  - [4.2 å¹¶è¡Œè®¡ç®—](#42-å¹¶è¡Œè®¡ç®—)
  - [4.3 æµç®—æ³•](#43-æµç®—æ³•)
  - [4.4 æ•°æ®æµä¸‹ç•Œ](#44-æ•°æ®æµä¸‹ç•Œ)
  - [4.5 åˆ†å¸ƒå¼ä¸€è‡´æ€§ä¸‹ç•Œ](#45-åˆ†å¸ƒå¼ä¸€è‡´æ€§ä¸‹ç•Œ)
  - [4.6 ä¿¡æ¯é€šä¿¡ä¸­çš„ç®—æ³•å¤æ‚åº¦å…³é”®ç®—æ³•](#46-ä¿¡æ¯é€šä¿¡ä¸­çš„ç®—æ³•å¤æ‚åº¦å…³é”®ç®—æ³•)
- [5. ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½](#5-ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½)
  - [5.1 ç›¸å…³æ–‡æ¡£](#51-ç›¸å…³æ–‡æ¡£)
  - [5.2 çŸ¥è¯†ä½“ç³»ä½ç½®](#52-çŸ¥è¯†ä½“ç³»ä½ç½®)
  - [5.3 VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£](#53-viewæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£)
- [6. å‚è€ƒæ–‡çŒ® / References](#6-å‚è€ƒæ–‡çŒ®--references)
  - [ç»å…¸å¥ åŸºæ–‡çŒ® / Classic Foundational Literature](#ç»å…¸å¥ åŸºæ–‡çŒ®--classic-foundational-literature)
  - [æ ‡å‡†æ•™æ / Standard Textbooks](#æ ‡å‡†æ•™æ--standard-textbooks)

---

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 é€šä¿¡å¤æ‚åº¦å®šä¹‰

**å®šä¹‰ 1.1.1** é€šä¿¡å¤æ‚åº¦é—®é¢˜ï¼š

ç»™å®šå‡½æ•° $f: X \times Y \rightarrow Z$ï¼Œä¸¤ä¸ªç©å®¶ Alice å’Œ Bob åˆ†åˆ«æŒæœ‰è¾“å…¥ $x \in X$ å’Œ $y \in Y$ï¼Œä»–ä»¬éœ€è¦é€šè¿‡é€šä¿¡åè®®è®¡ç®— $f(x,y)$ã€‚

**Definition 1.1.1** Communication complexity problem:

Given a function $f: X \times Y \rightarrow Z$, two players Alice and Bob hold inputs $x \in X$ and $y \in Y$ respectively, and they need to compute $f(x,y)$ through a communication protocol.

**å®šä¹‰ 1.1.2** é€šä¿¡å¤æ‚åº¦ï¼š

å‡½æ•° $f$ çš„é€šä¿¡å¤æ‚åº¦ $CC(f)$ æ˜¯è®¡ç®— $f$ æ‰€éœ€çš„æœ€å°‘é€šä¿¡ä½æ•°ã€‚

**Definition 1.1.2** Communication complexity:

The communication complexity $CC(f)$ of function $f$ is the minimum number of bits that need to be communicated to compute $f$.

**å½¢å¼åŒ–è¡¨ç¤º / Formal Representation:**

$$CC(f) = \min_{P} \max_{x,y} \text{åè®® } P \text{ åœ¨è¾“å…¥ } (x,y) \text{ ä¸Šçš„é€šä¿¡ä½æ•°}$$

å…¶ä¸­ $P$ éå†æ‰€æœ‰è®¡ç®— $f$ çš„é€šä¿¡åè®®ã€‚

where $P$ ranges over all communication protocols that compute $f$.

### 1.1.1 å¤æ‚åº¦å®šä¹‰çš„ç»Ÿä¸€æ¡†æ¶ / Unified Framework for Complexity Definitions

**å®šä¹‰ 1.1.3** (å¤æ‚åº¦å®šä¹‰çš„ç»Ÿä¸€æ¡†æ¶) è®¾ï¼š
**Definition 1.1.3** (Unified Framework for Complexity Definitions) Let:

| åç§° | å½¢å¼åŒ–å®šä¹‰ | å…¸å‹åº¦é‡ | ä¸ä¿¡æ¯è®ºçš„è”ç³» |
|------|------------|----------|----------------|
| **æ—¶é—´å¤æ‚åº¦ T(n)** | åœ¨ **ç¡®å®šæ€§å›¾çµæœº**ï¼ˆæˆ– RAMï¼‰ä¸Šï¼Œå¯¹é•¿åº¦ â‰¤ n çš„è¾“å…¥æ‰€éœ€çš„ **æ­¥éª¤æ•°** ä¸Šç•Œã€‚ | `O(f(n))`ã€`Î˜(f(n))` | æ¯ä¸€æ­¥å¯è§†ä¸ºä¸€æ¬¡ **æœ¬åœ°ä¿¡æ¯å¤„ç†**ï¼Œä¿¡æ¯é‡ â‰¤ æ­¥æ•° Ã— å¸¸æ•°ã€‚ |
| **ç©ºé—´å¤æ‚åº¦ S(n)** | åŒä¸Šï¼Œæ‰€éœ€çš„ **å·¥ä½œç£å¸¦æ ¼æ•°**ï¼ˆæˆ–å†…å­˜å•å…ƒï¼‰ä¸Šç•Œã€‚ | `O(g(n))` | ç©ºé—´å³ **å­˜å‚¨ä¿¡æ¯çš„ä½æ•°**ï¼Œå— **Shannon ç†µ** ä¸‹ç•Œé™åˆ¶ã€‚ |
| **é€šä¿¡å¤æ‚åº¦ C(P)** | å¯¹å‡½æ•° `f : X Ã— Y â†’ Z`ï¼Œ**ä¸¤ä¸ªï¼ˆæˆ–å¤šï¼‰å‚ä¸æ–¹** åœ¨ **æœ‰é™å¸¦å®½** çš„ **æ¶ˆæ¯ä¼ é€’æ¨¡å‹** ä¸­ï¼Œ**æœ€å°‘éœ€è¦ä¼ è¾“çš„æ¯”ç‰¹æ•°**ï¼ˆæˆ–ä¿¡æ¯ç†µï¼‰ä¸Šç•Œã€‚ | `Î©(h(n))`ã€`Î˜(h(n))` | ç›´æ¥ä½¿ç”¨ **ä¿¡æ¯è®º**ï¼šè‹¥è¦è®©æ¥æ”¶æ–¹ç¡®å®š `f(x,y)`ï¼Œå¿…é¡»è‡³å°‘æ¥æ”¶ `I(f(x,y); transcript)` ä½ä¿¡æ¯ã€‚ |
| **å¹¶è¡Œ/åˆ†å¸ƒå¼æ—¶é—´ R(P)** | åœ¨ **åŒæ­¥æ¶ˆæ¯-ä¼ é€’ç½‘ç»œ**ï¼ˆå¦‚ PRAMã€CONGESTã€LOCALï¼‰ä¸­ï¼Œå®Œæˆä»»åŠ¡æ‰€éœ€çš„ **æœ€å°‘åŒæ­¥è½®æ•°**ã€‚ | `O(r)` | æ¯è½®å¯ä¼ é€’çš„ **ä¿¡æ¯é‡** å—ç½‘ç»œæ¨¡å‹ï¼ˆå¸¦å®½ Bã€èŠ‚ç‚¹åº¦ Î”ï¼‰é™åˆ¶ã€‚ |
| **æ€»é€šä¿¡é‡ M(P)** | å®Œæˆä»»åŠ¡æœŸé—´æ‰€æœ‰èŠ‚ç‚¹å‘é€çš„ **æ¯”ç‰¹æ€»å’Œ**ã€‚ | `O(m)` | ä¸ **ä¿¡æ¯å‹ç¼©æé™**ï¼ˆç†µï¼‰ç›´æ¥å¯¹åº”ï¼Œ`M(P) â‰¥ H(f)`ï¼ˆé™¤å¸¸æ•°ï¼‰|

| Name | Formal Definition | Typical Measure | Connection with Information Theory |
|------|------------------|-----------------|-----------------------------------|
| **Time Complexity T(n)** | Upper bound on the **number of steps** required on a **deterministic Turing machine** (or RAM) for inputs of length â‰¤ n. | `O(f(n))`, `Î˜(f(n))` | Each step can be viewed as a **local information processing**, information content â‰¤ steps Ã— constant. |
| **Space Complexity S(n)** | Upper bound on the **working tape cells** (or memory units) required as above. | `O(g(n))` | Space is the **number of bits storing information**, bounded below by **Shannon entropy**. |
| **Communication Complexity C(P)** | For function `f : X Ã— Y â†’ Z`, upper bound on the **minimum number of bits** (or information entropy) that need to be transmitted in a **message-passing model** with **limited bandwidth** by **two (or more) parties**. | `Î©(h(n))`, `Î˜(h(n))` | Directly uses **information theory**: to determine `f(x,y)`, the receiver must receive at least `I(f(x,y); transcript)` bits of information. |
| **Parallel/Distributed Time R(P)** | In a **synchronous message-passing network** (e.g., PRAM, CONGEST, LOCAL), the **minimum number of synchronous rounds** required to complete the task. | `O(r)` | The **amount of information** that can be transmitted per round is limited by the network model (bandwidth B, node degree Î”). |
| **Total Communication M(P)** | The **total number of bits** sent by all nodes during task completion. | `O(m)` | Directly corresponds to the **information compression limit** (entropy), `M(P) â‰¥ H(f)` (up to constants) |

> **å¤æ‚åº¦ä¸ä¿¡æ¯è®ºçš„å…³ç³» / Relationship between Complexity and Information Theory**:
>
> - æ—¶é—´å¤æ‚åº¦ï¼šæ¯ä¸€æ­¥å¤„ç†çš„ä¿¡æ¯é‡å—é™äºè®¡ç®—æ¨¡å‹çš„æœ¬åœ°æ“ä½œèƒ½åŠ›
> - Time complexity: The amount of information processed per step is limited by the local operation capability of the computation model
>
> - ç©ºé—´å¤æ‚åº¦ï¼šå­˜å‚¨çš„ä¿¡æ¯é‡å— Shannon ç†µä¸‹ç•Œé™åˆ¶
> - Space complexity: The amount of stored information is bounded below by Shannon entropy
>
> - é€šä¿¡å¤æ‚åº¦ï¼šç›´æ¥ä½¿ç”¨ä¿¡æ¯è®ºï¼Œå¿…é¡»ä¼ è¾“è¶³å¤Ÿçš„ä¿¡æ¯é‡æ‰èƒ½ç¡®å®šå‡½æ•°å€¼
> - Communication complexity: Directly uses information theory, sufficient information must be transmitted to determine the function value
>
> - å¹¶è¡Œ/åˆ†å¸ƒå¼æ—¶é—´ï¼šå—ç½‘ç»œå¸¦å®½å’Œæ‹“æ‰‘ç»“æ„é™åˆ¶çš„ä¿¡æ¯ä¼ æ’­é€Ÿåº¦
> - Parallel/distributed time: Information propagation speed limited by network bandwidth and topology
>
> - æ€»é€šä¿¡é‡ï¼šä¸ä¿¡æ¯å‹ç¼©æé™ï¼ˆç†µï¼‰ç›´æ¥å¯¹åº”
> - Total communication: Directly corresponds to the information compression limit (entropy)

### 1.2 é€šä¿¡åè®®

**å®šä¹‰ 1.2.1** é€šä¿¡åè®®ï¼š

é€šä¿¡åè®®æ˜¯ä¸€ä¸ªäº¤äº’å¼è¿‡ç¨‹ï¼ŒAlice å’Œ Bob è½®æµå‘é€æ¶ˆæ¯ï¼Œæœ€ç»ˆ Bob è¾“å‡ºç»“æœã€‚

**Definition 1.2.1** Communication protocol:

A communication protocol is an interactive process where Alice and Bob take turns sending messages, and Bob eventually outputs the result.

**åè®®æ ‘ / Protocol Tree:**

é€šä¿¡åè®®å¯ä»¥ç”¨åè®®æ ‘è¡¨ç¤ºï¼š

- æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”ä¸€ä¸ªé€šä¿¡è½®æ¬¡
- è¾¹æ ‡è®°å‘é€çš„æ¶ˆæ¯
- å¶å­èŠ‚ç‚¹å¯¹åº”è¾“å‡ºç»“æœ

Communication protocols can be represented as protocol trees:

- Each node corresponds to a communication round
- Edges are labeled with messages sent
- Leaf nodes correspond to output results

### 1.3 é€šä¿¡çŸ©é˜µ

**å®šä¹‰ 1.3.1** é€šä¿¡çŸ©é˜µï¼š

å‡½æ•° $f: X \times Y \rightarrow Z$ çš„é€šä¿¡çŸ©é˜µ $M_f$ æ˜¯ä¸€ä¸ª $|X| \times |Y|$ çš„çŸ©é˜µï¼Œå…¶ä¸­ $M_f[x,y] = f(x,y)$ã€‚

**Definition 1.3.1** Communication matrix:

The communication matrix $M_f$ of function $f: X \times Y \rightarrow Z$ is an $|X| \times |Y|$ matrix where $M_f[x,y] = f(x,y)$.

**å®šç† 1.3.1** é€šä¿¡å¤æ‚åº¦ä¸‹ç•Œï¼š

å¯¹äºå‡½æ•° $f$ï¼Œç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦æ»¡è¶³ï¼š
$$D(f) \geq \log \text{rank}(M_f)$$

å…¶ä¸­ $\text{rank}(M_f)$ æ˜¯é€šä¿¡çŸ©é˜µçš„ç§©ã€‚

**Theorem 1.3.1** Communication complexity lower bound:

For function $f$, the deterministic communication complexity satisfies:
$$D(f) \geq \log \text{rank}(M_f)$$

where $\text{rank}(M_f)$ is the rank of the communication matrix.

---

## 2. é€šä¿¡æ¨¡å‹

### 2.1 ç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦

**å®šä¹‰ 2.1.1** ç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦ $D(f)$ï¼š

ç¡®å®šæ€§é€šä¿¡å¤æ‚åº¦æ˜¯ç¡®å®šæ€§åè®®è®¡ç®— $f$ æ‰€éœ€çš„æœ€å°‘é€šä¿¡ä½æ•°ã€‚

**Definition 2.1.1** Deterministic communication complexity $D(f)$:

The deterministic communication complexity is the minimum number of bits required by a deterministic protocol to compute $f$.

**ä¾‹å­ï¼šç›¸ç­‰æ€§é—®é¢˜ / Example: Equality Problem:**

- **é—®é¢˜ / Problem:** $EQ_n(x,y) = 1$ å½“ä¸”ä»…å½“ $x = y$
- **ç¡®å®šæ€§å¤æ‚åº¦ / Deterministic Complexity:** $D(EQ_n) = n$
- **è¯æ˜ / Proof:** éœ€è¦ä¼ è¾“æ‰€æœ‰ $n$ ä½æ‰èƒ½ç¡®å®šç›¸ç­‰æ€§

### 2.2 éšæœºæ€§é€šä¿¡å¤æ‚åº¦

**å®šä¹‰ 2.2.1** éšæœºæ€§é€šä¿¡å¤æ‚åº¦ $R_\epsilon(f)$ï¼š

éšæœºæ€§é€šä¿¡å¤æ‚åº¦æ˜¯éšæœºåè®®ä»¥é”™è¯¯æ¦‚ç‡ $\epsilon$ è®¡ç®— $f$ æ‰€éœ€çš„æœ€å°‘é€šä¿¡ä½æ•°ã€‚

**Definition 2.2.1** Randomized communication complexity $R_\epsilon(f)$:

The randomized communication complexity is the minimum number of bits required by a randomized protocol to compute $f$ with error probability $\epsilon$.

**å®šç† 2.2.1** éšæœºæ€§ä¼˜åŠ¿ï¼š

å¯¹äºç›¸ç­‰æ€§é—®é¢˜ï¼š

- $D(EQ_n) = n$
- $R_{1/3}(EQ_n) = O(\log n)$

éšæœºæ€§å¯ä»¥æ˜¾è‘—é™ä½é€šä¿¡å¤æ‚åº¦ã€‚

**Theorem 2.2.1** Randomized advantage:

For the equality problem:

- $D(EQ_n) = n$
- $R_{1/3}(EQ_n) = O(\log n)$

Randomization can significantly reduce communication complexity.

### 2.3 é‡å­é€šä¿¡å¤æ‚åº¦

**å®šä¹‰ 2.3.1** é‡å­é€šä¿¡å¤æ‚åº¦ $Q_\epsilon(f)$ï¼š

é‡å­é€šä¿¡å¤æ‚åº¦æ˜¯é‡å­åè®®ä»¥é”™è¯¯æ¦‚ç‡ $\epsilon$ è®¡ç®— $f$ æ‰€éœ€çš„æœ€å°‘é‡å­æ¯”ç‰¹æ•°ã€‚

**Definition 2.3.1** Quantum communication complexity $Q_\epsilon(f)$:

The quantum communication complexity is the minimum number of qubits required by a quantum protocol to compute $f$ with error probability $\epsilon$.

**å®šç† 2.3.1** é‡å­ä¼˜åŠ¿ï¼š

å¯¹äºæŸäº›é—®é¢˜ï¼Œé‡å­é€šä¿¡å¤æ‚åº¦å¯ä»¥ä½äºç»å…¸é€šä¿¡å¤æ‚åº¦ã€‚

**Theorem 2.3.1** Quantum advantage:

For some problems, quantum communication complexity can be lower than classical communication complexity.

---

## 3. ä¸‹ç•ŒæŠ€æœ¯

### 3.1 é€šä¿¡çŸ©é˜µæ–¹æ³•

**æ–¹æ³•æ¦‚è¿° / Method Overview:**

ä½¿ç”¨é€šä¿¡çŸ©é˜µçš„ä»£æ•°æ€§è´¨ï¼ˆå¦‚ç§©ã€è¡Œåˆ—å¼ï¼‰æ¥è¯æ˜é€šä¿¡å¤æ‚åº¦ä¸‹ç•Œã€‚

Use algebraic properties of communication matrices (such as rank, determinant) to prove communication complexity lower bounds.

**å®šç† 3.1.1** ç§©ä¸‹ç•Œï¼š

$$D(f) \geq \log \text{rank}(M_f)$$

**å®šç† 3.1.2** éè´Ÿç§©ä¸‹ç•Œï¼š

$$D(f) \geq \log \text{rank}_+(M_f)$$

å…¶ä¸­ $\text{rank}_+(M_f)$ æ˜¯éè´Ÿç§©ã€‚

where $\text{rank}_+(M_f)$ is the non-negative rank.

### 3.2 ä¿¡æ¯è®ºæ–¹æ³•

**æ–¹æ³•æ¦‚è¿° / Method Overview:**

ä½¿ç”¨ä¿¡æ¯è®ºå·¥å…·ï¼ˆå¦‚äº’ä¿¡æ¯ã€ç†µï¼‰æ¥åˆ†æé€šä¿¡åè®®çš„ä¿¡æ¯ä¼ è¾“ã€‚

Use information-theoretic tools (such as mutual information, entropy) to analyze information transmission in communication protocols.

**å®šç† 3.2.1** ä¿¡æ¯è®ºä¸‹ç•Œï¼š

å¯¹äºå‡½æ•° $f$ï¼Œå¦‚æœè¾“å…¥åˆ†å¸ƒä¸º $\mu$ï¼Œåˆ™ï¼š
$$CC(f) \geq I(X;Y|f(X,Y))$$

å…¶ä¸­ $I$ æ˜¯äº’ä¿¡æ¯ã€‚

**Theorem 3.2.1** Information-theoretic lower bound:

For function $f$, if the input distribution is $\mu$, then:
$$CC(f) \geq I(X;Y|f(X,Y))$$

where $I$ is mutual information.

**å®šç† 3.2.2** ä¿¡æ¯å¤æ‚åº¦ä¸‹ç•Œï¼ˆYao's Principleï¼‰ï¼š

å¯¹äºä»»æ„éšæœºåŒ–åè®®ï¼Œæœ€åæƒ…å†µçš„é€šä¿¡é‡ â‰¥ å¯¹æŸä¸ªåˆ†å¸ƒçš„æœŸæœ›ä¿¡æ¯é‡ï¼š

$$R_\epsilon(f) \geq \min_{\mu} I_\mu(X;Y|f(X,Y))$$

å…¶ä¸­ $\mu$ éå†æ‰€æœ‰è¾“å…¥åˆ†å¸ƒã€‚

**å®šç† 3.2.3** (Set-Disjointness é€šä¿¡ä¸‹ç•Œ) (Theorem - Set-Disjointness Communication Lower Bound):
å¯¹å‡½æ•° $DISJ_n(x,y) = \neg\exists i (x_i \wedge y_i)$ï¼Œä»»æ„éšæœºåŒ–åè®®çš„ **è¯¯å·® $\leq 1/3$** éœ€è¦ **$\Omega(n)$** ä½é€šä¿¡ã€‚

For function $DISJ_n(x,y) = \neg\exists i (x_i \wedge y_i)$, any randomized protocol with **error $\leq 1/3$** requires **$\Omega(n)$** bits of communication.

**è¯æ˜è¦ç‚¹** (Proof Outline):

- **ä¿¡æ¯å¤æ‚åº¦æ–¹æ³•** (Information Complexity Method): ä½¿ç”¨ **ä¿¡æ¯å¤æ‚åº¦**ï¼š$I(DISJ; transcript) \geq \Omega(n)$ã€‚
  Use **information complexity**: $I(DISJ; transcript) \geq \Omega(n)$.

- **ç»„åˆæ–¹æ³•** (Combinatorial Methods): åˆ©ç”¨ **discrepancy** ä¸ **fooling set** æŠ€æœ¯ã€‚
  Use **discrepancy** and **fooling set** techniques.

- **ä¸‹ç•Œæ„ä¹‰** (Lower Bound Significance): è¯¥ä¸‹ç•Œè¡¨æ˜ Set-Disjointness æ˜¯é€šä¿¡å¤æ‚åº¦ä¸­çš„"å›°éš¾"é—®é¢˜ï¼Œè®¸å¤šå…¶ä»–é—®é¢˜çš„ä¸‹ç•Œéƒ½é€šè¿‡å½’çº¦åˆ° Set-Disjointness å¾—åˆ°ã€‚
  This lower bound shows that Set-Disjointness is a "hard" problem in communication complexity, and lower bounds for many other problems are obtained by reducing to Set-Disjointness.

**Theorem 3.2.2** Information complexity lower bound (Yao's Principle):

For any randomized protocol, the worst-case communication â‰¥ expected information for some distribution:

$$R_\epsilon(f) \geq \min_{\mu} I_\mu(X;Y|f(X,Y))$$

where $\mu$ ranges over all input distributions.

**å®šç† 3.2.3** Set-Disjointness é€šä¿¡ä¸‹ç•Œï¼š

å¯¹ $DISJ_n(x,y) = \neg\exists i (x_i \land y_i)$ï¼Œä»»æ„éšæœºåŒ–åè®®çš„è¯¯å·® â‰¤ 1/3 éœ€è¦ **Î©(n)** ä½é€šä¿¡ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- ä½¿ç”¨ **ä¿¡æ¯å¤æ‚åº¦**ï¼š$I(DISJ; transcript) \geq \Omega(n)$
- åˆ©ç”¨ **discrepancy** ä¸ **fooling set**

**Theorem 3.2.3** Set-Disjointness communication lower bound:

For $DISJ_n(x,y) = \neg\exists i (x_i \land y_i)$, any randomized protocol with error â‰¤ 1/3 requires **Î©(n)** bits of communication.

**Proof sketch**:

- Use **information complexity**: $I(DISJ; transcript) \geq \Omega(n)$
- Apply **discrepancy** and **fooling set** techniques

**å®šç† 3.2.4** å…¨å±€æ’åºçš„åˆ†å¸ƒå¼ä¸‹ç•Œï¼š

åœ¨ CONGEST æ¨¡å‹ï¼ˆå¸¦å®½ $B = O(\log n)$ï¼‰ä¸­ï¼Œä»»æ„åœ¨ $p$ èŠ‚ç‚¹ä¸Šå®Œæˆå…¨å±€æ’åºï¼Œéœ€è¦ **Î©(log p)** è½®ï¼ˆå³ **Î©(log n)**ï¼‰ï¼Œæ€»é€šä¿¡ **Î©(n log n)** ä½ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- é€šè¿‡ **ä¿¡æ¯å‹ç¼©**ï¼šæ’åºç­‰ä»·äºæŠŠ $n$ é¡¹çš„å…¨åºä¿¡æ¯ï¼ˆ$n \log n$ ä½ï¼‰å¹¿æ’­åˆ°æ¯ä¸ªèŠ‚ç‚¹
- æ¯è½®æœ€å¤š $p \cdot B$ ä½ï¼Œæ•…è½®æ•°ä¸‹ç•Œ

**Theorem 3.2.4** Distributed global sorting lower bound:

In the CONGEST model (bandwidth $B = O(\log n)$), any protocol completing global sorting on $p$ nodes requires **Î©(log p)** rounds (i.e., **Î©(log n)**), with total communication **Î©(n log n)** bits.

**Proof sketch**:

- Through **information compression**: sorting is equivalent to broadcasting the total order information ($n \log n$ bits) of $n$ items to each node
- At most $p \cdot B$ bits per round, hence the round lower bound

**å®šç† 3.2.5** Gossipï¼ˆæ¶ˆæ¯å¹¿æ’­ï¼‰ä¸‹ç•Œï¼š

åœ¨ LOCAL æ¨¡å‹ï¼ˆæ¯è½® 1 ä½ï¼‰ä¸­ï¼Œåœ¨ç›´å¾„ $D$ çš„ç½‘ç»œä¸­ï¼Œå¹¿æ’­å•æ¡æ¶ˆæ¯çš„æœ€å°‘è½®æ•° $\geq D$ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- ä¿¡æ¯å¿…é¡»è·¨è¶Šæœ€é•¿è·¯å¾„
- å•è½®åªèƒ½æ²¿ä¸€æ¡è¾¹å‰è¿› 1 æ­¥

**Theorem 3.2.5** Gossip (message broadcast) lower bound:

In the LOCAL model (1 bit per round), in a network with diameter $D$, broadcasting a single message requires at least $D$ rounds.

**Proof sketch**:

- Information must traverse the longest path
- One round can only advance 1 step along one edge

### 3.3 ç»„åˆæ–¹æ³•

**æ–¹æ³•æ¦‚è¿° / Method Overview:**

ä½¿ç”¨ç»„åˆç»“æ„ï¼ˆå¦‚çŸ©å½¢ã€è¦†ç›–ï¼‰æ¥è¯æ˜é€šä¿¡å¤æ‚åº¦ä¸‹ç•Œã€‚

Use combinatorial structures (such as rectangles, covers) to prove communication complexity lower bounds.

**å®šä¹‰ 3.3.1** çŸ©å½¢ï¼š

é€šä¿¡çŸ©é˜µçš„å­é›† $R = A \times B$ ç§°ä¸ºçŸ©å½¢ï¼Œå¦‚æœ $M_f[x,y]$ åœ¨ $R$ ä¸Šä¸ºå¸¸æ•°ã€‚

**Definition 3.3.1** Rectangle:

A subset $R = A \times B$ of the communication matrix is called a rectangle if $M_f[x,y]$ is constant on $R$.

**å®šç† 3.3.1** çŸ©å½¢ä¸‹ç•Œï¼š

å¦‚æœé€šä¿¡çŸ©é˜µéœ€è¦è‡³å°‘ $t$ ä¸ªå•è‰²çŸ©å½¢è¦†ç›–ï¼Œåˆ™ï¼š
$$D(f) \geq \log t$$

---

## 4. åº”ç”¨é¢†åŸŸ

### 4.1 åˆ†å¸ƒå¼è®¡ç®—

**åº”ç”¨åœºæ™¯ / Application Scenarios:**

åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œé€šä¿¡å¤æ‚åº¦å†³å®šäº†èŠ‚ç‚¹é—´éœ€è¦äº¤æ¢çš„æ•°æ®é‡ï¼Œç›´æ¥å½±å“ç³»ç»Ÿæ€§èƒ½ã€‚

In distributed systems, communication complexity determines the amount of data that needs to be exchanged between nodes, directly affecting system performance.

**ä¾‹å­ï¼šåˆ†å¸ƒå¼æ’åº / Example: Distributed Sorting:**

- é—®é¢˜ï¼š$n$ ä¸ªèŠ‚ç‚¹å„æŒæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œéœ€è¦æ’åº
- é€šä¿¡å¤æ‚åº¦ï¼š$\Omega(n \log n)$
- åº”ç”¨ï¼šMapReduceã€åˆ†å¸ƒå¼æ•°æ®åº“

### 4.2 å¹¶è¡Œè®¡ç®—

**åº”ç”¨åœºæ™¯ / Application Scenarios:**

åœ¨å¹¶è¡Œç®—æ³•ä¸­ï¼Œé€šä¿¡å¤æ‚åº¦ä¸å¹¶è¡Œå¤æ‚åº¦å¯†åˆ‡ç›¸å…³ï¼Œå½±å“ç®—æ³•çš„å¯æ‰©å±•æ€§ã€‚

In parallel algorithms, communication complexity is closely related to parallel complexity, affecting algorithm scalability.

**ä¾‹å­ï¼šå¹¶è¡ŒçŸ©é˜µä¹˜æ³• / Example: Parallel Matrix Multiplication:**

- é—®é¢˜ï¼šåœ¨ $p$ ä¸ªå¤„ç†å™¨ä¸Šè®¡ç®—çŸ©é˜µä¹˜æ³•
- é€šä¿¡å¤æ‚åº¦ï¼š$\Omega(n^2/\sqrt{p})$
- åº”ç”¨ï¼šé«˜æ€§èƒ½è®¡ç®—ã€ç§‘å­¦è®¡ç®—

### 4.3 æµç®—æ³•

**åº”ç”¨åœºæ™¯ / Application Scenarios:**

æµç®—æ³•ä¸­çš„ç©ºé—´å¤æ‚åº¦ä¸‹ç•Œå¯ä»¥é€šè¿‡é€šä¿¡å¤æ‚åº¦ä¸‹ç•Œæ¥è¯æ˜ã€‚

Space complexity lower bounds in streaming algorithms can be proved using communication complexity lower bounds.

**å®šç† 4.3.1** æµç®—æ³•ä¸‹ç•Œï¼š

å¦‚æœå‡½æ•° $f$ çš„é€šä¿¡å¤æ‚åº¦ä¸º $CC(f)$ï¼Œåˆ™ä»»ä½•å•éæµç®—æ³•è®¡ç®— $f$ éœ€è¦è‡³å°‘ $\Omega(CC(f))$ ç©ºé—´ã€‚

**Theorem 4.3.1** Streaming algorithm lower bound:

If function $f$ has communication complexity $CC(f)$, then any single-pass streaming algorithm computing $f$ requires at least $\Omega(CC(f))$ space.

### 4.4 æ•°æ®æµä¸‹ç•Œ

**åº”ç”¨ / Applications:**

- **å…ƒç´ å”¯ä¸€æ€§ / Element Distinctness:** $\Omega(n)$ ç©ºé—´
- **é¢‘ç‡ä¼°è®¡ / Frequency Estimation:** $\Omega(1/\epsilon)$ ç©ºé—´
- **èŒƒå›´æŸ¥è¯¢ / Range Queries:** $\Omega(\log n)$ ç©ºé—´

**å®šç† 4.4.1** æµå¼è®¡æ•°çš„ç©ºé—´-è¯¯å·®ä¸‹ç•Œï¼š

å¯¹äºä¼°è®¡é¢‘æ•°çš„ $\epsilon$-è¿‘ä¼¼ï¼Œéœ€è¦ **Î©(1/ÎµÂ²)** ä½ç©ºé—´ï¼ˆå¦‚ Count-Min Sketchï¼‰ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- é€šè¿‡ **ä¿¡æ¯è®ºçš„ Fano ä¸ç­‰å¼**ï¼šè¯¯å·® $\epsilon$ ä¸å¯åŒºåˆ†çš„çŠ¶æ€æ•°å½¢æˆä¸‹ç•Œ

**Theorem 4.4.1** Streaming counting space-error lower bound:

For $\epsilon$-approximate frequency estimation, **Î©(1/ÎµÂ²)** bits of space are required (e.g., Count-Min Sketch).

**Proof sketch**:

- Through **information-theoretic Fano inequality**: error $\epsilon$ forms a lower bound with the number of distinguishable states

### 4.5 åˆ†å¸ƒå¼ä¸€è‡´æ€§ä¸‹ç•Œ

**å®šç† 4.5.1** å…±è¯†ï¼ˆPaxos/Raftï¼‰æ—¶ç©ºä¸‹ç•Œï¼š

åœ¨å¼‚æ­¥æ¶ˆæ¯-ä¼ é€’æ¨¡å‹ï¼ˆå¯å¤±æ•ˆï¼‰ä¸­ï¼Œåœ¨ $f$ ä¸ªæ•…éšœå®¹å¿çš„ç³»ç»Ÿä¸­ï¼Œ**å¿…éœ€** $\Omega(f+1)$ è½®äº¤äº’æ‰èƒ½è¾¾æˆä¸€è‡´ï¼ˆFLP ä¸å¯åˆ¤å®š + éšæœºåŒ–çªç ´ï¼‰ã€‚

**è¯æ˜è¦ç‚¹**ï¼š

- é€šè¿‡ **ä¸å¯åˆ¤å®šæ€§** ä¸ **ä¿¡æ¯æµ**ï¼šæ¯è½®æœ€å¤š $O(1)$ ä½ï¼Œå¿…é¡»æ”¶é›† $f+1$ ä¸ªä¸åŒæŠ•ç¥¨ä¿¡æ¯
- æ€»é€šä¿¡é‡ï¼š$\Omega(f \cdot n \cdot \log n)$ ä½ï¼ˆæ—¥å¿—å¤åˆ¶ï¼‰

**Theorem 4.5.1** Consensus (Paxos/Raft) space-time lower bound:

In the asynchronous message-passing model (with failures), in a system tolerating $f$ failures, **at least** $\Omega(f+1)$ rounds of interaction are required to reach consensus (FLP impossibility + randomization breakthrough).

**Proof sketch**:

- Through **impossibility** and **information flow**: at most $O(1)$ bits per round, must collect $f+1$ different voting information
- Total communication: $\Omega(f \cdot n \cdot \log n)$ bits (log replication)

### 4.6 ä¿¡æ¯é€šä¿¡ä¸­çš„ç®—æ³•å¤æ‚åº¦å…³é”®ç®—æ³•

**å…³é”®ç®—æ³•é€šä¿¡å¤æ‚åº¦åˆ†æ / Key Algorithm Communication Complexity Analysis:**

| # | æ•°æ®ç»“æ„ / ç®—æ³• | è®¡ç®—æ¨¡å‹ | è®¾è®¡ç®—æ³•èŒƒå¼ | ä¸»è¦é—®é¢˜ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´/æœ¬åœ°ä¿¡æ¯ | é€šä¿¡å¤æ‚åº¦ï¼ˆä½ï¼‰ | è½®æ•°ï¼ˆåŒæ­¥ï¼‰ | å¤æ‚åº¦ç±» | ç¡®å®šæ€§/éšæœºæ€§ | å¹¶è¡Œ/åˆ†å¸ƒå¼ç‰¹æ€§ | ä¿¡æ¯è®ºä¸‹ç•Œ | å…³é”®è¯æ˜æŠ€æœ¯ |
|---|----------------|----------|--------------|----------|------------|---------------|----------------|-------------|----------|----------------|----------------|------------|--------------|
| 1 | **MapReduce æ’åº** | PRAM / MapReduce | åˆ†æ²» + æ‰¹å¤„ç† | å¤§è§„æ¨¡å…¨å±€æ’åº | $\Theta(\log n)$ è½® | $O(n \cdot \text{word})$ ä½æœ¬åœ° | $\Theta(n \log n)$ ä½ï¼ˆshuffleï¼‰ | $\Theta(\log n)$ è½® | P | ç¡®å®šæ€§ | å¤§è§„æ¨¡åˆ†å¸ƒå¼ | **å…¨åºä¿¡æ¯** $\Omega(n \log n)$ ä½å¿…é¡»åœ¨ç½‘ç»œä¸­ä¼ æ’­ | ä¿¡æ¯å‹ç¼©ä¸‹ç•Œ + Shuffle åˆ†æ |
| 2 | **åˆ†å¸ƒå¼å“ˆå¸Œè¡¨ï¼ˆDHTï¼‰** | æ¶ˆæ¯-ä¼ é€’ (CONGEST) | éšæœºåŒ– (Chord, Kademlia) | é”®å€¼æŸ¥æ‰¾ | $\Theta(\log p)$ è·¯å¾„é•¿åº¦ | $O(1)$ æœ¬åœ° | $\Theta(\log p \cdot \log n)$ ä½ï¼ˆè·¯ç”±è¡¨ï¼‰ | $\Theta(\log p)$ è½® | P | éšæœºåŒ– | å®Œå…¨åˆ†å¸ƒå¼ | **Routing ä¿¡æ¯** å¿…é¡»ä¼ æ’­ $\Omega(\log n)$ ä½ | è·¯ç”±è¡¨ä¿¡æ¯è®ºä¸‹ç•Œ |
| 3 | **Count-Min Sketch** | æµå¼æ¨¡å‹ | éšæœºåŒ– (çº¿æ€§æ˜ å°„) | é¢‘æ•°è¿‘ä¼¼ | $O(1)$ æ›´æ–°/æŸ¥è¯¢ | $O(1/\varepsilon \cdot \log 1/\delta)$ ä½ | $0$ï¼ˆæœ¬åœ°ï¼‰ | 0 | BPP | éšæœºåŒ– | å¯å¹¶è¡Œï¼ˆåˆ†æ¡¶ï¼‰ | **ç©ºé—´-è¯¯å·®** $\Omega(1/\varepsilon^2)$ ä½ï¼ˆFanoï¼‰ | Fano ä¸ç­‰å¼ |
| 4 | **åˆ†å¸ƒå¼æœ€çŸ­è·¯ï¼ˆBellman-Ford å¹¶è¡Œï¼‰** | CONGEST | åˆ†æ²» + åŠ¨æ€ | å›¾éå† | $O(E \cdot \log n)$ è½® | $O(V)$ ä½æœ¬åœ° | $O(E \cdot \log n \cdot \log n)$ ä½ï¼ˆæ¯è½®å¹¿æ’­ï¼‰ | $O(\log n)$ è½® | P | ç¡®å®šæ€§ | æ¶ˆæ¯-ä¼ é€’ | **è·¯å¾„ä¿¡æ¯** $\Omega(n \log n)$ ä½å¿…é¡»åœ¨ç½‘ç»œä¸­ä¼ æ’­ | ä¿¡æ¯æµä¸‹ç•Œ |
| 5 | **Set-Disjointnessï¼ˆä¸¤æ–¹ï¼‰** | CONGEST | éšæœºåŒ–ï¼ˆåè®®ï¼‰ | é›†åˆäº¤å¹¶åˆ¤å®š | $\Omega(n)$ ä½é€šä¿¡ | $O(1)$ æœ¬åœ° | $\Omega(n)$ ä½ï¼ˆå¿…ä¼ ï¼‰ | $\Omega(1)$ è½® | BPP | éšæœºåŒ– | ä¸¤æ–¹æ¶ˆæ¯-ä¼ é€’ | **ä¿¡æ¯å¤æ‚åº¦** $\Omega(n)$ ä½ | ä¿¡æ¯å¤æ‚åº¦ + Yao's Principle |
| 6 | **Paxos / Raft å…±è¯†** | å¼‚æ­¥æ¶ˆæ¯-ä¼ é€’ | è´ªå¿ƒ/æŠ•ç¥¨ | åˆ†å¸ƒå¼ä¸€è‡´æ€§ | $O(f \cdot n)$ æ¶ˆæ¯ | $O(1)$ æœ¬åœ° | $\Omega(f \cdot n \cdot \log n)$ ä½ï¼ˆæ—¥å¿—å¤åˆ¶ï¼‰ | $\Omega(f+1)$ è½® | P (åœ¨å¯é ç½‘ç»œ) | ç¡®å®šæ€§/éšæœºåŒ–ï¼ˆéšæœº leaderï¼‰ | åˆ†å¸ƒå¼ç³»ç»Ÿ | **çŠ¶æ€æœºå¤åˆ¶** å¿…é¡»åœ¨å¤šæ•°èŠ‚ç‚¹é—´ä¼ æ’­ | FLP ä¸å¯åˆ¤å®š + ä¿¡æ¯æµ |
| 7 | **Grover é‡å­æœç´¢** | é‡å­å›¾çµæœº | é‡å­å¹¶è¡Œ | æœªæ’åºæœç´¢ | $\Theta(\sqrt{N})$ æ¬¡æŸ¥è¯¢ | $O(\log N)$ é‡å­æ¯”ç‰¹ | $\Theta(\sqrt{N} \cdot \log N)$ ä½ç­‰ä»·ä¿¡æ¯ | $\Theta(\sqrt{N})$ è½®ï¼ˆoracle è°ƒç”¨ï¼‰ | BQP | é‡å­ | é‡å­å¹¶è¡Œ | **é‡å­å¹…åº¦æ”¾å¤§** ä¸‹ç•Œ $\Omega(\sqrt{N})$ | Grover ä¸‹ç•Œè¯æ˜ |
| 8 | **åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ï¼ˆåŒæ­¥ SGDï¼‰** | å‚æ•°æœåŠ¡å™¨ (CONGEST) | éšæœºåŒ– (æ¢¯åº¦æŠ½æ ·) | å‚æ•°èšåˆ | $O(T \cdot \log p)$ è½®ï¼ˆT ä¸ºè¿­ä»£æ¬¡æ•°ï¼‰ | $O(d \cdot p)$ ä½æœ¬åœ°ï¼ˆæ¨¡å‹å‚æ•°ï¼‰ | $O(T \cdot d \cdot p \cdot \log n)$ ä½ï¼ˆæ¢¯åº¦åŒæ­¥ï¼‰ | $O(T \cdot \log p)$ è½® | P | éšæœºåŒ– | å‚æ•°æœåŠ¡å™¨ + All-Reduce | **æ¢¯åº¦ä¿¡æ¯** è‡³å°‘ $\Omega(d \cdot \log p)$ ä½æ¯è½® | ä¿¡æ¯å‹ç¼©ï¼ˆé‡åŒ–ã€ç¨€ç–ï¼‰ |
| 9 | **Gossipï¼ˆæ¶ˆæ¯å¹¿æ’­ï¼‰** | LOCAL | éšæœºåŒ–ï¼ˆéšæœºå¯¹ç­‰ï¼‰ | å…¨ç½‘å¹¿æ’­ | $\Theta(D)$ è½®ï¼ˆD ä¸ºç½‘ç»œç›´å¾„ï¼‰ | $O(1)$ æœ¬åœ° | $\Theta(D \cdot \log n)$ ä½ï¼ˆæ¯è½® 1 ä½ï¼‰ | $\Theta(D)$ è½® | P | éšæœºåŒ– | ç‚¹å¯¹ç‚¹ | **ä¿¡æ¯å¿…é¡»è·¨è¶Šæœ€é•¿è·¯å¾„** | è·¯å¾„ä¿¡æ¯ä¸‹ç•Œ |

> **ä¿¡æ¯è®ºä¸‹ç•Œçš„æ„ä¹‰** (Significance of Information-Theoretic Lower Bounds):
> è¿™äº›ä¸‹ç•Œè¡¨æ˜ï¼Œç®—æ³•çš„é€šä¿¡éœ€æ±‚å—åˆ°ä¿¡æ¯è®ºçš„æ ¹æœ¬é™åˆ¶ï¼Œæ— æ³•é€šè¿‡ç®—æ³•è®¾è®¡æŠ€å·§çªç ´è¿™äº›ç†è®ºæé™ã€‚
>
> These lower bounds show that algorithm communication requirements are fundamentally limited by information theory and cannot be overcome by algorithmic design techniques.

---

## 5. ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½

### 5.1 ç›¸å…³æ–‡æ¡£

- `04-ç®—æ³•å¤æ‚åº¦/01-æ—¶é—´å¤æ‚åº¦.md` - æ—¶é—´å¤æ‚åº¦åˆ†æ
- `04-ç®—æ³•å¤æ‚åº¦/06-ä¿¡æ¯è®ºä¸‹ç•Œ.md` - ä¿¡æ¯è®ºä¸‹ç•Œï¼ˆåŒ…å«é€šä¿¡å¤æ‚åº¦çš„ä¿¡æ¯è®ºæ–¹æ³•ï¼‰
- `01-åŸºç¡€ç†è®º/08-ä¿¡æ¯è®ºåŸºç¡€.md` - ä¿¡æ¯è®ºåŸºç¡€ç†è®º
- `09-ç®—æ³•ç†è®º/01-ç®—æ³•åŸºç¡€/22-ç®—æ³•å…­ç»´åˆ†ç±»æ¡†æ¶.md` - ç®—æ³•å…­ç»´åˆ†ç±»æ¡†æ¶
- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°æœ¬æ–‡æ¡£ï¼ˆåŸ `view/ç®—æ³•å…¨æ™¯æ¢³ç†-2025-01-11.md` Â§5ï¼‰

### 5.2 çŸ¥è¯†ä½“ç³»ä½ç½®

æœ¬æ–‡æ¡£å±äº **04-ç®—æ³•å¤æ‚åº¦** æ¨¡å—ï¼Œæ˜¯å¤æ‚åº¦åˆ†æçš„é‡è¦æ–‡æ¡£ï¼Œä¸ºåˆ†å¸ƒå¼ç®—æ³•å’Œé€šä¿¡åè®®æä¾›å¤æ‚åº¦ç†è®ºåŸºç¡€ã€‚

### 5.3 VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£

- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°æœ¬æ–‡æ¡£ Â§4.6ï¼ˆåŸ `view/ç®—æ³•å…¨æ™¯æ¢³ç†-2025-01-11.md` Â§5ï¼‰

---

## 6. å‚è€ƒæ–‡çŒ® / References

> **è¯´æ˜ / Note**: æœ¬æ–‡æ¡£çš„å‚è€ƒæ–‡çŒ®é‡‡ç”¨ç»Ÿä¸€çš„å¼•ç”¨æ ‡å‡†ï¼Œæ‰€æœ‰æ–‡çŒ®æ¡ç›®å‡æ¥è‡ª `docs/references_database.yaml` æ•°æ®åº“ã€‚æ–‡çŒ®æŒ‰ç…§"ç»å…¸å¥ åŸºæ–‡çŒ® â†’ æ ‡å‡†æ•™æ"çš„å±‚æ¬¡ç»„ç»‡ã€‚

### ç»å…¸å¥ åŸºæ–‡çŒ® / Classic Foundational Literature

1. **Yao, A. C.** (1979). "Some Complexity Questions Related to Distributive Computing". *Proceedings of the 11th Annual ACM Symposium on Theory of Computing*, 209-213.
   - **Yaoé€šä¿¡å¤æ‚åº¦ç†è®ºçš„å¼€åˆ›æ€§è®ºæ–‡**ï¼Œå¥ å®šäº†é€šä¿¡å¤æ‚åº¦ç†è®ºçš„åŸºç¡€ã€‚æœ¬æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹åŸºäºæ­¤è®ºæ–‡ã€‚

2. **Kushilevitz, E., & Nisan, N.** (1997). *Communication Complexity*. Cambridge University Press. ISBN: 978-0521029834
   - **Kushilevitz-Nisané€šä¿¡å¤æ‚åº¦çš„æƒå¨æ•™æ**ï¼Œç³»ç»Ÿä»‹ç»äº†é€šä¿¡å¤æ‚åº¦ç†è®ºã€‚æœ¬æ–‡æ¡£çš„åŸºç¡€æ¡†æ¶å‚è€ƒæ­¤ä¹¦ã€‚

### æ ‡å‡†æ•™æ / Standard Textbooks

1. **Arora, S., & Barak, B.** (2009). *Computational Complexity: A Modern Approach*. Cambridge University Press. ISBN: 978-0521424264
   - Arora-Barakè®¡ç®—å¤æ‚åº¦ç†è®ºçš„ç°ä»£ç»¼åˆï¼ŒåŒ…å«é€šä¿¡å¤æ‚åº¦ç« èŠ‚ã€‚

2. **Roughgarden, T.** (2016). *Communication Complexity (for Algorithm Designers)*. Foundations and Trends in Theoretical Computer Science, 11(3-4), 217-404.
   - Roughgardené€šä¿¡å¤æ‚åº¦çš„ç®—æ³•è®¾è®¡è§†è§’ï¼Œå¼ºè°ƒåº”ç”¨ã€‚

**åœ¨çº¿èµ„æº / Online Resources**:

1. **Wikipedia - Communication Complexity**: <https://en.wikipedia.org/wiki/Communication_complexity>
   - é€šä¿¡å¤æ‚åº¦çš„Wikipediaæ¡ç›®ï¼ŒåŒ…å«åŸºæœ¬å®šä¹‰ã€æ¨¡å‹å’Œåº”ç”¨ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

2. **Wikipedia - Distributed Computing**: <https://en.wikipedia.org/wiki/Distributed_computing>
   - åˆ†å¸ƒå¼è®¡ç®—çš„Wikipediaæ¡ç›®ï¼Œä»‹ç»é€šä¿¡å¤æ‚åº¦åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

3. **Wikipedia - Streaming Algorithm**: <https://en.wikipedia.org/wiki/Streaming_algorithm>
   - æµç®—æ³•çš„Wikipediaæ¡ç›®ï¼Œè¯´æ˜é€šä¿¡å¤æ‚åº¦ä¸æµç®—æ³•ä¸‹ç•Œçš„å…³ç³»ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

**å¼•ç”¨è§„èŒƒè¯´æ˜ / Citation Guidelines**:

æœ¬æ–‡æ¡£éµå¾ªé¡¹ç›®å¼•ç”¨è§„èŒƒï¼ˆè§ `docs/å¼•ç”¨è§„èŒƒä¸æ•°æ®åº“.md`ï¼‰ã€‚æ‰€æœ‰å¼•ç”¨æ¡ç›®åœ¨ `docs/references_database.yaml` ä¸­æœ‰å®Œæ•´è®°å½•ã€‚

æœ¬æ–‡æ¡£å†…å®¹å·²å¯¹ç…§Wikipediaç›¸å…³æ¡ç›®ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿æœ¯è¯­å®šä¹‰å’Œç†è®ºæ¡†æ¶ä¸å½“å‰å­¦æœ¯æ ‡å‡†ä¸€è‡´ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬ / Document Version**: 1.0
****æœ€åæ›´æ–° / Last Updated**: 2025-01-11
**çŠ¶æ€ / Status**: å·²å¯¹ç…§Wikipediaæ›´æ–° / Updated with Wikipedia references (as of 2025-01-11)

---

*æœ¬æ–‡æ¡£æä¾›äº†é€šä¿¡å¤æ‚åº¦çš„å…¨é¢ç†è®ºæ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºæœ¬æ¦‚å¿µã€é€šä¿¡æ¨¡å‹ã€ä¸‹ç•ŒæŠ€æœ¯å’Œåº”ç”¨é¢†åŸŸã€‚æ‰€æœ‰å†…å®¹å‡é‡‡ç”¨ä¸¥æ ¼çš„æ•°å­¦å½¢å¼åŒ–è¡¨ç¤ºï¼Œç¬¦åˆå›½é™…å­¦æœ¯æ ‡å‡†å’ŒWikiè§„èŒƒã€‚*
