---
title: 1.8 信息论基础 / Information Theory Fundamentals
version: 1.1
status: maintained
last_updated: 2025-11-14
owner: 基础理论工作组
---

## 1.8 信息论基础 / Information Theory Fundamentals

### 摘要 / Executive Summary

- 建立信息论的基础理论，统一信息量、熵、信道容量等核心概念。
- 建立信息论在算法理论中的基础地位。

### 关键术语与符号 / Glossary

- 信息量、熵、条件熵、互信息、信道容量、编码理论。
- 术语对齐与引用规范：`docs/术语与符号总表.md`，`01-基础理论/00-撰写规范与引用指南.md`

### 术语与符号规范 / Terminology & Notation

- 信息量（Information Content）：事件发生的信息量。
- 熵（Entropy）：随机变量的平均信息量。
- 条件熵（Conditional Entropy）：给定条件下随机变量的平均信息量。
- 互信息（Mutual Information）：两个随机变量之间的信息量。
- 记号约定：`H(X)` 表示熵，`H(X|Y)` 表示条件熵，`I(X;Y)` 表示互信息。

### 交叉引用导航 / Cross-References

- 概率与统计基础：参见 `01-基础理论/07-概率与统计基础.md`。
- 数学基础：参见 `01-基础理论/02-数学基础.md`。
- 算法理论：参见 `09-算法理论/` 相关文档。

### 快速导航 / Quick Links

- 基本概念
- 熵理论
- 编码理论

## 目录 (Table of Contents)

- [1.8 信息论基础 / Information Theory Fundamentals](#18-信息论基础--information-theory-fundamentals)
  - [概述 / Overview](#概述--overview)
    - [核心概念 / Core Concepts](#核心概念--core-concepts)
    - [信息量定义 / Information Content Definition](#信息量定义--information-content-definition)
  - [熵理论 / Entropy Theory](#熵理论--entropy-theory)
    - [香农熵 / Shannon Entropy](#香农熵--shannon-entropy)
    - [熵的性质](#熵的性质)
  - [信道理论](#信道理论)
    - [信道模型](#信道模型)
    - [信道容量](#信道容量)
  - [编码理论](#编码理论)
    - [信源编码](#信源编码)
    - [信道编码](#信道编码)
  - [率失真理论](#率失真理论)
    - [率失真函数](#率失真函数)
  - [应用示例](#应用示例)
    - [数据压缩](#数据压缩)
    - [信道编码应用](#信道编码应用)
  - [总结](#总结)
  - [参考文献 / References](#参考文献--references)

## 概述 / Overview

信息论（Information Theory）是研究信息传输、存储和处理的数学理论，由香农（Claude Shannon）在1948年创立。为算法分析、数据压缩、通信系统与机器学习提供理论基础。

Information theory is the mathematical theory of information transmission, storage, and processing, founded by Claude Shannon in 1948. It provides theoretical foundations for algorithm analysis, data compression, communication systems, and machine learning.

### 核心概念 / Core Concepts

1. **信息量 Information Content**: 衡量信息的不确定性
2. **熵 Entropy**: 信息源的平均信息量
3. **信道容量 Channel Capacity**: 信道的最大传输能力
4. **编码理论 Coding Theory**: 高效可靠的信息编码方法

### 信息量定义 / Information Content Definition

**定义 1.1** 自信息量定义为 $I(x) = -\log p(x)$，其中 $p(x)$ 为事件 $x$ 的概率。

The self-information of an event $x$ is defined as $I(x) = -\log p(x)$, where $p(x)$ is the probability of event $x$.

**定义 1.2** 联合信息量定义为 $I(x,y) = -\log p(x,y)$。

The joint information of events $x$ and $y$ is defined as $I(x,y) = -\log p(x,y)$.

**定义 1.3** 条件信息量定义为 $I(x|y) = -\log p(x|y)$。

The conditional information of event $x$ given event $y$ is defined as $I(x|y) = -\log p(x|y)$.

```rust
// 信息量的基本定义 / Basic Definition of Information Content
pub struct InformationTheory {
    base: f64, // 对数的底数（通常为2） / Base of logarithm (usually 2)
}

impl InformationTheory {
    pub fn new(base: f64) -> Self {
        Self { base }
    }
    
    // 自信息量：I(x) = -log(p(x)) / Self-information: I(x) = -log(p(x))
    pub fn self_information(&self, probability: f64) -> f64 {
        if probability <= 0.0 || probability > 1.0 {
            return f64::INFINITY;
        }
        -probability.log(self.base)
    }
    
    // 联合信息量：I(x,y) = -log(p(x,y)) / Joint information: I(x,y) = -log(p(x,y))
    pub fn joint_information(&self, joint_probability: f64) -> f64 {
        self.self_information(joint_probability)
    }
    
    // 条件信息量：I(x|y) = -log(p(x|y)) / Conditional information: I(x|y) = -log(p(x|y))
    pub fn conditional_information(&self, conditional_probability: f64) -> f64 {
        self.self_information(conditional_probability)
    }
}
```

## 熵理论 / Entropy Theory

### 香农熵 / Shannon Entropy

**定义 2.1** 离散随机变量 $X$ 的香农熵定义为 $H(X) = -\sum_{x} p(x) \log p(x)$。

The Shannon entropy of a discrete random variable $X$ is defined as $H(X) = -\sum_{x} p(x) \log p(x)$.

**定义 2.2** 联合熵定义为 $H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y)$。

The joint entropy is defined as $H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y)$.

**定义 2.3** 条件熵定义为 $H(X|Y) = -\sum_{x,y} p(x,y) \log p(x|y)$。

The conditional entropy is defined as $H(X|Y) = -\sum_{x,y} p(x,y) \log p(x|y)$.

**定义 2.4** 互信息定义为 $I(X;Y) = H(X) - H(X|Y)$。

The mutual information is defined as $I(X;Y) = H(X) - H(X|Y)$.

```rust
// 香农熵计算 / Shannon Entropy Calculation
pub struct ShannonEntropy {
    base: f64,
}

impl ShannonEntropy {
    pub fn new(base: f64) -> Self {
        Self { base }
    }
    
    // 离散随机变量的熵：H(X) = -Σ p(x) * log(p(x)) / Entropy of discrete RV: H(X) = -Σ p(x) * log(p(x))
    pub fn entropy(&self, probabilities: &[f64]) -> f64 {
        probabilities.iter()
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }
    
    // 联合熵：H(X,Y) = -Σ p(x,y) * log(p(x,y)) / Joint entropy: H(X,Y) = -Σ p(x,y) * log(p(x,y))
    pub fn joint_entropy(&self, joint_probabilities: &[Vec<f64>]) -> f64 {
        joint_probabilities.iter()
            .flat_map(|row| row.iter())
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }
    
    // 条件熵：H(X|Y) = -Σ p(x,y) * log(p(x|y)) / Conditional entropy: H(X|Y) = -Σ p(x,y) * log(p(x|y))
    pub fn conditional_entropy(&self, joint_probabilities: &[Vec<f64>], marginal_probabilities: &[f64]) -> f64 {
        let mut conditional_entropy = 0.0;
        
        for (i, row) in joint_probabilities.iter().enumerate() {
            for (j, &joint_prob) in row.iter().enumerate() {
                if joint_prob > 0.0 && marginal_probabilities[i] > 0.0 {
                    let conditional_prob = joint_prob / marginal_probabilities[i];
                    conditional_entropy -= joint_prob * conditional_prob.log(self.base);
                }
            }
        }
        
        conditional_entropy
    }
    
    // 互信息：I(X;Y) = H(X) - H(X|Y) / Mutual information: I(X;Y) = H(X) - H(X|Y)
    pub fn mutual_information(&self, joint_probabilities: &[Vec<f64>], x_marginal: &[f64], y_marginal: &[f64]) -> f64 {
        let h_x = self.entropy(x_marginal);
        let h_x_given_y = self.conditional_entropy(joint_probabilities, y_marginal);
        h_x - h_x_given_y
    }
}
```

### 熵的性质

```rust
// 熵的基本性质
pub struct EntropyProperties;

impl EntropyProperties {
    // 非负性：H(X) ≥ 0
    pub fn non_negativity(entropy: f64) -> bool {
        entropy >= 0.0
    }
    
    // 对称性：H(X,Y) = H(Y,X)
    pub fn symmetry(joint_entropy_xy: f64, joint_entropy_yx: f64) -> bool {
        (joint_entropy_xy - joint_entropy_yx).abs() < f64::EPSILON
    }
    
    // 链式法则：H(X,Y) = H(X) + H(Y|X)
    pub fn chain_rule(joint_entropy: f64, marginal_entropy: f64, conditional_entropy: f64) -> bool {
        (joint_entropy - (marginal_entropy + conditional_entropy)).abs() < f64::EPSILON
    }
    
    // 最大熵原理：在给定约束下，熵最大的分布最不确定
    pub fn maximum_entropy_principle(&self, constraints: &[Constraint]) -> Distribution {
        // 使用拉格朗日乘数法求解最大熵分布
        self.solve_maximum_entropy(constraints)
    }
}
```

## 信道理论

### 信道模型

```rust
// 离散无记忆信道
pub struct DiscreteMemorylessChannel {
    transition_matrix: Vec<Vec<f64>>, // P(Y|X)
    input_alphabet_size: usize,
    output_alphabet_size: usize,
}

impl DiscreteMemorylessChannel {
    pub fn new(transition_matrix: Vec<Vec<f64>>) -> Result<Self, ChannelError> {
        // 验证转移矩阵的有效性
        for row in &transition_matrix {
            let sum: f64 = row.iter().sum();
            if (sum - 1.0).abs() > f64::EPSILON {
                return Err(ChannelError::InvalidTransitionMatrix);
            }
        }
        
        Ok(Self {
            input_alphabet_size: transition_matrix.len(),
            output_alphabet_size: transition_matrix[0].len(),
            transition_matrix,
        })
    }
    
    // 计算输出概率：P(Y) = Σ P(X) * P(Y|X)
    pub fn output_probability(&self, input_distribution: &[f64], output: usize) -> f64 {
        input_distribution.iter()
            .enumerate()
            .map(|(input, &input_prob)| {
                input_prob * self.transition_matrix[input][output]
            })
            .sum()
    }
    
    // 计算后验概率：P(X|Y) = P(X,Y) / P(Y)
    pub fn posterior_probability(&self, input_distribution: &[f64], input: usize, output: usize) -> f64 {
        let joint_prob = input_distribution[input] * self.transition_matrix[input][output];
        let output_prob = self.output_probability(input_distribution, output);
        
        if output_prob > 0.0 {
            joint_prob / output_prob
        } else {
            0.0
        }
    }
}
```

### 信道容量

```rust
// 信道容量计算
pub struct ChannelCapacity {
    channel: DiscreteMemorylessChannel,
}

impl ChannelCapacity {
    pub fn new(channel: DiscreteMemorylessChannel) -> Self {
        Self { channel }
    }
    
    // 信道容量：C = max I(X;Y)
    pub fn capacity(&self) -> f64 {
        // 使用迭代算法求解最大互信息
        self.maximize_mutual_information()
    }
    
    // 使用Blahut-Arimoto算法最大化互信息
    fn maximize_mutual_information(&self) -> f64 {
        let mut input_distribution = vec![1.0 / self.channel.input_alphabet_size as f64; self.channel.input_alphabet_size];
        let mut max_mutual_info = 0.0;
        
        for _ in 0..100 { // 最大迭代次数
            // 计算互信息
            let mutual_info = self.calculate_mutual_information(&input_distribution);
            max_mutual_info = max_mutual_info.max(mutual_info);
            
            // 更新输入分布
            input_distribution = self.update_input_distribution(&input_distribution);
        }
        
        max_mutual_info
    }
    
    fn calculate_mutual_information(&self, input_distribution: &[f64]) -> f64 {
        let mut mutual_info = 0.0;
        
        for input in 0..self.channel.input_alphabet_size {
            for output in 0..self.channel.output_alphabet_size {
                let joint_prob = input_distribution[input] * self.channel.transition_matrix[input][output];
                if joint_prob > 0.0 {
                    let output_prob = self.channel.output_probability(input_distribution, output);
                    if output_prob > 0.0 {
                        mutual_info += joint_prob * (joint_prob / (input_distribution[input] * output_prob)).log(2.0);
                    }
                }
            }
        }
        
        mutual_info
    }
    
    fn update_input_distribution(&self, current_distribution: &[f64]) -> Vec<f64> {
        let mut new_distribution = vec![0.0; self.channel.input_alphabet_size];
        let mut normalization_factor = 0.0;
        
        for input in 0..self.channel.input_alphabet_size {
            let mut exp_term = 0.0;
            for output in 0..self.channel.output_alphabet_size {
                if self.channel.transition_matrix[input][output] > 0.0 {
                    let output_prob = self.channel.output_probability(current_distribution, output);
                    if output_prob > 0.0 {
                        exp_term += self.channel.transition_matrix[input][output] * 
                                  (self.channel.transition_matrix[input][output] / output_prob).log(2.0);
                    }
                }
            }
            new_distribution[input] = current_distribution[input] * 2.0_f64.powf(exp_term);
            normalization_factor += new_distribution[input];
        }
        
        // 归一化
        for prob in &mut new_distribution {
            *prob /= normalization_factor;
        }
        
        new_distribution
    }
}
```

## 编码理论

### 信源编码

```rust
// 霍夫曼编码
pub struct HuffmanCoding {
    symbol_probabilities: Vec<(char, f64)>,
}

impl HuffmanCoding {
    pub fn new(symbol_probabilities: Vec<(char, f64)>) -> Self {
        Self { symbol_probabilities }
    }
    
    pub fn encode(&self) -> HashMap<char, String> {
        let mut codes = HashMap::new();
        let mut nodes = self.build_initial_nodes();
        
        // 构建霍夫曼树
        while nodes.len() > 1 {
            // 选择两个最小概率的节点
            nodes.sort_by(|a, b| a.probability.partial_cmp(&b.probability).unwrap());
            
            let left = nodes.remove(0);
            let right = nodes.remove(0);
            
            // 创建父节点
            let parent = HuffmanNode {
                symbol: None,
                probability: left.probability + right.probability,
                left: Some(Box::new(left)),
                right: Some(Box::new(right)),
            };
            
            nodes.push(parent);
        }
        
        // 生成编码
        if let Some(root) = nodes.first() {
            self.generate_codes(root, String::new(), &mut codes);
        }
        
        codes
    }
    
    fn build_initial_nodes(&self) -> Vec<HuffmanNode> {
        self.symbol_probabilities.iter()
            .map(|(symbol, prob)| HuffmanNode {
                symbol: Some(*symbol),
                probability: *prob,
                left: None,
                right: None,
            })
            .collect()
    }
    
    fn generate_codes(&self, node: &HuffmanNode, current_code: String, codes: &mut HashMap<char, String>) {
        if let Some(symbol) = node.symbol {
            codes.insert(symbol, current_code);
        } else {
            if let Some(ref left) = node.left {
                self.generate_codes(left, current_code.clone() + "0", codes);
            }
            if let Some(ref right) = node.right {
                self.generate_codes(right, current_code + "1", codes);
            }
        }
    }
}

#[derive(Clone)]
struct HuffmanNode {
    symbol: Option<char>,
    probability: f64,
    left: Option<Box<HuffmanNode>>,
    right: Option<Box<HuffmanNode>>,
}
```

### 信道编码

```rust
// 汉明码
pub struct HammingCode {
    data_bits: usize,
    parity_bits: usize,
    total_bits: usize,
}

impl HammingCode {
    pub fn new(data_bits: usize) -> Self {
        let parity_bits = Self::calculate_parity_bits(data_bits);
        let total_bits = data_bits + parity_bits;
        
        Self {
            data_bits,
            parity_bits,
            total_bits,
        }
    }
    
    fn calculate_parity_bits(data_bits: usize) -> usize {
        let mut parity_bits = 1;
        while (1 << parity_bits) < data_bits + parity_bits + 1 {
            parity_bits += 1;
        }
        parity_bits
    }
    
    // 编码
    pub fn encode(&self, data: &[bool]) -> Vec<bool> {
        assert_eq!(data.len(), self.data_bits);
        
        let mut encoded = vec![false; self.total_bits];
        let mut data_index = 0;
        
        // 放置数据位
        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                encoded[i] = data[data_index];
                data_index += 1;
            }
        }
        
        // 计算校验位
        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            encoded[parity_position] = self.calculate_parity_bit(&encoded, i);
        }
        
        encoded
    }
    
    // 解码和纠错
    pub fn decode(&self, received: &[bool]) -> Result<Vec<bool>, DecodingError> {
        assert_eq!(received.len(), self.total_bits);
        
        // 计算综合征
        let syndrome = self.calculate_syndrome(received);
        
        if syndrome == 0 {
            // 无错误，提取数据
            Ok(self.extract_data(received))
        } else {
            // 检测到错误，尝试纠错
            let error_position = syndrome - 1;
            if error_position < self.total_bits {
                let mut corrected = received.to_vec();
                corrected[error_position] = !corrected[error_position];
                Ok(self.extract_data(&corrected))
            } else {
                Err(DecodingError::UncorrectableError)
            }
        }
    }
    
    fn is_power_of_two(&self, n: usize) -> bool {
        n > 0 && (n & (n - 1)) == 0
    }
    
    fn calculate_parity_bit(&self, data: &[bool], parity_index: usize) -> bool {
        let mut parity = false;
        let parity_mask = 1 << parity_index;
        
        for (i, &bit) in data.iter().enumerate() {
            if (i + 1) & parity_mask != 0 {
                parity ^= bit;
            }
        }
        
        parity
    }
    
    fn calculate_syndrome(&self, received: &[bool]) -> usize {
        let mut syndrome = 0;
        
        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            if self.calculate_parity_bit(received, i) != received[parity_position] {
                syndrome += 1 << i;
            }
        }
        
        syndrome
    }
    
    fn extract_data(&self, encoded: &[bool]) -> Vec<bool> {
        let mut data = Vec::new();
        
        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                data.push(encoded[i]);
            }
        }
        
        data
    }
}

#[derive(Debug)]
pub enum DecodingError {
    UncorrectableError,
}
```

## 率失真理论

### 率失真函数

```rust
// 率失真理论
pub struct RateDistortionTheory {
    distortion_measure: Box<dyn DistortionMeasure>,
}

impl RateDistortionTheory {
    pub fn new(distortion_measure: Box<dyn DistortionMeasure>) -> Self {
        Self { distortion_measure }
    }
    
    // 率失真函数：R(D) = min I(X;X̂) subject to E[d(X,X̂)] ≤ D
    pub fn rate_distortion_function(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // 使用拉格朗日乘数法求解
        self.minimize_rate_under_distortion_constraint(source_distribution, target_distortion)
    }
    
    fn minimize_rate_under_distortion_constraint(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // 简化的迭代算法
        let mut lambda = 1.0; // 拉格朗日乘数
        let mut rate = 0.0;
        
        for _ in 0..100 {
            // 更新条件概率分布
            let conditional_distribution = self.update_conditional_distribution(source_distribution, lambda);
            
            // 计算当前失真和率
            let current_distortion = self.calculate_distortion(source_distribution, &conditional_distribution);
            rate = self.calculate_rate(source_distribution, &conditional_distribution);
            
            // 更新拉格朗日乘数
            lambda *= (current_distortion / target_distortion).powf(0.1);
        }
        
        rate
    }
    
    fn update_conditional_distribution(&self, source_distribution: &[f64], lambda: f64) -> Vec<Vec<f64>> {
        // 基于拉格朗日乘数更新条件概率分布
        // 这里简化实现
        vec![vec![1.0 / source_distribution.len() as f64; source_distribution.len()]; source_distribution.len()]
    }
    
    fn calculate_distortion(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        let mut total_distortion = 0.0;
        
        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                let distortion = self.distortion_measure.measure(i, j);
                total_distortion += source_prob * conditional_prob * distortion;
            }
        }
        
        total_distortion
    }
    
    fn calculate_rate(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        // 计算互信息作为率
        let mut rate = 0.0;
        
        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                if source_prob > 0.0 && conditional_prob > 0.0 {
                    let joint_prob = source_prob * conditional_prob;
                    rate += joint_prob * (conditional_prob / source_prob).log(2.0);
                }
            }
        }
        
        rate
    }
}

pub trait DistortionMeasure {
    fn measure(&self, x: usize, y: usize) -> f64;
}

// 汉明失真度量
pub struct HammingDistortion;

impl DistortionMeasure for HammingDistortion {
    fn measure(&self, x: usize, y: usize) -> f64 {
        if x == y { 0.0 } else { 1.0 }
    }
}
```

## 应用示例

### 数据压缩

```rust
// 数据压缩系统
pub struct DataCompression {
    huffman_coding: HuffmanCoding,
    arithmetic_coding: ArithmeticCoding,
}

impl DataCompression {
    pub fn compress_huffman(&self, data: &str) -> (Vec<u8>, HashMap<char, String>) {
        // 计算符号频率
        let mut frequencies = HashMap::new();
        for ch in data.chars() {
            *frequencies.entry(ch).or_insert(0) += 1;
        }
        
        // 转换为概率
        let total = data.len() as f64;
        let symbol_probabilities: Vec<(char, f64)> = frequencies
            .iter()
            .map(|(&ch, &count)| (ch, count as f64 / total))
            .collect();
        
        // 生成霍夫曼编码
        let huffman = HuffmanCoding::new(symbol_probabilities);
        let codes = huffman.encode();
        
        // 编码数据
        let encoded_data = self.encode_with_codes(data, &codes);
        
        (encoded_data, codes)
    }
    
    fn encode_with_codes(&self, data: &str, codes: &HashMap<char, String>) -> Vec<u8> {
        let mut encoded_bits = String::new();
        
        for ch in data.chars() {
            if let Some(code) = codes.get(&ch) {
                encoded_bits.push_str(code);
            }
        }
        
        // 转换为字节
        self.bits_to_bytes(&encoded_bits)
    }
    
    fn bits_to_bytes(&self, bits: &str) -> Vec<u8> {
        let mut bytes = Vec::new();
        let mut current_byte = 0u8;
        let mut bit_count = 0;
        
        for bit in bits.chars() {
            current_byte = (current_byte << 1) | if bit == '1' { 1 } else { 0 };
            bit_count += 1;
            
            if bit_count == 8 {
                bytes.push(current_byte);
                current_byte = 0;
                bit_count = 0;
            }
        }
        
        // 处理剩余的位
        if bit_count > 0 {
            current_byte <<= 8 - bit_count;
            bytes.push(current_byte);
        }
        
        bytes
    }
}
```

### 信道编码应用

```rust
// 通信系统
pub struct CommunicationSystem {
    hamming_code: HammingCode,
    channel: DiscreteMemorylessChannel,
}

impl CommunicationSystem {
    pub fn new() -> Self {
        Self {
            hamming_code: HammingCode::new(4), // 4位数据，3位校验
            channel: DiscreteMemorylessChannel::new(vec![
                vec![0.9, 0.1], // 输入0的输出概率
                vec![0.1, 0.9], // 输入1的输出概率
            ]).unwrap(),
        }
    }
    
    pub fn transmit(&self, data: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        // 1. 信道编码
        let encoded = self.hamming_code.encode(data);
        
        // 2. 通过信道传输
        let received = self.transmit_through_channel(&encoded)?;
        
        // 3. 信道解码
        let decoded = self.hamming_code.decode(&received)?;
        
        Ok(decoded)
    }
    
    fn transmit_through_channel(&self, encoded: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        let mut received = Vec::new();
        
        for &bit in encoded {
            let input = if bit { 1 } else { 0 };
            let output = self.simulate_channel_output(input);
            received.push(output == 1);
        }
        
        Ok(received)
    }
    
    fn simulate_channel_output(&self, input: usize) -> usize {
        let random = rand::random::<f64>();
        let threshold = self.channel.transition_matrix[input][0];
        
        if random < threshold { 0 } else { 1 }
    }
}

#[derive(Debug)]
pub enum CommunicationError {
    DecodingError(DecodingError),
    ChannelError(ChannelError),
}

#[derive(Debug)]
pub enum ChannelError {
    InvalidTransitionMatrix,
}
```

## 总结

信息论基础为现代通信和数据处理提供了坚实的数学基础：

1. **熵理论**: 量化信息的不确定性和信息量
2. **信道理论**: 分析信息传输的极限和最优策略
3. **编码理论**: 设计高效可靠的信息编码方法
4. **率失真理论**: 在失真约束下优化信息压缩

这些理论在数据压缩、错误纠正、通信系统、机器学习等领域有广泛应用。

---

## 参考文献 / References

本文档基于已发表的学术文献和公开资料编写。以下是主要参考文献：

**经典奠基文献 / Classic Foundational Literature**:

1. [Shannon1948] Shannon, C. E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*, 27(3): 379-423. DOI: 10.1002/j.1538-7305.1948.tb01338.x
   - 信息论的奠基性论文，定义了熵、信道容量等核心概念，开创了信息论领域。

**标准教材 / Standard Textbooks**:

1. [CoverThomas2006] Cover, T. M., & Thomas, J. A. (2006). *Elements of Information Theory* (2nd Edition). Wiley-Interscience. ISBN: 978-0471241959.
   - 信息论的经典教材，详细介绍熵、互信息、信道容量和率失真理论，本文档的主要参考。

2. [MacKay2003] MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. ISBN: 978-0521642981.
   - 信息论与机器学习的综合教材，连接信息论与实际应用。

**现代综合与高级主题 / Modern Synthesis and Advanced Topics**:

1. [Yeung2008] Yeung, R. W. (2008). *Information Theory and Network Coding*. Springer. ISBN: 978-0387792330.
   - 信息论与网络编码的现代综合，扩展了传统信息论到网络环境。

2. [Csiszar2011] Csiszár, I., & Körner, J. (2011). *Information Theory: Coding Theorems for Discrete Memoryless Systems* (2nd Edition). Cambridge University Press. ISBN: 978-0521196819.
   - 编码定理的严格数学处理，高级信息论的权威参考。

**算法与复杂度连接 / Connection to Algorithms and Complexity**:

1. [Sipser2012] Sipser, M. (2012). *Introduction to the Theory of Computation* (3rd Edition). Cengage Learning. ISBN: 978-1133187790.
   - 计算理论导论，包含Kolmogorov复杂度和信息论在算法中的应用。

2. [AroraBarak2009] Arora, S., & Barak, B. (2009). *Computational Complexity: A Modern Approach*. Cambridge University Press. ISBN: 978-0521424264.
   - 计算复杂度理论，讨论信息论在复杂度分析中的应用。

**在线资源 / Online Resources**:

1. **Wikipedia - Information Theory**: <https://en.wikipedia.org/wiki/Information_theory>
   - 信息论的Wikipedia条目，包含Shannon熵、互信息等核心概念（截至2025年11月14日）。

2. **Wikipedia - Entropy (Information Theory)**: <https://en.wikipedia.org/wiki/Entropy_(information_theory)>
   - 信息熵的Wikipedia条目，详细介绍Shannon熵的定义和性质（截至2025年11月14日）。

3. **Wikipedia - Channel Capacity**: <https://en.wikipedia.org/wiki/Channel_capacity>
   - 信道容量的Wikipedia条目，包含Shannon定理和编码理论（截至2025年11月14日）。

4. MIT OpenCourseWare - "6.441 Information Theory": <https://ocw.mit.edu/courses/6-441-information-theory-spring-2016/>
   - MIT信息论课程的免费在线资源。

5. Stanford Encyclopedia of Philosophy - Information: <https://plato.stanford.edu/>
   - 信息论的哲学讨论和概念解析。

**引用规范说明 / Citation Guidelines**:

本文档遵循项目引用规范（见 `docs/引用规范与数据库.md`）。所有引用条目在 `docs/references_database.yaml` 中有完整记录。

本文档内容已对照Wikipedia相关条目（截至2025年11月14日）进行验证，确保术语定义和理论框架与当前学术标准一致。

---

**文档版本 / Document Version**: 1.1  
**最后更新 / Last Updated**: 2025-11-14  
**状态 / Status**: 已对照Wikipedia更新 / Updated with Wikipedia references (as of 2025-11-14)

---

*本文档展示了信息论的基础理论，通过严格的数学定义和算法实现，为信息处理提供了理论基础。*
