# 信息论基础 - 信息科学的数学基础

## 基本概念

### 信息论概述

信息论（Information Theory）是研究信息传输、存储和处理的数学理论，由香农（Claude Shannon）在1948年创立。核心概念包括：

1. **信息量**: 衡量信息的不确定性
2. **熵**: 信息源的平均信息量
3. **信道容量**: 信道的最大传输能力
4. **编码理论**: 高效可靠的信息编码方法

### 信息量定义

```rust
// 信息量的基本定义
pub struct InformationTheory {
    base: f64, // 对数的底数（通常为2）
}

impl InformationTheory {
    pub fn new(base: f64) -> Self {
        Self { base }
    }
    
    // 自信息量：I(x) = -log(p(x))
    pub fn self_information(&self, probability: f64) -> f64 {
        if probability <= 0.0 || probability > 1.0 {
            return f64::INFINITY;
        }
        -probability.log(self.base)
    }
    
    // 联合信息量：I(x,y) = -log(p(x,y))
    pub fn joint_information(&self, joint_probability: f64) -> f64 {
        self.self_information(joint_probability)
    }
    
    // 条件信息量：I(x|y) = -log(p(x|y))
    pub fn conditional_information(&self, conditional_probability: f64) -> f64 {
        self.self_information(conditional_probability)
    }
}
```

## 熵理论

### 香农熵

```rust
// 香农熵计算
pub struct ShannonEntropy {
    base: f64,
}

impl ShannonEntropy {
    pub fn new(base: f64) -> Self {
        Self { base }
    }
    
    // 离散随机变量的熵：H(X) = -Σ p(x) * log(p(x))
    pub fn entropy(&self, probabilities: &[f64]) -> f64 {
        probabilities.iter()
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }
    
    // 联合熵：H(X,Y) = -Σ p(x,y) * log(p(x,y))
    pub fn joint_entropy(&self, joint_probabilities: &[Vec<f64>]) -> f64 {
        joint_probabilities.iter()
            .flat_map(|row| row.iter())
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }
    
    // 条件熵：H(X|Y) = -Σ p(x,y) * log(p(x|y))
    pub fn conditional_entropy(&self, joint_probabilities: &[Vec<f64>], marginal_probabilities: &[f64]) -> f64 {
        let mut conditional_entropy = 0.0;
        
        for (i, row) in joint_probabilities.iter().enumerate() {
            for (j, &joint_prob) in row.iter().enumerate() {
                if joint_prob > 0.0 && marginal_probabilities[i] > 0.0 {
                    let conditional_prob = joint_prob / marginal_probabilities[i];
                    conditional_entropy -= joint_prob * conditional_prob.log(self.base);
                }
            }
        }
        
        conditional_entropy
    }
    
    // 互信息：I(X;Y) = H(X) - H(X|Y)
    pub fn mutual_information(&self, joint_probabilities: &[Vec<f64>], x_marginal: &[f64], y_marginal: &[f64]) -> f64 {
        let h_x = self.entropy(x_marginal);
        let h_x_given_y = self.conditional_entropy(joint_probabilities, y_marginal);
        h_x - h_x_given_y
    }
}
```

### 熵的性质

```rust
// 熵的基本性质
pub struct EntropyProperties;

impl EntropyProperties {
    // 非负性：H(X) ≥ 0
    pub fn non_negativity(entropy: f64) -> bool {
        entropy >= 0.0
    }
    
    // 对称性：H(X,Y) = H(Y,X)
    pub fn symmetry(joint_entropy_xy: f64, joint_entropy_yx: f64) -> bool {
        (joint_entropy_xy - joint_entropy_yx).abs() < f64::EPSILON
    }
    
    // 链式法则：H(X,Y) = H(X) + H(Y|X)
    pub fn chain_rule(joint_entropy: f64, marginal_entropy: f64, conditional_entropy: f64) -> bool {
        (joint_entropy - (marginal_entropy + conditional_entropy)).abs() < f64::EPSILON
    }
    
    // 最大熵原理：在给定约束下，熵最大的分布最不确定
    pub fn maximum_entropy_principle(&self, constraints: &[Constraint]) -> Distribution {
        // 使用拉格朗日乘数法求解最大熵分布
        self.solve_maximum_entropy(constraints)
    }
}
```

## 信道理论

### 信道模型

```rust
// 离散无记忆信道
pub struct DiscreteMemorylessChannel {
    transition_matrix: Vec<Vec<f64>>, // P(Y|X)
    input_alphabet_size: usize,
    output_alphabet_size: usize,
}

impl DiscreteMemorylessChannel {
    pub fn new(transition_matrix: Vec<Vec<f64>>) -> Result<Self, ChannelError> {
        // 验证转移矩阵的有效性
        for row in &transition_matrix {
            let sum: f64 = row.iter().sum();
            if (sum - 1.0).abs() > f64::EPSILON {
                return Err(ChannelError::InvalidTransitionMatrix);
            }
        }
        
        Ok(Self {
            input_alphabet_size: transition_matrix.len(),
            output_alphabet_size: transition_matrix[0].len(),
            transition_matrix,
        })
    }
    
    // 计算输出概率：P(Y) = Σ P(X) * P(Y|X)
    pub fn output_probability(&self, input_distribution: &[f64], output: usize) -> f64 {
        input_distribution.iter()
            .enumerate()
            .map(|(input, &input_prob)| {
                input_prob * self.transition_matrix[input][output]
            })
            .sum()
    }
    
    // 计算后验概率：P(X|Y) = P(X,Y) / P(Y)
    pub fn posterior_probability(&self, input_distribution: &[f64], input: usize, output: usize) -> f64 {
        let joint_prob = input_distribution[input] * self.transition_matrix[input][output];
        let output_prob = self.output_probability(input_distribution, output);
        
        if output_prob > 0.0 {
            joint_prob / output_prob
        } else {
            0.0
        }
    }
}
```

### 信道容量

```rust
// 信道容量计算
pub struct ChannelCapacity {
    channel: DiscreteMemorylessChannel,
}

impl ChannelCapacity {
    pub fn new(channel: DiscreteMemorylessChannel) -> Self {
        Self { channel }
    }
    
    // 信道容量：C = max I(X;Y)
    pub fn capacity(&self) -> f64 {
        // 使用迭代算法求解最大互信息
        self.maximize_mutual_information()
    }
    
    // 使用Blahut-Arimoto算法最大化互信息
    fn maximize_mutual_information(&self) -> f64 {
        let mut input_distribution = vec![1.0 / self.channel.input_alphabet_size as f64; self.channel.input_alphabet_size];
        let mut max_mutual_info = 0.0;
        
        for _ in 0..100 { // 最大迭代次数
            // 计算互信息
            let mutual_info = self.calculate_mutual_information(&input_distribution);
            max_mutual_info = max_mutual_info.max(mutual_info);
            
            // 更新输入分布
            input_distribution = self.update_input_distribution(&input_distribution);
        }
        
        max_mutual_info
    }
    
    fn calculate_mutual_information(&self, input_distribution: &[f64]) -> f64 {
        let mut mutual_info = 0.0;
        
        for input in 0..self.channel.input_alphabet_size {
            for output in 0..self.channel.output_alphabet_size {
                let joint_prob = input_distribution[input] * self.channel.transition_matrix[input][output];
                if joint_prob > 0.0 {
                    let output_prob = self.channel.output_probability(input_distribution, output);
                    if output_prob > 0.0 {
                        mutual_info += joint_prob * (joint_prob / (input_distribution[input] * output_prob)).log(2.0);
                    }
                }
            }
        }
        
        mutual_info
    }
    
    fn update_input_distribution(&self, current_distribution: &[f64]) -> Vec<f64> {
        let mut new_distribution = vec![0.0; self.channel.input_alphabet_size];
        let mut normalization_factor = 0.0;
        
        for input in 0..self.channel.input_alphabet_size {
            let mut exp_term = 0.0;
            for output in 0..self.channel.output_alphabet_size {
                if self.channel.transition_matrix[input][output] > 0.0 {
                    let output_prob = self.channel.output_probability(current_distribution, output);
                    if output_prob > 0.0 {
                        exp_term += self.channel.transition_matrix[input][output] * 
                                  (self.channel.transition_matrix[input][output] / output_prob).log(2.0);
                    }
                }
            }
            new_distribution[input] = current_distribution[input] * 2.0_f64.powf(exp_term);
            normalization_factor += new_distribution[input];
        }
        
        // 归一化
        for prob in &mut new_distribution {
            *prob /= normalization_factor;
        }
        
        new_distribution
    }
}
```

## 编码理论

### 信源编码

```rust
// 霍夫曼编码
pub struct HuffmanCoding {
    symbol_probabilities: Vec<(char, f64)>,
}

impl HuffmanCoding {
    pub fn new(symbol_probabilities: Vec<(char, f64)>) -> Self {
        Self { symbol_probabilities }
    }
    
    pub fn encode(&self) -> HashMap<char, String> {
        let mut codes = HashMap::new();
        let mut nodes = self.build_initial_nodes();
        
        // 构建霍夫曼树
        while nodes.len() > 1 {
            // 选择两个最小概率的节点
            nodes.sort_by(|a, b| a.probability.partial_cmp(&b.probability).unwrap());
            
            let left = nodes.remove(0);
            let right = nodes.remove(0);
            
            // 创建父节点
            let parent = HuffmanNode {
                symbol: None,
                probability: left.probability + right.probability,
                left: Some(Box::new(left)),
                right: Some(Box::new(right)),
            };
            
            nodes.push(parent);
        }
        
        // 生成编码
        if let Some(root) = nodes.first() {
            self.generate_codes(root, String::new(), &mut codes);
        }
        
        codes
    }
    
    fn build_initial_nodes(&self) -> Vec<HuffmanNode> {
        self.symbol_probabilities.iter()
            .map(|(symbol, prob)| HuffmanNode {
                symbol: Some(*symbol),
                probability: *prob,
                left: None,
                right: None,
            })
            .collect()
    }
    
    fn generate_codes(&self, node: &HuffmanNode, current_code: String, codes: &mut HashMap<char, String>) {
        if let Some(symbol) = node.symbol {
            codes.insert(symbol, current_code);
        } else {
            if let Some(ref left) = node.left {
                self.generate_codes(left, current_code.clone() + "0", codes);
            }
            if let Some(ref right) = node.right {
                self.generate_codes(right, current_code + "1", codes);
            }
        }
    }
}

#[derive(Clone)]
struct HuffmanNode {
    symbol: Option<char>,
    probability: f64,
    left: Option<Box<HuffmanNode>>,
    right: Option<Box<HuffmanNode>>,
}
```

### 信道编码

```rust
// 汉明码
pub struct HammingCode {
    data_bits: usize,
    parity_bits: usize,
    total_bits: usize,
}

impl HammingCode {
    pub fn new(data_bits: usize) -> Self {
        let parity_bits = Self::calculate_parity_bits(data_bits);
        let total_bits = data_bits + parity_bits;
        
        Self {
            data_bits,
            parity_bits,
            total_bits,
        }
    }
    
    fn calculate_parity_bits(data_bits: usize) -> usize {
        let mut parity_bits = 1;
        while (1 << parity_bits) < data_bits + parity_bits + 1 {
            parity_bits += 1;
        }
        parity_bits
    }
    
    // 编码
    pub fn encode(&self, data: &[bool]) -> Vec<bool> {
        assert_eq!(data.len(), self.data_bits);
        
        let mut encoded = vec![false; self.total_bits];
        let mut data_index = 0;
        
        // 放置数据位
        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                encoded[i] = data[data_index];
                data_index += 1;
            }
        }
        
        // 计算校验位
        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            encoded[parity_position] = self.calculate_parity_bit(&encoded, i);
        }
        
        encoded
    }
    
    // 解码和纠错
    pub fn decode(&self, received: &[bool]) -> Result<Vec<bool>, DecodingError> {
        assert_eq!(received.len(), self.total_bits);
        
        // 计算综合征
        let syndrome = self.calculate_syndrome(received);
        
        if syndrome == 0 {
            // 无错误，提取数据
            Ok(self.extract_data(received))
        } else {
            // 检测到错误，尝试纠错
            let error_position = syndrome - 1;
            if error_position < self.total_bits {
                let mut corrected = received.to_vec();
                corrected[error_position] = !corrected[error_position];
                Ok(self.extract_data(&corrected))
            } else {
                Err(DecodingError::UncorrectableError)
            }
        }
    }
    
    fn is_power_of_two(&self, n: usize) -> bool {
        n > 0 && (n & (n - 1)) == 0
    }
    
    fn calculate_parity_bit(&self, data: &[bool], parity_index: usize) -> bool {
        let mut parity = false;
        let parity_mask = 1 << parity_index;
        
        for (i, &bit) in data.iter().enumerate() {
            if (i + 1) & parity_mask != 0 {
                parity ^= bit;
            }
        }
        
        parity
    }
    
    fn calculate_syndrome(&self, received: &[bool]) -> usize {
        let mut syndrome = 0;
        
        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            if self.calculate_parity_bit(received, i) != received[parity_position] {
                syndrome += 1 << i;
            }
        }
        
        syndrome
    }
    
    fn extract_data(&self, encoded: &[bool]) -> Vec<bool> {
        let mut data = Vec::new();
        
        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                data.push(encoded[i]);
            }
        }
        
        data
    }
}

#[derive(Debug)]
pub enum DecodingError {
    UncorrectableError,
}
```

## 率失真理论

### 率失真函数

```rust
// 率失真理论
pub struct RateDistortionTheory {
    distortion_measure: Box<dyn DistortionMeasure>,
}

impl RateDistortionTheory {
    pub fn new(distortion_measure: Box<dyn DistortionMeasure>) -> Self {
        Self { distortion_measure }
    }
    
    // 率失真函数：R(D) = min I(X;X̂) subject to E[d(X,X̂)] ≤ D
    pub fn rate_distortion_function(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // 使用拉格朗日乘数法求解
        self.minimize_rate_under_distortion_constraint(source_distribution, target_distortion)
    }
    
    fn minimize_rate_under_distortion_constraint(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // 简化的迭代算法
        let mut lambda = 1.0; // 拉格朗日乘数
        let mut rate = 0.0;
        
        for _ in 0..100 {
            // 更新条件概率分布
            let conditional_distribution = self.update_conditional_distribution(source_distribution, lambda);
            
            // 计算当前失真和率
            let current_distortion = self.calculate_distortion(source_distribution, &conditional_distribution);
            rate = self.calculate_rate(source_distribution, &conditional_distribution);
            
            // 更新拉格朗日乘数
            lambda *= (current_distortion / target_distortion).powf(0.1);
        }
        
        rate
    }
    
    fn update_conditional_distribution(&self, source_distribution: &[f64], lambda: f64) -> Vec<Vec<f64>> {
        // 基于拉格朗日乘数更新条件概率分布
        // 这里简化实现
        vec![vec![1.0 / source_distribution.len() as f64; source_distribution.len()]; source_distribution.len()]
    }
    
    fn calculate_distortion(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        let mut total_distortion = 0.0;
        
        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                let distortion = self.distortion_measure.measure(i, j);
                total_distortion += source_prob * conditional_prob * distortion;
            }
        }
        
        total_distortion
    }
    
    fn calculate_rate(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        // 计算互信息作为率
        let mut rate = 0.0;
        
        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                if source_prob > 0.0 && conditional_prob > 0.0 {
                    let joint_prob = source_prob * conditional_prob;
                    rate += joint_prob * (conditional_prob / source_prob).log(2.0);
                }
            }
        }
        
        rate
    }
}

pub trait DistortionMeasure {
    fn measure(&self, x: usize, y: usize) -> f64;
}

// 汉明失真度量
pub struct HammingDistortion;

impl DistortionMeasure for HammingDistortion {
    fn measure(&self, x: usize, y: usize) -> f64 {
        if x == y { 0.0 } else { 1.0 }
    }
}
```

## 应用示例

### 数据压缩

```rust
// 数据压缩系统
pub struct DataCompression {
    huffman_coding: HuffmanCoding,
    arithmetic_coding: ArithmeticCoding,
}

impl DataCompression {
    pub fn compress_huffman(&self, data: &str) -> (Vec<u8>, HashMap<char, String>) {
        // 计算符号频率
        let mut frequencies = HashMap::new();
        for ch in data.chars() {
            *frequencies.entry(ch).or_insert(0) += 1;
        }
        
        // 转换为概率
        let total = data.len() as f64;
        let symbol_probabilities: Vec<(char, f64)> = frequencies
            .iter()
            .map(|(&ch, &count)| (ch, count as f64 / total))
            .collect();
        
        // 生成霍夫曼编码
        let huffman = HuffmanCoding::new(symbol_probabilities);
        let codes = huffman.encode();
        
        // 编码数据
        let encoded_data = self.encode_with_codes(data, &codes);
        
        (encoded_data, codes)
    }
    
    fn encode_with_codes(&self, data: &str, codes: &HashMap<char, String>) -> Vec<u8> {
        let mut encoded_bits = String::new();
        
        for ch in data.chars() {
            if let Some(code) = codes.get(&ch) {
                encoded_bits.push_str(code);
            }
        }
        
        // 转换为字节
        self.bits_to_bytes(&encoded_bits)
    }
    
    fn bits_to_bytes(&self, bits: &str) -> Vec<u8> {
        let mut bytes = Vec::new();
        let mut current_byte = 0u8;
        let mut bit_count = 0;
        
        for bit in bits.chars() {
            current_byte = (current_byte << 1) | if bit == '1' { 1 } else { 0 };
            bit_count += 1;
            
            if bit_count == 8 {
                bytes.push(current_byte);
                current_byte = 0;
                bit_count = 0;
            }
        }
        
        // 处理剩余的位
        if bit_count > 0 {
            current_byte <<= 8 - bit_count;
            bytes.push(current_byte);
        }
        
        bytes
    }
}
```

### 信道编码应用

```rust
// 通信系统
pub struct CommunicationSystem {
    hamming_code: HammingCode,
    channel: DiscreteMemorylessChannel,
}

impl CommunicationSystem {
    pub fn new() -> Self {
        Self {
            hamming_code: HammingCode::new(4), // 4位数据，3位校验
            channel: DiscreteMemorylessChannel::new(vec![
                vec![0.9, 0.1], // 输入0的输出概率
                vec![0.1, 0.9], // 输入1的输出概率
            ]).unwrap(),
        }
    }
    
    pub fn transmit(&self, data: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        // 1. 信道编码
        let encoded = self.hamming_code.encode(data);
        
        // 2. 通过信道传输
        let received = self.transmit_through_channel(&encoded)?;
        
        // 3. 信道解码
        let decoded = self.hamming_code.decode(&received)?;
        
        Ok(decoded)
    }
    
    fn transmit_through_channel(&self, encoded: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        let mut received = Vec::new();
        
        for &bit in encoded {
            let input = if bit { 1 } else { 0 };
            let output = self.simulate_channel_output(input);
            received.push(output == 1);
        }
        
        Ok(received)
    }
    
    fn simulate_channel_output(&self, input: usize) -> usize {
        let random = rand::random::<f64>();
        let threshold = self.channel.transition_matrix[input][0];
        
        if random < threshold { 0 } else { 1 }
    }
}

#[derive(Debug)]
pub enum CommunicationError {
    DecodingError(DecodingError),
    ChannelError(ChannelError),
}

#[derive(Debug)]
pub enum ChannelError {
    InvalidTransitionMatrix,
}
```

## 总结

信息论基础为现代通信和数据处理提供了坚实的数学基础：

1. **熵理论**: 量化信息的不确定性和信息量
2. **信道理论**: 分析信息传输的极限和最优策略
3. **编码理论**: 设计高效可靠的信息编码方法
4. **率失真理论**: 在失真约束下优化信息压缩

这些理论在数据压缩、错误纠正、通信系统、机器学习等领域有广泛应用。

---

*本文档展示了信息论的基础理论，通过严格的数学定义和算法实现，为信息处理提供了理论基础。*
