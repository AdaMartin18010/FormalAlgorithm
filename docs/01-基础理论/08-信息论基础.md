---
title: 1.8 ä¿¡æ¯è®ºåŸºç¡€ / Information Theory Fundamentals
version: 1.1
status: maintained
last_updated: 2025-01-11
owner: åŸºç¡€ç†è®ºå·¥ä½œç»„
---

> ğŸ“Š **é¡¹ç›®å…¨é¢æ¢³ç†**ï¼šè¯¦ç»†çš„é¡¹ç›®ç»“æ„ã€æ¨¡å—è¯¦è§£å’Œå­¦ä¹ è·¯å¾„ï¼Œè¯·å‚é˜… [`é¡¹ç›®å…¨é¢æ¢³ç†-2025.md`](../é¡¹ç›®å…¨é¢æ¢³ç†-2025.md)

## 1.8 ä¿¡æ¯è®ºåŸºç¡€ / Information Theory Fundamentals

### æ‘˜è¦ / Executive Summary

- å»ºç«‹ä¿¡æ¯è®ºçš„åŸºç¡€ç†è®ºï¼Œç»Ÿä¸€ä¿¡æ¯é‡ã€ç†µã€ä¿¡é“å®¹é‡ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚
- å»ºç«‹ä¿¡æ¯è®ºåœ¨ç®—æ³•ç†è®ºä¸­çš„åŸºç¡€åœ°ä½ã€‚

### å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary

- ä¿¡æ¯é‡ã€ç†µã€æ¡ä»¶ç†µã€äº’ä¿¡æ¯ã€ä¿¡é“å®¹é‡ã€ç¼–ç ç†è®ºã€‚
- æœ¯è¯­å¯¹é½ä¸å¼•ç”¨è§„èŒƒï¼š`docs/æœ¯è¯­ä¸ç¬¦å·æ€»è¡¨.md`ï¼Œ`01-åŸºç¡€ç†è®º/00-æ’°å†™è§„èŒƒä¸å¼•ç”¨æŒ‡å—.md`

### æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ / Terminology & Notation

- ä¿¡æ¯é‡ï¼ˆInformation Contentï¼‰ï¼šäº‹ä»¶å‘ç”Ÿçš„ä¿¡æ¯é‡ã€‚
- ç†µï¼ˆEntropyï¼‰ï¼šéšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡ã€‚
- æ¡ä»¶ç†µï¼ˆConditional Entropyï¼‰ï¼šç»™å®šæ¡ä»¶ä¸‹éšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡ã€‚
- äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰ï¼šä¸¤ä¸ªéšæœºå˜é‡ä¹‹é—´çš„ä¿¡æ¯é‡ã€‚
- è®°å·çº¦å®šï¼š`H(X)` è¡¨ç¤ºç†µï¼Œ`H(X|Y)` è¡¨ç¤ºæ¡ä»¶ç†µï¼Œ`I(X;Y)` è¡¨ç¤ºäº’ä¿¡æ¯ã€‚

### äº¤å‰å¼•ç”¨å¯¼èˆª / Cross-References

- æ¦‚ç‡ä¸ç»Ÿè®¡åŸºç¡€ï¼šå‚è§ `01-åŸºç¡€ç†è®º/07-æ¦‚ç‡ä¸ç»Ÿè®¡åŸºç¡€.md`ã€‚
- æ•°å­¦åŸºç¡€ï¼šå‚è§ `01-åŸºç¡€ç†è®º/02-æ•°å­¦åŸºç¡€.md`ã€‚
- ç®—æ³•ç†è®ºï¼šå‚è§ `09-ç®—æ³•ç†è®º/` ç›¸å…³æ–‡æ¡£ã€‚

### å¿«é€Ÿå¯¼èˆª / Quick Links

- åŸºæœ¬æ¦‚å¿µ
- ç†µç†è®º
- ç¼–ç ç†è®º

## ç›®å½• (Table of Contents)

- [1.8 ä¿¡æ¯è®ºåŸºç¡€ / Information Theory Fundamentals](#18-ä¿¡æ¯è®ºåŸºç¡€--information-theory-fundamentals)
  - [æ‘˜è¦ / Executive Summary](#æ‘˜è¦--executive-summary)
  - [å…³é”®æœ¯è¯­ä¸ç¬¦å· / Glossary](#å…³é”®æœ¯è¯­ä¸ç¬¦å·--glossary)
  - [æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ / Terminology \& Notation](#æœ¯è¯­ä¸ç¬¦å·è§„èŒƒ--terminology--notation)
  - [äº¤å‰å¼•ç”¨å¯¼èˆª / Cross-References](#äº¤å‰å¼•ç”¨å¯¼èˆª--cross-references)
  - [å¿«é€Ÿå¯¼èˆª / Quick Links](#å¿«é€Ÿå¯¼èˆª--quick-links)
- [ç›®å½• (Table of Contents)](#ç›®å½•-table-of-contents)
- [æ¦‚è¿° / Overview](#æ¦‚è¿°--overview)
  - [æ ¸å¿ƒæ¦‚å¿µ / Core Concepts](#æ ¸å¿ƒæ¦‚å¿µ--core-concepts)
  - [ä¿¡æ¯é‡å®šä¹‰ / Information Content Definition](#ä¿¡æ¯é‡å®šä¹‰--information-content-definition)
  - [ä¿¡æ¯Â·æ•°æ®Â·æ•°æ®ç»“æ„çš„å®šä¹‰ / Definition of Information, Data, and Data Structures](#ä¿¡æ¯æ•°æ®æ•°æ®ç»“æ„çš„å®šä¹‰--definition-of-information-data-and-data-structures)
- [ç†µç†è®º / Entropy Theory](#ç†µç†è®º--entropy-theory)
  - [é¦™å†œç†µ / Shannon Entropy](#é¦™å†œç†µ--shannon-entropy)
  - [ç†µçš„æ€§è´¨](#ç†µçš„æ€§è´¨)
- [ä¿¡é“ç†è®º](#ä¿¡é“ç†è®º)
  - [ä¿¡é“æ¨¡å‹](#ä¿¡é“æ¨¡å‹)
  - [ä¿¡é“å®¹é‡](#ä¿¡é“å®¹é‡)
- [ç¼–ç ç†è®º](#ç¼–ç ç†è®º)
  - [ä¿¡æºç¼–ç ](#ä¿¡æºç¼–ç )
  - [ä¿¡é“ç¼–ç ](#ä¿¡é“ç¼–ç )
- [ç‡å¤±çœŸç†è®º](#ç‡å¤±çœŸç†è®º)
  - [ç‡å¤±çœŸå‡½æ•°](#ç‡å¤±çœŸå‡½æ•°)
- [åº”ç”¨ç¤ºä¾‹](#åº”ç”¨ç¤ºä¾‹)
  - [æ•°æ®å‹ç¼©](#æ•°æ®å‹ç¼©)
  - [ä¿¡é“ç¼–ç åº”ç”¨](#ä¿¡é“ç¼–ç åº”ç”¨)
- [æ€»ç»“](#æ€»ç»“)
- [å‚è€ƒæ–‡çŒ® / References](#å‚è€ƒæ–‡çŒ®--references)
- [ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½ / Alignment with Project Structure](#ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½--alignment-with-project-structure)
  - [ç›¸å…³æ–‡æ¡£ / Related Documents](#ç›¸å…³æ–‡æ¡£--related-documents)
  - [çŸ¥è¯†ä½“ç³»ä½ç½® / Knowledge System Position](#çŸ¥è¯†ä½“ç³»ä½ç½®--knowledge-system-position)
  - [VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£ / VIEW Folder Related Documents](#viewæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£--view-folder-related-documents)

## æ¦‚è¿° / Overview

ä¿¡æ¯è®ºï¼ˆInformation Theoryï¼‰æ˜¯ç ”ç©¶ä¿¡æ¯ä¼ è¾“ã€å­˜å‚¨å’Œå¤„ç†çš„æ•°å­¦ç†è®ºï¼Œç”±é¦™å†œï¼ˆClaude Shannonï¼‰åœ¨1948å¹´åˆ›ç«‹ã€‚ä¸ºç®—æ³•åˆ†æã€æ•°æ®å‹ç¼©ã€é€šä¿¡ç³»ç»Ÿä¸æœºå™¨å­¦ä¹ æä¾›ç†è®ºåŸºç¡€ã€‚

Information theory is the mathematical theory of information transmission, storage, and processing, founded by Claude Shannon in 1948. It provides theoretical foundations for algorithm analysis, data compression, communication systems, and machine learning.

### æ ¸å¿ƒæ¦‚å¿µ / Core Concepts

1. **ä¿¡æ¯é‡ Information Content**: è¡¡é‡ä¿¡æ¯çš„ä¸ç¡®å®šæ€§
2. **ç†µ Entropy**: ä¿¡æ¯æºçš„å¹³å‡ä¿¡æ¯é‡
3. **ä¿¡é“å®¹é‡ Channel Capacity**: ä¿¡é“çš„æœ€å¤§ä¼ è¾“èƒ½åŠ›
4. **ç¼–ç ç†è®º Coding Theory**: é«˜æ•ˆå¯é çš„ä¿¡æ¯ç¼–ç æ–¹æ³•

### ä¿¡æ¯é‡å®šä¹‰ / Information Content Definition

**å®šä¹‰ 1.1** è‡ªä¿¡æ¯é‡å®šä¹‰ä¸º $I(x) = -\log p(x)$ï¼Œå…¶ä¸­ $p(x)$ ä¸ºäº‹ä»¶ $x$ çš„æ¦‚ç‡ã€‚

The self-information of an event $x$ is defined as $I(x) = -\log p(x)$, where $p(x)$ is the probability of event $x$.

**å®šä¹‰ 1.2** è”åˆä¿¡æ¯é‡å®šä¹‰ä¸º $I(x,y) = -\log p(x,y)$ã€‚

The joint information of events $x$ and $y$ is defined as $I(x,y) = -\log p(x,y)$.

**å®šä¹‰ 1.3** æ¡ä»¶ä¿¡æ¯é‡å®šä¹‰ä¸º $I(x|y) = -\log p(x|y)$ã€‚

The conditional information of event $x$ given event $y$ is defined as $I(x|y) = -\log p(x|y)$.

### ä¿¡æ¯Â·æ•°æ®Â·æ•°æ®ç»“æ„çš„å®šä¹‰ / Definition of Information, Data, and Data Structures

**å®šä¹‰ 1.4** (ä¿¡æ¯Â·æ•°æ®Â·æ•°æ®ç»“æ„çš„å½¢å¼åŒ–å®šä¹‰) è®¾ï¼š
**Definition 1.4** (Formal Definition of Information, Data, and Data Structures) Let:

| ç¬¦å· | å«ä¹‰ | å½¢å¼åŒ–æè¿° |
|------|------|-----------|
| **Î£** | æœ‰é™å­—ç¬¦é›†ï¼ˆå­—æ¯è¡¨ï¼‰ | å¦‚ Î£ = {0,1} æˆ– ASCII |
| **æ•°æ® D** | Î£* ä¸­çš„æœ‰é™ä¸²ï¼Œè®°ä½œ `D âˆˆ Î£*` | ä»»ä½•å¯å­˜å‚¨ã€å¯ä¼ è¾“çš„ç¦»æ•£å¯¹è±¡ |
| **ä¿¡æ¯é‡ I(D)** | å¯¹æ•°æ® D çš„ä¸ç¡®å®šæ€§åº¦é‡ | **Shannon ç†µ** `H(D) = -âˆ‘_x p(x) logâ‚‚ p(x)`ï¼ˆéšæœºæºï¼‰<br>**Kolmogorov å¤æ‚åº¦** `K(D)`ï¼ˆæœ€çŸ­è‡ªæè¿°é•¿åº¦ï¼‰ |
| **æ•°æ®ç»“æ„ DS** | å…·æœ‰ **åº•å±‚è¡¨ç¤º** `repr: DS â†’ Î£*` ä¸ **æ“ä½œé›†åˆ** `Ops(DS) = {opâ‚,â€¦,op_k}` çš„æŠ½è±¡ä»£æ•°å¯¹è±¡ | ä¾‹ï¼šäºŒå‰æœç´¢æ ‘ `BST = (Node, left, right, key, â€¦)`ï¼Œ`Ops = {search, insert, delete}` |

| Symbol | Meaning | Formal Description |
|--------|---------|-------------------|
| **Î£** | Finite alphabet | e.g., Î£ = {0,1} or ASCII |
| **Data D** | Finite string in Î£*, denoted as `D âˆˆ Î£*` | Any storable, transmittable discrete object |
| **Information Content I(D)** | Uncertainty measure of data D | **Shannon entropy** `H(D) = -âˆ‘_x p(x) logâ‚‚ p(x)` (random source)<br>**Kolmogorov complexity** `K(D)` (shortest self-description length) |
| **Data Structure DS** | Abstract algebraic object with **underlying representation** `repr: DS â†’ Î£*` and **operation set** `Ops(DS) = {opâ‚,â€¦,op_k}` | Example: Binary search tree `BST = (Node, left, right, key, â€¦)`, `Ops = {search, insert, delete}` |

> **ä¿¡æ¯-æ•°æ®-ç»“æ„çš„å…³ç³» / Information-Data-Structure Relationship**:
>
> - `I(D)` ä¸º **æ•°æ®çš„å†…åœ¨éš¾åº¦**ï¼ˆå‹ç¼©æé™ï¼‰ã€‚
> - `I(D)` is the **intrinsic difficulty of data** (compression limit).
>
> - **æ•°æ®ç»“æ„** é€šè¿‡ **ç»„ç»‡æ–¹å¼** æŠŠ `D` çš„ **æŸ¥è¯¢/æ›´æ–°** ä»£ä»·æ˜ å°„åˆ° **æ—¶é—´/ç©ºé—´å¤æ‚åº¦**ã€‚
> - **Data structures** map the **query/update** cost of `D` to **time/space complexity** through **organization**.
>
> - **ç®—æ³•** åœ¨ç‰¹å®š **è®¡ç®—æ¨¡å‹** ä¸Šå®ç°å¯¹ `DS` çš„æ“ä½œï¼Œä»è€Œ **åˆ©ç”¨æˆ–å…‹æœä¿¡æ¯çš„ä¸å¯å‹ç¼©æ€§**ã€‚
> - **Algorithms** implement operations on `DS` in a specific **computation model**, thereby **utilizing or overcoming the incompressibility of information**.

```rust
// ä¿¡æ¯é‡çš„åŸºæœ¬å®šä¹‰ / Basic Definition of Information Content
pub struct InformationTheory {
    base: f64, // å¯¹æ•°çš„åº•æ•°ï¼ˆé€šå¸¸ä¸º2ï¼‰ / Base of logarithm (usually 2)
}

impl InformationTheory {
    pub fn new(base: f64) -> Self {
        Self { base }
    }

    // è‡ªä¿¡æ¯é‡ï¼šI(x) = -log(p(x)) / Self-information: I(x) = -log(p(x))
    pub fn self_information(&self, probability: f64) -> f64 {
        if probability <= 0.0 || probability > 1.0 {
            return f64::INFINITY;
        }
        -probability.log(self.base)
    }

    // è”åˆä¿¡æ¯é‡ï¼šI(x,y) = -log(p(x,y)) / Joint information: I(x,y) = -log(p(x,y))
    pub fn joint_information(&self, joint_probability: f64) -> f64 {
        self.self_information(joint_probability)
    }

    // æ¡ä»¶ä¿¡æ¯é‡ï¼šI(x|y) = -log(p(x|y)) / Conditional information: I(x|y) = -log(p(x|y))
    pub fn conditional_information(&self, conditional_probability: f64) -> f64 {
        self.self_information(conditional_probability)
    }
}
```

## ç†µç†è®º / Entropy Theory

### é¦™å†œç†µ / Shannon Entropy

**å®šä¹‰ 2.1** ç¦»æ•£éšæœºå˜é‡ $X$ çš„é¦™å†œç†µå®šä¹‰ä¸º $H(X) = -\sum_{x} p(x) \log p(x)$ã€‚

The Shannon entropy of a discrete random variable $X$ is defined as $H(X) = -\sum_{x} p(x) \log p(x)$.

**å®šä¹‰ 2.2** è”åˆç†µå®šä¹‰ä¸º $H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y)$ã€‚

The joint entropy is defined as $H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y)$.

**å®šä¹‰ 2.3** æ¡ä»¶ç†µå®šä¹‰ä¸º $H(X|Y) = -\sum_{x,y} p(x,y) \log p(x|y)$ã€‚

The conditional entropy is defined as $H(X|Y) = -\sum_{x,y} p(x,y) \log p(x|y)$.

**å®šä¹‰ 2.4** äº’ä¿¡æ¯å®šä¹‰ä¸º $I(X;Y) = H(X) - H(X|Y)$ã€‚

The mutual information is defined as $I(X;Y) = H(X) - H(X|Y)$.

```rust
// é¦™å†œç†µè®¡ç®— / Shannon Entropy Calculation
pub struct ShannonEntropy {
    base: f64,
}

impl ShannonEntropy {
    pub fn new(base: f64) -> Self {
        Self { base }
    }

    // ç¦»æ•£éšæœºå˜é‡çš„ç†µï¼šH(X) = -Î£ p(x) * log(p(x)) / Entropy of discrete RV: H(X) = -Î£ p(x) * log(p(x))
    pub fn entropy(&self, probabilities: &[f64]) -> f64 {
        probabilities.iter()
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }

    // è”åˆç†µï¼šH(X,Y) = -Î£ p(x,y) * log(p(x,y)) / Joint entropy: H(X,Y) = -Î£ p(x,y) * log(p(x,y))
    pub fn joint_entropy(&self, joint_probabilities: &[Vec<f64>]) -> f64 {
        joint_probabilities.iter()
            .flat_map(|row| row.iter())
            .filter(|&&p| p > 0.0)
            .map(|&p| -p * p.log(self.base))
            .sum()
    }

    // æ¡ä»¶ç†µï¼šH(X|Y) = -Î£ p(x,y) * log(p(x|y)) / Conditional entropy: H(X|Y) = -Î£ p(x,y) * log(p(x|y))
    pub fn conditional_entropy(&self, joint_probabilities: &[Vec<f64>], marginal_probabilities: &[f64]) -> f64 {
        let mut conditional_entropy = 0.0;

        for (i, row) in joint_probabilities.iter().enumerate() {
            for (j, &joint_prob) in row.iter().enumerate() {
                if joint_prob > 0.0 && marginal_probabilities[i] > 0.0 {
                    let conditional_prob = joint_prob / marginal_probabilities[i];
                    conditional_entropy -= joint_prob * conditional_prob.log(self.base);
                }
            }
        }

        conditional_entropy
    }

    // äº’ä¿¡æ¯ï¼šI(X;Y) = H(X) - H(X|Y) / Mutual information: I(X;Y) = H(X) - H(X|Y)
    pub fn mutual_information(&self, joint_probabilities: &[Vec<f64>], x_marginal: &[f64], y_marginal: &[f64]) -> f64 {
        let h_x = self.entropy(x_marginal);
        let h_x_given_y = self.conditional_entropy(joint_probabilities, y_marginal);
        h_x - h_x_given_y
    }
}
```

### ç†µçš„æ€§è´¨

```rust
// ç†µçš„åŸºæœ¬æ€§è´¨
pub struct EntropyProperties;

impl EntropyProperties {
    // éè´Ÿæ€§ï¼šH(X) â‰¥ 0
    pub fn non_negativity(entropy: f64) -> bool {
        entropy >= 0.0
    }

    // å¯¹ç§°æ€§ï¼šH(X,Y) = H(Y,X)
    pub fn symmetry(joint_entropy_xy: f64, joint_entropy_yx: f64) -> bool {
        (joint_entropy_xy - joint_entropy_yx).abs() < f64::EPSILON
    }

    // é“¾å¼æ³•åˆ™ï¼šH(X,Y) = H(X) + H(Y|X)
    pub fn chain_rule(joint_entropy: f64, marginal_entropy: f64, conditional_entropy: f64) -> bool {
        (joint_entropy - (marginal_entropy + conditional_entropy)).abs() < f64::EPSILON
    }

    // æœ€å¤§ç†µåŸç†ï¼šåœ¨ç»™å®šçº¦æŸä¸‹ï¼Œç†µæœ€å¤§çš„åˆ†å¸ƒæœ€ä¸ç¡®å®š
    pub fn maximum_entropy_principle(&self, constraints: &[Constraint]) -> Distribution {
        // ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£æœ€å¤§ç†µåˆ†å¸ƒ
        self.solve_maximum_entropy(constraints)
    }
}
```

## ä¿¡é“ç†è®º

### ä¿¡é“æ¨¡å‹

```rust
// ç¦»æ•£æ— è®°å¿†ä¿¡é“
pub struct DiscreteMemorylessChannel {
    transition_matrix: Vec<Vec<f64>>, // P(Y|X)
    input_alphabet_size: usize,
    output_alphabet_size: usize,
}

impl DiscreteMemorylessChannel {
    pub fn new(transition_matrix: Vec<Vec<f64>>) -> Result<Self, ChannelError> {
        // éªŒè¯è½¬ç§»çŸ©é˜µçš„æœ‰æ•ˆæ€§
        for row in &transition_matrix {
            let sum: f64 = row.iter().sum();
            if (sum - 1.0).abs() > f64::EPSILON {
                return Err(ChannelError::InvalidTransitionMatrix);
            }
        }

        Ok(Self {
            input_alphabet_size: transition_matrix.len(),
            output_alphabet_size: transition_matrix[0].len(),
            transition_matrix,
        })
    }

    // è®¡ç®—è¾“å‡ºæ¦‚ç‡ï¼šP(Y) = Î£ P(X) * P(Y|X)
    pub fn output_probability(&self, input_distribution: &[f64], output: usize) -> f64 {
        input_distribution.iter()
            .enumerate()
            .map(|(input, &input_prob)| {
                input_prob * self.transition_matrix[input][output]
            })
            .sum()
    }

    // è®¡ç®—åéªŒæ¦‚ç‡ï¼šP(X|Y) = P(X,Y) / P(Y)
    pub fn posterior_probability(&self, input_distribution: &[f64], input: usize, output: usize) -> f64 {
        let joint_prob = input_distribution[input] * self.transition_matrix[input][output];
        let output_prob = self.output_probability(input_distribution, output);

        if output_prob > 0.0 {
            joint_prob / output_prob
        } else {
            0.0
        }
    }
}
```

### ä¿¡é“å®¹é‡

```rust
// ä¿¡é“å®¹é‡è®¡ç®—
pub struct ChannelCapacity {
    channel: DiscreteMemorylessChannel,
}

impl ChannelCapacity {
    pub fn new(channel: DiscreteMemorylessChannel) -> Self {
        Self { channel }
    }

    // ä¿¡é“å®¹é‡ï¼šC = max I(X;Y)
    pub fn capacity(&self) -> f64 {
        // ä½¿ç”¨è¿­ä»£ç®—æ³•æ±‚è§£æœ€å¤§äº’ä¿¡æ¯
        self.maximize_mutual_information()
    }

    // ä½¿ç”¨Blahut-Arimotoç®—æ³•æœ€å¤§åŒ–äº’ä¿¡æ¯
    fn maximize_mutual_information(&self) -> f64 {
        let mut input_distribution = vec![1.0 / self.channel.input_alphabet_size as f64; self.channel.input_alphabet_size];
        let mut max_mutual_info = 0.0;

        for _ in 0..100 { // æœ€å¤§è¿­ä»£æ¬¡æ•°
            // è®¡ç®—äº’ä¿¡æ¯
            let mutual_info = self.calculate_mutual_information(&input_distribution);
            max_mutual_info = max_mutual_info.max(mutual_info);

            // æ›´æ–°è¾“å…¥åˆ†å¸ƒ
            input_distribution = self.update_input_distribution(&input_distribution);
        }

        max_mutual_info
    }

    fn calculate_mutual_information(&self, input_distribution: &[f64]) -> f64 {
        let mut mutual_info = 0.0;

        for input in 0..self.channel.input_alphabet_size {
            for output in 0..self.channel.output_alphabet_size {
                let joint_prob = input_distribution[input] * self.channel.transition_matrix[input][output];
                if joint_prob > 0.0 {
                    let output_prob = self.channel.output_probability(input_distribution, output);
                    if output_prob > 0.0 {
                        mutual_info += joint_prob * (joint_prob / (input_distribution[input] * output_prob)).log(2.0);
                    }
                }
            }
        }

        mutual_info
    }

    fn update_input_distribution(&self, current_distribution: &[f64]) -> Vec<f64> {
        let mut new_distribution = vec![0.0; self.channel.input_alphabet_size];
        let mut normalization_factor = 0.0;

        for input in 0..self.channel.input_alphabet_size {
            let mut exp_term = 0.0;
            for output in 0..self.channel.output_alphabet_size {
                if self.channel.transition_matrix[input][output] > 0.0 {
                    let output_prob = self.channel.output_probability(current_distribution, output);
                    if output_prob > 0.0 {
                        exp_term += self.channel.transition_matrix[input][output] *
                                  (self.channel.transition_matrix[input][output] / output_prob).log(2.0);
                    }
                }
            }
            new_distribution[input] = current_distribution[input] * 2.0_f64.powf(exp_term);
            normalization_factor += new_distribution[input];
        }

        // å½’ä¸€åŒ–
        for prob in &mut new_distribution {
            *prob /= normalization_factor;
        }

        new_distribution
    }
}
```

## ç¼–ç ç†è®º

### ä¿¡æºç¼–ç 

```rust
// éœå¤«æ›¼ç¼–ç 
pub struct HuffmanCoding {
    symbol_probabilities: Vec<(char, f64)>,
}

impl HuffmanCoding {
    pub fn new(symbol_probabilities: Vec<(char, f64)>) -> Self {
        Self { symbol_probabilities }
    }

    pub fn encode(&self) -> HashMap<char, String> {
        let mut codes = HashMap::new();
        let mut nodes = self.build_initial_nodes();

        // æ„å»ºéœå¤«æ›¼æ ‘
        while nodes.len() > 1 {
            // é€‰æ‹©ä¸¤ä¸ªæœ€å°æ¦‚ç‡çš„èŠ‚ç‚¹
            nodes.sort_by(|a, b| a.probability.partial_cmp(&b.probability).unwrap());

            let left = nodes.remove(0);
            let right = nodes.remove(0);

            // åˆ›å»ºçˆ¶èŠ‚ç‚¹
            let parent = HuffmanNode {
                symbol: None,
                probability: left.probability + right.probability,
                left: Some(Box::new(left)),
                right: Some(Box::new(right)),
            };

            nodes.push(parent);
        }

        // ç”Ÿæˆç¼–ç 
        if let Some(root) = nodes.first() {
            self.generate_codes(root, String::new(), &mut codes);
        }

        codes
    }

    fn build_initial_nodes(&self) -> Vec<HuffmanNode> {
        self.symbol_probabilities.iter()
            .map(|(symbol, prob)| HuffmanNode {
                symbol: Some(*symbol),
                probability: *prob,
                left: None,
                right: None,
            })
            .collect()
    }

    fn generate_codes(&self, node: &HuffmanNode, current_code: String, codes: &mut HashMap<char, String>) {
        if let Some(symbol) = node.symbol {
            codes.insert(symbol, current_code);
        } else {
            if let Some(ref left) = node.left {
                self.generate_codes(left, current_code.clone() + "0", codes);
            }
            if let Some(ref right) = node.right {
                self.generate_codes(right, current_code + "1", codes);
            }
        }
    }
}

#[derive(Clone)]
struct HuffmanNode {
    symbol: Option<char>,
    probability: f64,
    left: Option<Box<HuffmanNode>>,
    right: Option<Box<HuffmanNode>>,
}
```

### ä¿¡é“ç¼–ç 

```rust
// æ±‰æ˜ç 
pub struct HammingCode {
    data_bits: usize,
    parity_bits: usize,
    total_bits: usize,
}

impl HammingCode {
    pub fn new(data_bits: usize) -> Self {
        let parity_bits = Self::calculate_parity_bits(data_bits);
        let total_bits = data_bits + parity_bits;

        Self {
            data_bits,
            parity_bits,
            total_bits,
        }
    }

    fn calculate_parity_bits(data_bits: usize) -> usize {
        let mut parity_bits = 1;
        while (1 << parity_bits) < data_bits + parity_bits + 1 {
            parity_bits += 1;
        }
        parity_bits
    }

    // ç¼–ç 
    pub fn encode(&self, data: &[bool]) -> Vec<bool> {
        assert_eq!(data.len(), self.data_bits);

        let mut encoded = vec![false; self.total_bits];
        let mut data_index = 0;

        // æ”¾ç½®æ•°æ®ä½
        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                encoded[i] = data[data_index];
                data_index += 1;
            }
        }

        // è®¡ç®—æ ¡éªŒä½
        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            encoded[parity_position] = self.calculate_parity_bit(&encoded, i);
        }

        encoded
    }

    // è§£ç å’Œçº é”™
    pub fn decode(&self, received: &[bool]) -> Result<Vec<bool>, DecodingError> {
        assert_eq!(received.len(), self.total_bits);

        // è®¡ç®—ç»¼åˆå¾
        let syndrome = self.calculate_syndrome(received);

        if syndrome == 0 {
            // æ— é”™è¯¯ï¼Œæå–æ•°æ®
            Ok(self.extract_data(received))
        } else {
            // æ£€æµ‹åˆ°é”™è¯¯ï¼Œå°è¯•çº é”™
            let error_position = syndrome - 1;
            if error_position < self.total_bits {
                let mut corrected = received.to_vec();
                corrected[error_position] = !corrected[error_position];
                Ok(self.extract_data(&corrected))
            } else {
                Err(DecodingError::UncorrectableError)
            }
        }
    }

    fn is_power_of_two(&self, n: usize) -> bool {
        n > 0 && (n & (n - 1)) == 0
    }

    fn calculate_parity_bit(&self, data: &[bool], parity_index: usize) -> bool {
        let mut parity = false;
        let parity_mask = 1 << parity_index;

        for (i, &bit) in data.iter().enumerate() {
            if (i + 1) & parity_mask != 0 {
                parity ^= bit;
            }
        }

        parity
    }

    fn calculate_syndrome(&self, received: &[bool]) -> usize {
        let mut syndrome = 0;

        for i in 0..self.parity_bits {
            let parity_position = (1 << i) - 1;
            if self.calculate_parity_bit(received, i) != received[parity_position] {
                syndrome += 1 << i;
            }
        }

        syndrome
    }

    fn extract_data(&self, encoded: &[bool]) -> Vec<bool> {
        let mut data = Vec::new();

        for i in 0..self.total_bits {
            if !self.is_power_of_two(i + 1) {
                data.push(encoded[i]);
            }
        }

        data
    }
}

#[derive(Debug)]
pub enum DecodingError {
    UncorrectableError,
}
```

## ç‡å¤±çœŸç†è®º

### ç‡å¤±çœŸå‡½æ•°

```rust
// ç‡å¤±çœŸç†è®º
pub struct RateDistortionTheory {
    distortion_measure: Box<dyn DistortionMeasure>,
}

impl RateDistortionTheory {
    pub fn new(distortion_measure: Box<dyn DistortionMeasure>) -> Self {
        Self { distortion_measure }
    }

    // ç‡å¤±çœŸå‡½æ•°ï¼šR(D) = min I(X;XÌ‚) subject to E[d(X,XÌ‚)] â‰¤ D
    pub fn rate_distortion_function(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ±‚è§£
        self.minimize_rate_under_distortion_constraint(source_distribution, target_distortion)
    }

    fn minimize_rate_under_distortion_constraint(&self, source_distribution: &[f64], target_distortion: f64) -> f64 {
        // ç®€åŒ–çš„è¿­ä»£ç®—æ³•
        let mut lambda = 1.0; // æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
        let mut rate = 0.0;

        for _ in 0..100 {
            // æ›´æ–°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
            let conditional_distribution = self.update_conditional_distribution(source_distribution, lambda);

            // è®¡ç®—å½“å‰å¤±çœŸå’Œç‡
            let current_distortion = self.calculate_distortion(source_distribution, &conditional_distribution);
            rate = self.calculate_rate(source_distribution, &conditional_distribution);

            // æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°
            lambda *= (current_distortion / target_distortion).powf(0.1);
        }

        rate
    }

    fn update_conditional_distribution(&self, source_distribution: &[f64], lambda: f64) -> Vec<Vec<f64>> {
        // åŸºäºæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ›´æ–°æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
        // è¿™é‡Œç®€åŒ–å®ç°
        vec![vec![1.0 / source_distribution.len() as f64; source_distribution.len()]; source_distribution.len()]
    }

    fn calculate_distortion(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        let mut total_distortion = 0.0;

        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                let distortion = self.distortion_measure.measure(i, j);
                total_distortion += source_prob * conditional_prob * distortion;
            }
        }

        total_distortion
    }

    fn calculate_rate(&self, source_distribution: &[f64], conditional_distribution: &[Vec<f64>]) -> f64 {
        // è®¡ç®—äº’ä¿¡æ¯ä½œä¸ºç‡
        let mut rate = 0.0;

        for (i, &source_prob) in source_distribution.iter().enumerate() {
            for (j, &conditional_prob) in conditional_distribution[i].iter().enumerate() {
                if source_prob > 0.0 && conditional_prob > 0.0 {
                    let joint_prob = source_prob * conditional_prob;
                    rate += joint_prob * (conditional_prob / source_prob).log(2.0);
                }
            }
        }

        rate
    }
}

pub trait DistortionMeasure {
    fn measure(&self, x: usize, y: usize) -> f64;
}

// æ±‰æ˜å¤±çœŸåº¦é‡
pub struct HammingDistortion;

impl DistortionMeasure for HammingDistortion {
    fn measure(&self, x: usize, y: usize) -> f64 {
        if x == y { 0.0 } else { 1.0 }
    }
}
```

## åº”ç”¨ç¤ºä¾‹

### æ•°æ®å‹ç¼©

```rust
// æ•°æ®å‹ç¼©ç³»ç»Ÿ
pub struct DataCompression {
    huffman_coding: HuffmanCoding,
    arithmetic_coding: ArithmeticCoding,
}

impl DataCompression {
    pub fn compress_huffman(&self, data: &str) -> (Vec<u8>, HashMap<char, String>) {
        // è®¡ç®—ç¬¦å·é¢‘ç‡
        let mut frequencies = HashMap::new();
        for ch in data.chars() {
            *frequencies.entry(ch).or_insert(0) += 1;
        }

        // è½¬æ¢ä¸ºæ¦‚ç‡
        let total = data.len() as f64;
        let symbol_probabilities: Vec<(char, f64)> = frequencies
            .iter()
            .map(|(&ch, &count)| (ch, count as f64 / total))
            .collect();

        // ç”Ÿæˆéœå¤«æ›¼ç¼–ç 
        let huffman = HuffmanCoding::new(symbol_probabilities);
        let codes = huffman.encode();

        // ç¼–ç æ•°æ®
        let encoded_data = self.encode_with_codes(data, &codes);

        (encoded_data, codes)
    }

    fn encode_with_codes(&self, data: &str, codes: &HashMap<char, String>) -> Vec<u8> {
        let mut encoded_bits = String::new();

        for ch in data.chars() {
            if let Some(code) = codes.get(&ch) {
                encoded_bits.push_str(code);
            }
        }

        // è½¬æ¢ä¸ºå­—èŠ‚
        self.bits_to_bytes(&encoded_bits)
    }

    fn bits_to_bytes(&self, bits: &str) -> Vec<u8> {
        let mut bytes = Vec::new();
        let mut current_byte = 0u8;
        let mut bit_count = 0;

        for bit in bits.chars() {
            current_byte = (current_byte << 1) | if bit == '1' { 1 } else { 0 };
            bit_count += 1;

            if bit_count == 8 {
                bytes.push(current_byte);
                current_byte = 0;
                bit_count = 0;
            }
        }

        // å¤„ç†å‰©ä½™çš„ä½
        if bit_count > 0 {
            current_byte <<= 8 - bit_count;
            bytes.push(current_byte);
        }

        bytes
    }
}
```

### ä¿¡é“ç¼–ç åº”ç”¨

```rust
// é€šä¿¡ç³»ç»Ÿ
pub struct CommunicationSystem {
    hamming_code: HammingCode,
    channel: DiscreteMemorylessChannel,
}

impl CommunicationSystem {
    pub fn new() -> Self {
        Self {
            hamming_code: HammingCode::new(4), // 4ä½æ•°æ®ï¼Œ3ä½æ ¡éªŒ
            channel: DiscreteMemorylessChannel::new(vec![
                vec![0.9, 0.1], // è¾“å…¥0çš„è¾“å‡ºæ¦‚ç‡
                vec![0.1, 0.9], // è¾“å…¥1çš„è¾“å‡ºæ¦‚ç‡
            ]).unwrap(),
        }
    }

    pub fn transmit(&self, data: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        // 1. ä¿¡é“ç¼–ç 
        let encoded = self.hamming_code.encode(data);

        // 2. é€šè¿‡ä¿¡é“ä¼ è¾“
        let received = self.transmit_through_channel(&encoded)?;

        // 3. ä¿¡é“è§£ç 
        let decoded = self.hamming_code.decode(&received)?;

        Ok(decoded)
    }

    fn transmit_through_channel(&self, encoded: &[bool]) -> Result<Vec<bool>, CommunicationError> {
        let mut received = Vec::new();

        for &bit in encoded {
            let input = if bit { 1 } else { 0 };
            let output = self.simulate_channel_output(input);
            received.push(output == 1);
        }

        Ok(received)
    }

    fn simulate_channel_output(&self, input: usize) -> usize {
        let random = rand::random::<f64>();
        let threshold = self.channel.transition_matrix[input][0];

        if random < threshold { 0 } else { 1 }
    }
}

#[derive(Debug)]
pub enum CommunicationError {
    DecodingError(DecodingError),
    ChannelError(ChannelError),
}

#[derive(Debug)]
pub enum ChannelError {
    InvalidTransitionMatrix,
}
```

## æ€»ç»“

ä¿¡æ¯è®ºåŸºç¡€ä¸ºç°ä»£é€šä¿¡å’Œæ•°æ®å¤„ç†æä¾›äº†åšå®çš„æ•°å­¦åŸºç¡€ï¼š

1. **ç†µç†è®º**: é‡åŒ–ä¿¡æ¯çš„ä¸ç¡®å®šæ€§å’Œä¿¡æ¯é‡
2. **ä¿¡é“ç†è®º**: åˆ†æä¿¡æ¯ä¼ è¾“çš„æé™å’Œæœ€ä¼˜ç­–ç•¥
3. **ç¼–ç ç†è®º**: è®¾è®¡é«˜æ•ˆå¯é çš„ä¿¡æ¯ç¼–ç æ–¹æ³•
4. **ç‡å¤±çœŸç†è®º**: åœ¨å¤±çœŸçº¦æŸä¸‹ä¼˜åŒ–ä¿¡æ¯å‹ç¼©

è¿™äº›ç†è®ºåœ¨æ•°æ®å‹ç¼©ã€é”™è¯¯çº æ­£ã€é€šä¿¡ç³»ç»Ÿã€æœºå™¨å­¦ä¹ ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚

---

## å‚è€ƒæ–‡çŒ® / References

æœ¬æ–‡æ¡£åŸºäºå·²å‘è¡¨çš„å­¦æœ¯æ–‡çŒ®å’Œå…¬å¼€èµ„æ–™ç¼–å†™ã€‚ä»¥ä¸‹æ˜¯ä¸»è¦å‚è€ƒæ–‡çŒ®ï¼š

**ç»å…¸å¥ åŸºæ–‡çŒ® / Classic Foundational Literature**:

1. [Shannon1948] Shannon, C. E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*, 27(3): 379-423. DOI: 10.1002/j.1538-7305.1948.tb01338.x
   - ä¿¡æ¯è®ºçš„å¥ åŸºæ€§è®ºæ–‡ï¼Œå®šä¹‰äº†ç†µã€ä¿¡é“å®¹é‡ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œå¼€åˆ›äº†ä¿¡æ¯è®ºé¢†åŸŸã€‚

**æ ‡å‡†æ•™æ / Standard Textbooks**:

1. [CoverThomas2006] Cover, T. M., & Thomas, J. A. (2006). *Elements of Information Theory* (2nd Edition). Wiley-Interscience. ISBN: 978-0471241959.
   - ä¿¡æ¯è®ºçš„ç»å…¸æ•™æï¼Œè¯¦ç»†ä»‹ç»ç†µã€äº’ä¿¡æ¯ã€ä¿¡é“å®¹é‡å’Œç‡å¤±çœŸç†è®ºï¼Œæœ¬æ–‡æ¡£çš„ä¸»è¦å‚è€ƒã€‚

2. [MacKay2003] MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. ISBN: 978-0521642981.
   - ä¿¡æ¯è®ºä¸æœºå™¨å­¦ä¹ çš„ç»¼åˆæ•™æï¼Œè¿æ¥ä¿¡æ¯è®ºä¸å®é™…åº”ç”¨ã€‚

**ç°ä»£ç»¼åˆä¸é«˜çº§ä¸»é¢˜ / Modern Synthesis and Advanced Topics**:

1. [Yeung2008] Yeung, R. W. (2008). *Information Theory and Network Coding*. Springer. ISBN: 978-0387792330.
   - ä¿¡æ¯è®ºä¸ç½‘ç»œç¼–ç çš„ç°ä»£ç»¼åˆï¼Œæ‰©å±•äº†ä¼ ç»Ÿä¿¡æ¯è®ºåˆ°ç½‘ç»œç¯å¢ƒã€‚

2. [Csiszar2011] CsiszÃ¡r, I., & KÃ¶rner, J. (2011). *Information Theory: Coding Theorems for Discrete Memoryless Systems* (2nd Edition). Cambridge University Press. ISBN: 978-0521196819.
   - ç¼–ç å®šç†çš„ä¸¥æ ¼æ•°å­¦å¤„ç†ï¼Œé«˜çº§ä¿¡æ¯è®ºçš„æƒå¨å‚è€ƒã€‚

**ç®—æ³•ä¸å¤æ‚åº¦è¿æ¥ / Connection to Algorithms and Complexity**:

1. [Sipser2012] Sipser, M. (2012). *Introduction to the Theory of Computation* (3rd Edition). Cengage Learning. ISBN: 978-1133187790.
   - è®¡ç®—ç†è®ºå¯¼è®ºï¼ŒåŒ…å«Kolmogorovå¤æ‚åº¦å’Œä¿¡æ¯è®ºåœ¨ç®—æ³•ä¸­çš„åº”ç”¨ã€‚

2. [AroraBarak2009] Arora, S., & Barak, B. (2009). *Computational Complexity: A Modern Approach*. Cambridge University Press. ISBN: 978-0521424264.
   - è®¡ç®—å¤æ‚åº¦ç†è®ºï¼Œè®¨è®ºä¿¡æ¯è®ºåœ¨å¤æ‚åº¦åˆ†æä¸­çš„åº”ç”¨ã€‚

**åœ¨çº¿èµ„æº / Online Resources**:

1. **Wikipedia - Information Theory**: <https://en.wikipedia.org/wiki/Information_theory>
   - ä¿¡æ¯è®ºçš„Wikipediaæ¡ç›®ï¼ŒåŒ…å«Shannonç†µã€äº’ä¿¡æ¯ç­‰æ ¸å¿ƒæ¦‚å¿µï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

2. **Wikipedia - Entropy (Information Theory)**: <https://en.wikipedia.org/wiki/Entropy_(information_theory)>
   - ä¿¡æ¯ç†µçš„Wikipediaæ¡ç›®ï¼Œè¯¦ç»†ä»‹ç»Shannonç†µçš„å®šä¹‰å’Œæ€§è´¨ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

3. **Wikipedia - Channel Capacity**: <https://en.wikipedia.org/wiki/Channel_capacity>
   - ä¿¡é“å®¹é‡çš„Wikipediaæ¡ç›®ï¼ŒåŒ…å«Shannonå®šç†å’Œç¼–ç ç†è®ºï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰ã€‚

4. MIT OpenCourseWare - "6.441 Information Theory": <https://ocw.mit.edu/courses/6-441-information-theory-spring-2016/>
   - MITä¿¡æ¯è®ºè¯¾ç¨‹çš„å…è´¹åœ¨çº¿èµ„æºã€‚

5. Stanford Encyclopedia of Philosophy - Information: <https://plato.stanford.edu/>
   - ä¿¡æ¯è®ºçš„å“²å­¦è®¨è®ºå’Œæ¦‚å¿µè§£æã€‚

**å¼•ç”¨è§„èŒƒè¯´æ˜ / Citation Guidelines**:

æœ¬æ–‡æ¡£éµå¾ªé¡¹ç›®å¼•ç”¨è§„èŒƒï¼ˆè§ `docs/å¼•ç”¨è§„èŒƒä¸æ•°æ®åº“.md`ï¼‰ã€‚æ‰€æœ‰å¼•ç”¨æ¡ç›®åœ¨ `docs/references_database.yaml` ä¸­æœ‰å®Œæ•´è®°å½•ã€‚

æœ¬æ–‡æ¡£å†…å®¹å·²å¯¹ç…§Wikipediaç›¸å…³æ¡ç›®ï¼ˆæˆªè‡³2025å¹´1æœˆ11æ—¥ï¼‰è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿æœ¯è¯­å®šä¹‰å’Œç†è®ºæ¡†æ¶ä¸å½“å‰å­¦æœ¯æ ‡å‡†ä¸€è‡´ã€‚

---

## ä¸é¡¹ç›®ç»“æ„ä¸»é¢˜çš„å¯¹é½ / Alignment with Project Structure

### ç›¸å…³æ–‡æ¡£ / Related Documents

- `01-åŸºç¡€ç†è®º/07-æ¦‚ç‡ä¸ç»Ÿè®¡åŸºç¡€.md` - æ¦‚ç‡ä¸ç»Ÿè®¡åŸºç¡€ï¼ˆä¿¡æ¯è®ºçš„æ•°å­¦åŸºç¡€ï¼‰
- `04-ç®—æ³•å¤æ‚åº¦/06-ä¿¡æ¯è®ºä¸‹ç•Œ.md` - ä¿¡æ¯è®ºä¸‹ç•Œï¼ˆä¿¡æ¯è®ºåœ¨ç®—æ³•å¤æ‚åº¦ä¸­çš„åº”ç”¨ï¼‰
- `04-ç®—æ³•å¤æ‚åº¦/05-é€šä¿¡å¤æ‚åº¦.md` - é€šä¿¡å¤æ‚åº¦ï¼ˆä¿¡æ¯è®ºåœ¨é€šä¿¡å¤æ‚åº¦ä¸­çš„åº”ç”¨ï¼‰
- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°æœ¬æ–‡æ¡£ï¼ˆåŸ `view/ç®—æ³•å…¨æ™¯æ¢³ç†-2025-01-11.md` Â§1.2, Â§3.8ï¼‰

### çŸ¥è¯†ä½“ç³»ä½ç½® / Knowledge System Position

æœ¬æ–‡æ¡£å±äº **01-åŸºç¡€ç†è®º** æ¨¡å—ï¼Œæ˜¯ä¿¡æ¯è®ºçš„åŸºç¡€æ–‡æ¡£ï¼Œä¸ºç®—æ³•å¤æ‚åº¦åˆ†æå’Œé€šä¿¡å¤æ‚åº¦åˆ†ææä¾›ç†è®ºåŸºç¡€ã€‚

### VIEWæ–‡ä»¶å¤¹ç›¸å…³æ–‡æ¡£ / VIEW Folder Related Documents

- ç›¸å…³å†…å®¹å·²æ•´åˆåˆ°æœ¬æ–‡æ¡£å’Œ `04-ç®—æ³•å¤æ‚åº¦/06-ä¿¡æ¯è®ºä¸‹ç•Œ.md`ï¼ˆåŸ `view/ç®—æ³•å…¨æ™¯æ¢³ç†-2025-01-11.md` Â§1.2, Â§3.8ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬ / Document Version**: 1.1
**æœ€åæ›´æ–° / Last Updated**: 2025-01-11
**çŠ¶æ€ / Status**: å·²å¯¹ç…§Wikipediaæ›´æ–° / Updated with Wikipedia references (as of 2025-01-11)

---

*æœ¬æ–‡æ¡£å±•ç¤ºäº†ä¿¡æ¯è®ºçš„åŸºç¡€ç†è®ºï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°å­¦å®šä¹‰å’Œç®—æ³•å®ç°ï¼Œä¸ºä¿¡æ¯å¤„ç†æä¾›äº†ç†è®ºåŸºç¡€ã€‚*
