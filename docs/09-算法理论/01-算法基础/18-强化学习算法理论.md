# 强化学习算法理论 (Reinforcement Learning Algorithm Theory)

## 基本概念 (Basic Concepts)

### 定义 (Definition)

强化学习是一种通过与环境交互来学习最优策略的机器学习方法，智能体通过试错来最大化累积奖励，实现从经验中学习的目标。

**Reinforcement learning is a machine learning method that learns optimal policies through interaction with the environment, where agents learn from experience through trial and error to maximize cumulative rewards.**

### 核心思想 (Core Ideas)

1. **智能体与环境交互** (Agent-Environment Interaction)
   - 智能体在环境中执行动作并获得反馈
   - Agent executes actions in environment and receives feedback

2. **奖励机制** (Reward Mechanism)
   - 通过奖励信号指导学习过程
   - Guide learning process through reward signals

3. **策略优化** (Policy Optimization)
   - 学习最优的行为策略
   - Learn optimal behavioral policies

4. **价值函数** (Value Function)
   - 评估状态或动作的长期价值
   - Evaluate long-term value of states or actions

## 马尔可夫决策过程 (Markov Decision Process)

### 数学基础 (Mathematical Foundation)

设 $S$ 为状态空间，$A$ 为动作空间，$P$ 为转移概率，$R$ 为奖励函数，则：

**Let $S$ be the state space, $A$ be the action space, $P$ be the transition probability, and $R$ be the reward function, then:**

**马尔可夫决策过程** (Markov Decision Process):
$$M = (S, A, P, R, \gamma)$$

**状态转移概率** (State Transition Probability):
$$P(s'|s, a) = P(S_{t+1} = s'|S_t = s, A_t = a)$$

**奖励函数** (Reward Function):
$$R(s, a, s') = \mathbb{E}[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s']$$

**折扣因子** (Discount Factor):
$$\gamma \in [0, 1]$$

### 价值函数 (Value Functions)

**状态价值函数** (State Value Function):
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s\right]$$

**动作价值函数** (Action Value Function):
$$Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s, A_t = a\right]$$

**贝尔曼方程** (Bellman Equation):
$$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V^\pi(s'))$$

## 经典问题 (Classic Problems)

### 1. 多臂老虎机问题 (Multi-Armed Bandit Problem)

**问题描述** (Problem Description):
在多个选项中做出最优选择，平衡探索与利用。

**Make optimal choices among multiple options, balancing exploration and exploitation.**

**强化学习算法** (Reinforcement Learning Algorithm):
ε-贪心算法、UCB算法。

**ε-greedy algorithm, UCB algorithm.**

**时间复杂度** (Time Complexity): $O(n)$
**遗憾界** (Regret Bound): $O(\sqrt{n \log n})$

### 2. 网格世界问题 (Grid World Problem)

**问题描述** (Problem Description):
在网格环境中找到从起点到终点的最优路径。

**Find optimal path from start to goal in grid environment.**

**强化学习算法** (Reinforcement Learning Algorithm):
Q学习算法、SARSA算法。

**Q-learning algorithm, SARSA algorithm.**

**时间复杂度** (Time Complexity): $O(|S| \cdot |A| \cdot \text{episodes})$
**收敛性** (Convergence): 保证收敛到最优策略

### 3. 连续控制问题 (Continuous Control Problem)

**问题描述** (Problem Description):
在连续动作空间中学习最优控制策略。

**Learn optimal control policies in continuous action spaces.**

**强化学习算法** (Reinforcement Learning Algorithm):
策略梯度算法、Actor-Critic算法。

**Policy gradient algorithm, Actor-Critic algorithm.**

**时间复杂度** (Time Complexity): $O(d^2 \cdot \text{episodes})$
**样本效率** (Sample Efficiency): 中等

## 学习算法分析 (Learning Algorithm Analysis)

### 1. Q学习算法 (Q-Learning Algorithm)

**Q值更新** (Q-Value Update):
$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

**收敛条件** (Convergence Condition):
$$\sum_{t=0}^{\infty} \alpha_t = \infty, \quad \sum_{t=0}^{\infty} \alpha_t^2 < \infty$$

**学习率** (Learning Rate):
$$\alpha_t = \frac{1}{1 + \text{visits}(s, a)}$$

### 2. 策略梯度算法 (Policy Gradient Algorithm)

**策略梯度定理** (Policy Gradient Theorem):
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a)]$$

**REINFORCE算法** (REINFORCE Algorithm):
$$\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) G_t$$

**基线方法** (Baseline Method):
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)(Q^\pi(s, a) - b(s))]$$

### 3. Actor-Critic算法 (Actor-Critic Algorithm)

**Actor更新** (Actor Update):
$$\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a|s) \delta$$

**Critic更新** (Critic Update):
$$\phi \leftarrow \phi + \alpha_\phi \delta \nabla_\phi V_\phi(s)$$

**TD误差** (TD Error):
$$\delta = r + \gamma V_\phi(s') - V_\phi(s)$$

## 实现示例 (Implementation Examples)

### Rust实现 (Rust Implementation)

```rust
use rand::Rng;
use std::collections::HashMap;

/// 强化学习算法实现
/// Reinforcement learning algorithm implementation
pub struct ReinforcementLearningAlgorithms;

impl ReinforcementLearningAlgorithms {
    /// 多臂老虎机
    /// Multi-armed bandit
    pub struct MultiArmedBandit {
        arms: Vec<f64>,
        counts: Vec<usize>,
        values: Vec<f64>,
        epsilon: f64,
    }

    impl MultiArmedBandit {
        pub fn new(num_arms: usize, epsilon: f64) -> Self {
            Self {
                arms: vec![0.0; num_arms],
                counts: vec![0; num_arms],
                values: vec![0.0; num_arms],
                epsilon,
            }
        }

        pub fn select_action(&mut self) -> usize {
            let mut rng = rand::thread_rng();
            
            if rng.gen_bool(self.epsilon) {
                // 探索：随机选择
                rng.gen_range(0..self.arms.len())
            } else {
                // 利用：选择最优动作
                self.values.iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap()
            }
        }

        pub fn update(&mut self, action: usize, reward: f64) {
            self.counts[action] += 1;
            let count = self.counts[action] as f64;
            
            // 增量更新Q值
            self.values[action] += (reward - self.values[action]) / count;
        }

        pub fn get_regret(&self, optimal_reward: f64) -> f64 {
            let total_reward: f64 = self.values.iter().sum();
            let total_optimal = optimal_reward * self.counts.iter().sum::<usize>() as f64;
            total_optimal - total_reward
        }
    }

    /// Q学习算法
    /// Q-learning algorithm
    pub struct QLearning {
        q_table: HashMap<(usize, usize), f64>,
        learning_rate: f64,
        discount_factor: f64,
        epsilon: f64,
    }

    impl QLearning {
        pub fn new(learning_rate: f64, discount_factor: f64, epsilon: f64) -> Self {
            Self {
                q_table: HashMap::new(),
                learning_rate,
                discount_factor,
                epsilon,
            }
        }

        pub fn select_action(&self, state: usize, available_actions: &[usize]) -> usize {
            let mut rng = rand::thread_rng();
            
            if rng.gen_bool(self.epsilon) {
                // 探索：随机选择
                available_actions[rng.gen_range(0..available_actions.len())]
            } else {
                // 利用：选择Q值最大的动作
                available_actions.iter()
                    .max_by(|&&a, &&b| {
                        let q_a = self.q_table.get(&(state, a)).unwrap_or(&0.0);
                        let q_b = self.q_table.get(&(state, b)).unwrap_or(&0.0);
                        q_a.partial_cmp(q_b).unwrap()
                    })
                    .unwrap()
                    .clone()
            }
        }

        pub fn update(&mut self, state: usize, action: usize, reward: f64, next_state: usize, next_actions: &[usize]) {
            let current_q = self.q_table.get(&(state, action)).unwrap_or(&0.0);
            
            // 计算下一状态的最大Q值
            let max_next_q = next_actions.iter()
                .map(|&a| self.q_table.get(&(next_state, a)).unwrap_or(&0.0))
                .fold(0.0, |max, &q| max.max(q));
            
            // Q学习更新公式
            let new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q);
            self.q_table.insert((state, action), new_q);
        }

        pub fn get_policy(&self, state: usize, available_actions: &[usize]) -> usize {
            available_actions.iter()
                .max_by(|&&a, &&b| {
                    let q_a = self.q_table.get(&(state, a)).unwrap_or(&0.0);
                    let q_b = self.q_table.get(&(state, b)).unwrap_or(&0.0);
                    q_a.partial_cmp(q_b).unwrap()
                })
                .unwrap()
                .clone()
        }
    }

    /// 策略梯度算法
    /// Policy gradient algorithm
    pub struct PolicyGradient {
        policy_params: Vec<f64>,
        learning_rate: f64,
        discount_factor: f64,
    }

    impl PolicyGradient {
        pub fn new(num_params: usize, learning_rate: f64, discount_factor: f64) -> Self {
            let mut rng = rand::thread_rng();
            let policy_params: Vec<f64> = (0..num_params)
                .map(|_| rng.gen_range(-0.1..0.1))
                .collect();
            
            Self {
                policy_params,
                learning_rate,
                discount_factor,
            }
        }

        pub fn select_action(&self, state_features: &[f64], available_actions: &[usize]) -> usize {
            let action_probs = self.compute_action_probs(state_features, available_actions);
            
            // 根据概率分布选择动作
            let mut rng = rand::thread_rng();
            let random_value = rng.gen_range(0.0..1.0);
            let mut cumulative_prob = 0.0;
            
            for (i, &prob) in action_probs.iter().enumerate() {
                cumulative_prob += prob;
                if random_value <= cumulative_prob {
                    return available_actions[i];
                }
            }
            
            available_actions[0]
        }

        pub fn update(&mut self, episode: &[(Vec<f64>, usize, f64)]) {
            let episode_return = self.compute_returns(episode);
            
            for (i, (state_features, action, _)) in episode.iter().enumerate() {
                let action_probs = self.compute_action_probs(state_features, &[action.clone()]);
                let log_prob = action_probs[0].ln();
                
                // 策略梯度更新
                let gradient = log_prob * episode_return[i];
                
                for (j, param) in self.policy_params.iter_mut().enumerate() {
                    if j < state_features.len() {
                        *param += self.learning_rate * gradient * state_features[j];
                    }
                }
            }
        }

        fn compute_action_probs(&self, state_features: &[f64], available_actions: &[usize]) -> Vec<f64> {
            let mut logits = Vec::new();
            
            for &action in available_actions {
                let mut logit = 0.0;
                for (i, feature) in state_features.iter().enumerate() {
                    if i < self.policy_params.len() {
                        logit += self.policy_params[i] * feature;
                    }
                }
                logits.push(logit);
            }
            
            // Softmax归一化
            let max_logit = logits.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
            let exp_logits: Vec<f64> = logits.iter().map(|&x| (x - max_logit).exp()).collect();
            let sum_exp = exp_logits.iter().sum::<f64>();
            
            exp_logits.iter().map(|&x| x / sum_exp).collect()
        }

        fn compute_returns(&self, episode: &[(Vec<f64>, usize, f64)]) -> Vec<f64> {
            let mut returns = Vec::new();
            let mut future_return = 0.0;
            
            for (_, _, reward) in episode.iter().rev() {
                future_return = reward + self.discount_factor * future_return;
                returns.insert(0, future_return);
            }
            
            returns
        }
    }

    /// Actor-Critic算法
    /// Actor-Critic algorithm
    pub struct ActorCritic {
        actor_params: Vec<f64>,
        critic_params: Vec<f64>,
        actor_lr: f64,
        critic_lr: f64,
        discount_factor: f64,
    }

    impl ActorCritic {
        pub fn new(num_params: usize, actor_lr: f64, critic_lr: f64, discount_factor: f64) -> Self {
            let mut rng = rand::thread_rng();
            let actor_params: Vec<f64> = (0..num_params)
                .map(|_| rng.gen_range(-0.1..0.1))
                .collect();
            let critic_params: Vec<f64> = (0..num_params)
                .map(|_| rng.gen_range(-0.1..0.1))
                .collect();
            
            Self {
                actor_params,
                critic_params,
                actor_lr,
                critic_lr,
                discount_factor,
            }
        }

        pub fn select_action(&self, state_features: &[f64], available_actions: &[usize]) -> usize {
            let action_probs = self.compute_action_probs(state_features, available_actions);
            
            let mut rng = rand::thread_rng();
            let random_value = rng.gen_range(0.0..1.0);
            let mut cumulative_prob = 0.0;
            
            for (i, &prob) in action_probs.iter().enumerate() {
                cumulative_prob += prob;
                if random_value <= cumulative_prob {
                    return available_actions[i];
                }
            }
            
            available_actions[0]
        }

        pub fn update(&mut self, state_features: &[f64], action: usize, reward: f64, next_state_features: &[f64]) {
            // 计算当前状态的价值
            let current_value = self.compute_value(state_features);
            
            // 计算下一状态的价值
            let next_value = self.compute_value(next_state_features);
            
            // 计算TD误差
            let td_error = reward + self.discount_factor * next_value - current_value;
            
            // 更新Critic（价值函数）
            for (i, param) in self.critic_params.iter_mut().enumerate() {
                if i < state_features.len() {
                    *param += self.critic_lr * td_error * state_features[i];
                }
            }
            
            // 更新Actor（策略）
            let action_probs = self.compute_action_probs(state_features, &[action]);
            let log_prob = action_probs[0].ln();
            
            for (i, param) in self.actor_params.iter_mut().enumerate() {
                if i < state_features.len() {
                    *param += self.actor_lr * td_error * log_prob * state_features[i];
                }
            }
        }

        fn compute_value(&self, state_features: &[f64]) -> f64 {
            let mut value = 0.0;
            for (i, feature) in state_features.iter().enumerate() {
                if i < self.critic_params.len() {
                    value += self.critic_params[i] * feature;
                }
            }
            value
        }

        fn compute_action_probs(&self, state_features: &[f64], available_actions: &[usize]) -> Vec<f64> {
            let mut logits = Vec::new();
            
            for &action in available_actions {
                let mut logit = 0.0;
                for (i, feature) in state_features.iter().enumerate() {
                    if i < self.actor_params.len() {
                        logit += self.actor_params[i] * feature;
                    }
                }
                logits.push(logit);
            }
            
            // Softmax归一化
            let max_logit = logits.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
            let exp_logits: Vec<f64> = logits.iter().map(||x| (x - max_logit).exp()).collect();
            let sum_exp = exp_logits.iter().sum::<f64>();
            
            exp_logits.iter().map(|&x| x / sum_exp).collect()
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_multi_armed_bandit() {
        let mut bandit = ReinforcementLearningAlgorithms::MultiArmedBandit::new(3, 0.1);
        
        for _ in 0..100 {
            let action = bandit.select_action();
            let reward = if action == 0 { 0.8 } else { 0.2 };
            bandit.update(action, reward);
        }
        
        assert!(bandit.get_regret(0.8) > 0.0);
    }

    #[test]
    fn test_q_learning() {
        let mut q_learning = ReinforcementLearningAlgorithms::QLearning::new(0.1, 0.9, 0.1);
        
        let available_actions = vec![0, 1, 2];
        let action = q_learning.select_action(0, &available_actions);
        q_learning.update(0, action, 1.0, 1, &available_actions);
        
        assert!(action < 3);
    }

    #[test]
    fn test_policy_gradient() {
        let mut pg = ReinforcementLearningAlgorithms::PolicyGradient::new(4, 0.01, 0.9);
        
        let state_features = vec![1.0, 0.0, 0.0, 0.0];
        let available_actions = vec![0, 1];
        let action = pg.select_action(&state_features, &available_actions);
        
        let episode = vec![(state_features, action, 1.0)];
        pg.update(&episode);
        
        assert!(action < 2);
    }

    #[test]
    fn test_actor_critic() {
        let mut ac = ReinforcementLearningAlgorithms::ActorCritic::new(4, 0.01, 0.01, 0.9);
        
        let state_features = vec![1.0, 0.0, 0.0, 0.0];
        let next_state_features = vec![0.0, 1.0, 0.0, 0.0];
        let available_actions = vec![0, 1];
        let action = ac.select_action(&state_features, &available_actions);
        
        ac.update(&state_features, action, 1.0, &next_state_features);
        
        assert!(action < 2);
    }
}
```

### Haskell实现 (Haskell Implementation)

```haskell
-- 强化学习算法模块
-- Reinforcement learning algorithm module
module ReinforcementLearningAlgorithms where

import System.Random
import Data.List (maximumBy)
import Data.Ord (comparing)
import qualified Data.Map as Map

-- 多臂老虎机
-- Multi-armed bandit
data MultiArmedBandit = MultiArmedBandit {
    arms :: [Double],
    counts :: [Int],
    values :: [Double],
    epsilon :: Double
}

newMultiArmedBandit :: Int -> Double -> MultiArmedBandit
newMultiArmedBandit numArms epsilon = MultiArmedBandit {
    arms = replicate numArms 0.0,
    counts = replicate numArms 0,
    values = replicate numArms 0.0,
    epsilon = epsilon
}

selectAction :: MultiArmedBandit -> IO Int
selectAction bandit = do
    randomValue <- randomRIO (0.0, 1.0)
    if randomValue < epsilon bandit
    then do
        -- 探索：随机选择
        randomRIO (0, length (arms bandit) - 1)
    else do
        -- 利用：选择最优动作
        return $ fst $ maximumBy (comparing snd) (zip [0..] (values bandit))

update :: MultiArmedBandit -> Int -> Double -> MultiArmedBandit
update bandit action reward = 
    let newCounts = updateList (counts bandit) action ((counts bandit !! action) + 1)
        count = fromIntegral (newCounts !! action)
        oldValue = values bandit !! action
        newValue = oldValue + (reward - oldValue) / count
        newValues = updateList (values bandit) action newValue
    in bandit { counts = newCounts, values = newValues }

updateList :: [a] -> Int -> a -> [a]
updateList list index value = 
    take index list ++ [value] ++ drop (index + 1) list

getRegret :: MultiArmedBandit -> Double -> Double
getBanditRegret bandit optimalReward = 
    let totalReward = sum (values bandit)
        totalOptimal = optimalReward * fromIntegral (sum (counts bandit))
    in totalOptimal - totalReward

-- Q学习算法
-- Q-learning algorithm
data QLearning = QLearning {
    qTable :: Map.Map (Int, Int) Double,
    learningRate :: Double,
    discountFactor :: Double,
    epsilon :: Double
}

newQLearning :: Double -> Double -> Double -> QLearning
newQLearning learningRate discountFactor epsilon = QLearning {
    qTable = Map.empty,
    learningRate = learningRate,
    discountFactor = discountFactor,
    epsilon = epsilon
}

selectActionQL :: QLearning -> Int -> [Int] -> IO Int
selectActionQL ql state availableActions = do
    randomValue <- randomRIO (0.0, 1.0)
    if randomValue < epsilon ql
    then do
        -- 探索：随机选择
        randomIndex <- randomRIO (0, length availableActions - 1)
        return $ availableActions !! randomIndex
    else do
        -- 利用：选择Q值最大的动作
        let qValues = map (\action -> Map.findWithDefault 0.0 (state, action) (qTable ql)) availableActions
            maxIndex = fst $ maximumBy (comparing snd) (zip [0..] qValues)
        return $ availableActions !! maxIndex

updateQL :: QLearning -> Int -> Int -> Double -> Int -> [Int] -> QLearning
updateQL ql state action reward nextState nextActions = 
    let currentQ = Map.findWithDefault 0.0 (state, action) (qTable ql)
        maxNextQ = maximum $ map (\a -> Map.findWithDefault 0.0 (nextState, a) (qTable ql)) nextActions
        newQ = currentQ + learningRate ql * (reward + discountFactor ql * maxNextQ - currentQ)
        newQTable = Map.insert (state, action) newQ (qTable ql)
    in ql { qTable = newQTable }

getPolicy :: QLearning -> Int -> [Int] -> Int
getPolicy ql state availableActions = 
    let qValues = map (\action -> Map.findWithDefault 0.0 (state, action) (qTable ql)) availableActions
        maxIndex = fst $ maximumBy (comparing snd) (zip [0..] qValues)
    in availableActions !! maxIndex

-- 策略梯度算法
-- Policy gradient algorithm
data PolicyGradient = PolicyGradient {
    policyParams :: [Double],
    learningRate :: Double,
    discountFactor :: Double
}

newPolicyGradient :: Int -> Double -> Double -> IO PolicyGradient
newPolicyGradient numParams learningRate discountFactor = do
    params <- mapM (\_ -> randomRIO (-0.1, 0.1)) [1..numParams]
    return $ PolicyGradient params learningRate discountFactor

selectActionPG :: PolicyGradient -> [Double] -> [Int] -> IO Int
selectActionPG pg stateFeatures availableActions = do
    let actionProbs = computeActionProbs pg stateFeatures availableActions
    randomValue <- randomRIO (0.0, 1.0)
    return $ selectByProbability availableActions actionProbs randomValue

selectByProbability :: [Int] -> [Double] -> Double -> Int
selectByProbability actions probs randomValue = 
    go actions probs randomValue 0.0
  where
    go [] _ _ = 0
    go (action:rest) (prob:probs) randomValue cumulative
        | randomValue <= cumulative + prob = action
        | otherwise = go rest probs randomValue (cumulative + prob)

updatePG :: PolicyGradient -> [(Int, Double)] -> PolicyGradient
updatePG pg episode = 
    let returns = computeReturns pg episode
        gradients = zipWith (\returnVal (action, _) => returnVal) returns episode
        newParams = zipWith (\param grad => param + learningRate pg * grad) (policyParams pg) gradients
    in pg { policyParams = newParams }

computeActionProbs :: PolicyGradient -> [Double] -> [Int] -> [Double]
computeActionProbs pg stateFeatures availableActions = 
    let logits = map (\action -> computeLogit pg stateFeatures action) availableActions
    in softmax logits

computeLogit :: PolicyGradient -> [Double] -> Int -> Double
computeLogit pg stateFeatures action = 
    sum $ zipWith (*) (policyParams pg) stateFeatures

softmax :: [Double] -> [Double]
softmax logits = 
    let maxLogit = maximum logits
        expLogits = map (\x -> exp (x - maxLogit)) logits
        sumExp = sum expLogits
    in map (/ sumExp) expLogits

computeReturns :: PolicyGradient -> [(Int, Double)] -> [Double]
computeReturns pg episode = 
    go episode 0.0 []
  where
    go [] _ returns = reverse returns
    go ((action, reward):rest) futureReturn returns = 
        let newReturn = reward + discountFactor pg * futureReturn
        in go rest newReturn (newReturn:returns)

-- Actor-Critic算法
-- Actor-Critic algorithm
data ActorCritic = ActorCritic {
    actorParams :: [Double],
    criticParams :: [Double],
    actorLr :: Double,
    criticLr :: Double,
    discountFactor :: Double
}

newActorCritic :: Int -> Double -> Double -> Double -> IO ActorCritic
newActorCritic numParams actorLr criticLr discountFactor = do
    actorParams <- mapM (\_ -> randomRIO (-0.1, 0.1)) [1..numParams]
    criticParams <- mapM (\_ -> randomRIO (-0.1, 0.1)) [1..numParams]
    return $ ActorCritic actorParams criticParams actorLr criticLr discountFactor

selectActionAC :: ActorCritic -> [Double] -> [Int] -> IO Int
selectActionAC ac stateFeatures availableActions = do
    let actionProbs = computeActionProbsAC ac stateFeatures availableActions
    randomValue <- randomRIO (0.0, 1.0)
    return $ selectByProbability availableActions actionProbs randomValue

updateAC :: ActorCritic -> [Double] -> Int -> Double -> [Double] -> ActorCritic
updateAC ac stateFeatures action reward nextStateFeatures = 
    let currentValue = computeValue ac stateFeatures
        nextValue = computeValue ac nextStateFeatures
        tdError = reward + discountFactor ac * nextValue - currentValue
        
        -- 更新Critic
        newCriticParams = zipWith (\param feature -> param + criticLr ac * tdError * feature) 
                                  (criticParams ac) stateFeatures
        
        -- 更新Actor
        actionProbs = computeActionProbsAC ac stateFeatures [action]
        logProb = log (head actionProbs)
        newActorParams = zipWith (\param feature -> param + actorLr ac * tdError * logProb * feature) 
                                 (actorParams ac) stateFeatures
    in ac { actorParams = newActorParams, criticParams = newCriticParams }

computeValue :: ActorCritic -> [Double] -> Double
computeValue ac stateFeatures = 
    sum $ zipWith (*) (criticParams ac) stateFeatures

computeActionProbsAC :: ActorCritic -> [Double] -> [Int] -> [Double]
computeActionProbsAC ac stateFeatures availableActions = 
    let logits = map (\action -> computeLogitAC ac stateFeatures action) availableActions
    in softmax logits

computeLogitAC :: ActorCritic -> [Double] -> Int -> Double
computeLogitAC ac stateFeatures action = 
    sum $ zipWith (*) (actorParams ac) stateFeatures

-- 测试函数
-- Test functions
testReinforcementLearningAlgorithms :: IO ()
testReinforcementLearningAlgorithms = do
    putStrLn "Testing Reinforcement Learning Algorithms..."
    
    -- 测试多臂老虎机
    -- Test multi-armed bandit
    let bandit = newMultiArmedBandit 3 0.1
    action <- selectAction bandit
    let updatedBandit = update bandit action 1.0
    putStrLn $ "Multi-armed bandit action: " ++ show action
    
    -- 测试Q学习
    -- Test Q-learning
    let ql = newQLearning 0.1 0.9 0.1
    action <- selectActionQL ql 0 [0, 1, 2]
    let updatedQL = updateQL ql 0 action 1.0 1 [0, 1, 2]
    putStrLn $ "Q-learning action: " ++ show action
    
    -- 测试策略梯度
    -- Test policy gradient
    pg <- newPolicyGradient 4 0.01 0.9
    let stateFeatures = [1.0, 0.0, 0.0, 0.0]
    action <- selectActionPG pg stateFeatures [0, 1]
    let episode = [(action, 1.0)]
    let updatedPG = updatePG pg episode
    putStrLn $ "Policy gradient action: " ++ show action
    
    -- 测试Actor-Critic
    -- Test Actor-Critic
    ac <- newActorCritic 4 0.01 0.01 0.9
    let stateFeatures = [1.0, 0.0, 0.0, 0.0]
    let nextStateFeatures = [0.0, 1.0, 0.0, 0.0]
    action <- selectActionAC ac stateFeatures [0, 1]
    let updatedAC = updateAC ac stateFeatures action 1.0 nextStateFeatures
    putStrLn $ "Actor-Critic action: " ++ show action
    
    putStrLn "Reinforcement learning algorithm tests completed!"
```

### Lean实现 (Lean Implementation)

```lean
-- 强化学习算法理论的形式化定义
-- Formal definition of reinforcement learning algorithm theory
import Mathlib.Data.Nat.Basic
import Mathlib.Data.List.Basic
import Mathlib.Algebra.BigOperators.Basic

-- 马尔可夫决策过程定义
-- Definition of Markov Decision Process
def MDP := {
    states : List Nat,
    actions : List Nat,
    transition : Nat → Nat → Nat → Float,
    reward : Nat → Nat → Nat → Float,
    gamma : Float
}

-- Q学习算法定义
-- Definition of Q-learning algorithm
def QLearning := {
    qTable : List ((Nat × Nat) × Float),
    learningRate : Float,
    discountFactor : Float
}

-- 策略梯度算法定义
-- Definition of policy gradient algorithm
def PolicyGradient := {
    policyParams : List Float,
    learningRate : Float,
    discountFactor : Float
}

-- Q学习更新
-- Q-learning update
def qLearningUpdate (ql : QLearning) (state : Nat) (action : Nat) (reward : Float) (nextState : Nat) : QLearning :=
  let currentQ := getQValue ql state action
  let maxNextQ := getMaxQValue ql nextState
  let newQ := currentQ + ql.learningRate * (reward + ql.discountFactor * maxNextQ - currentQ)
  let newQTable := updateQTable ql.qTable (state, action) newQ
  ql { qTable := newQTable }

-- 策略梯度更新
-- Policy gradient update
def policyGradientUpdate (pg : PolicyGradient) (episode : List (Nat × Float)) : PolicyGradient :=
  let returns := computeReturns pg episode
  let gradients := zipWith (\returnVal (action, _) => returnVal) returns episode
  let newParams := zipWith (\param grad => param + pg.learningRate * grad) pg.policyParams gradients
  pg { policyParams := newParams }

-- 价值函数计算
-- Value function computation
def computeValue (pg : PolicyGradient) (stateFeatures : List Float) : Float :=
  sum (zipWith (*) pg.policyParams stateFeatures)

-- 动作概率计算
-- Action probability computation
def computeActionProbs (pg : PolicyGradient) (stateFeatures : List Float) (actions : List Nat) : List Float :=
  let logits := map (\action => computeLogit pg stateFeatures action) actions
  softmax logits

-- 强化学习正确性定理
-- Reinforcement learning correctness theorem
theorem q_learning_convergence (ql : QLearning) (state : Nat) (action : Nat) :
  let updatedQL := qLearningUpdate ql state action reward nextState
  getQValue updatedQL state action ≥ getQValue ql state action := by
  -- 证明Q学习的收敛性
  -- Prove convergence of Q-learning
  sorry

-- 策略梯度定理
-- Policy gradient theorem
theorem policy_gradient_correctness (pg : PolicyGradient) (episode : List (Nat × Float)) :
  let updatedPG := policyGradientUpdate pg episode
  computeValue updatedPG stateFeatures ≥ computeValue pg stateFeatures := by
  -- 证明策略梯度的正确性
  -- Prove correctness of policy gradient
  sorry

-- 实现示例
-- Implementation examples
def solveQLearning (state : Nat) (action : Nat) (reward : Float) (nextState : Nat) : Float :=
  -- 实现Q学习算法
  -- Implement Q-learning algorithm
  0.0

def solvePolicyGradient (episode : List (Nat × Float)) : List Float :=
  -- 实现策略梯度算法
  -- Implement policy gradient algorithm
  []

-- 测试定理
-- Test theorems
theorem q_learning_test :
  let state := 0
  let action := 1
  let reward := 1.0
  let nextState := 2
  let result := solveQLearning state action reward nextState
  result ≥ 0.0 := by
  -- 测试Q学习算法
  -- Test Q-learning algorithm
  sorry

theorem policy_gradient_test :
  let episode := [(0, 1.0), (1, 0.5)]
  let result := solvePolicyGradient episode
  result.length > 0 := by
  -- 测试策略梯度算法
  -- Test policy gradient algorithm
  sorry
```

## 复杂度分析 (Complexity Analysis)

### 时间复杂度 (Time Complexity)

1. **Q学习算法**: $O(|S| \cdot |A| \cdot \text{episodes})$
2. **策略梯度算法**: $O(d^2 \cdot \text{episodes})$
3. **Actor-Critic算法**: $O(d^2 \cdot \text{episodes})$
4. **多臂老虎机**: $O(n \cdot \text{rounds})$

### 空间复杂度 (Space Complexity)

1. **Q学习算法**: $O(|S| \cdot |A|)$
2. **策略梯度算法**: $O(d)$
3. **Actor-Critic算法**: $O(d)$
4. **多臂老虎机**: $O(n)$

### 收敛性分析 (Convergence Analysis)

1. **Q学习**: 保证收敛到最优Q函数
2. **策略梯度**: 收敛到局部最优策略
3. **Actor-Critic**: 收敛到局部最优策略
4. **多臂老虎机**: 遗憾界为 $O(\sqrt{n \log n})$

## 应用领域 (Application Areas)

### 1. 游戏AI (Game AI)

- 棋类游戏、电子游戏等
- Chess games, video games, etc.

### 2. 机器人控制 (Robot Control)

- 路径规划、运动控制等
- Path planning, motion control, etc.

### 3. 自动驾驶 (Autonomous Driving)

- 决策制定、轨迹规划等
- Decision making, trajectory planning, etc.

### 4. 推荐系统 (Recommendation Systems)

- 个性化推荐、A/B测试等
- Personalized recommendation, A/B testing, etc.

## 总结 (Summary)

强化学习算法通过与环境交互来学习最优策略，具有自主学习和适应能力。其关键在于设计有效的奖励机制和学习算法。

**Reinforcement learning algorithms learn optimal policies through interaction with the environment, featuring autonomous learning and adaptation capabilities. The key lies in designing effective reward mechanisms and learning algorithms.**

### 关键要点 (Key Points)

1. **智能体与环境交互**: 通过试错学习最优策略
2. **奖励机制**: 通过奖励信号指导学习过程
3. **策略优化**: 学习最优的行为策略
4. **价值函数**: 评估状态或动作的长期价值

### 发展趋势 (Development Trends)

1. **理论深化**: 更深入的收敛性分析
2. **应用扩展**: 更多实际应用场景
3. **算法优化**: 更高效的采样策略
4. **多智能体学习**: 多智能体协作学习

---

*本文档提供了强化学习算法理论的完整形式化定义，包含数学基础、经典问题、学习算法分析和实现示例，为算法研究和应用提供严格的理论基础。*

**This document provides a complete formal definition of reinforcement learning algorithm theory, including mathematical foundations, classic problems, learning algorithm analysis, and implementation examples, providing a rigorous theoretical foundation for algorithm research and applications.**
